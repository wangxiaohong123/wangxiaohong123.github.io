---
title: 6.搜索
tags:
  - elasticsearch
categories: 数据库
copyright: true
---

##### query string的语法

简单的查询可以把过滤条件直接放到url中，使用q表示过滤，比如查询test_field1字段等于test1：

```http
### 两中写法都是包含就可以，包含的意思是分词之后，存在相同单词
GET /test_index/_search?q=test_field1:test1
GET /test_index/_search?q=+test_field1:test1

### 减号表示不等于
GET /test_index/_search?q=-test_field1:test1
```

当我们不指定field的时候比如下面这样就是搜索所有字段，只要有一个字段等于或者不等于(取决于是否使用减号)指定值就可以：

```http
GET /test_index/_search?q=test1
GET /test_index/_search?q=-test1
```

在新建document的时候，会把所有的field拼到一个字符串里，并且定义为\_all字段，分词后也会建立倒排索引，在搜索时，如果没指定字段，就会去查找\_all字段中包含test的所有document。

##### 搜索模式

*   精确匹配：exact value，全部匹配才算匹配；
*   全文检索：full text，缩写（比如china 和cn），格式转换(likes、like、liked)大小写，近义词都可以当成匹配；

在建立倒排索引之前会进行normalization处理(提升recall召回率)，把拆出来的词同义词转换、单复数转换等动作，然后在根据转换后的词建立倒排索引，查询的时候也是一样的，先拆词在normalization，在去检索。

不同类型的field，有的可能是exact value比如date，有的可能是full text，比如_all、text。

##### 分词器

切分词语和normalization操作都是分词器的工作。主要是三步：

1.  character filter：过滤文本的内容，比如把html标签和符号去掉；
2.  tokenizer：分词；
3.  token filter：normalization操作，转换之后可能就把没意义的词去掉了，比如a、the之类的；

###### 1.内置分词器：

*   standard analyzer：默认的分词器。可以大小写转换、去符号，但是没拆'\_'；
*   simple analyzer：大小写转换、去符号，也能去'\_'；
*   whitespace analyzer：什么都不干，大小写都不转，只按照空格拆；
*   language analyzer：特定语言分词器，比如英语分词器，他可以把单词转成近义词、大小写、去符号、时态转换、去掉没意义的词；

###### 2.测试分词

发送下面的请求会根据参数里analyzer指定的分词器拆分text内容并返回结果：

```http
GET /_analyze
{
    "analyzer": "standard",
    "text": "测试分词器"
}
```

##### 搜索api

http协议中是不允许get请求带上body的，但是es觉得get更能表达是查询的请求，所以在es里支持了get+body搜索，当碰到没法使用get+body的情况时，使用post也可以。

###### 1.超时机制

es的超时并不是查找所有为基础去判断超时时间，他是在超时时间内把能查到的document都返回，比如超时时只查到1条数据，那就只返回一条数据。默认是不开启的，就是说查询耗时多久，程序就要阻塞多久，手动指定timeout可以在url后面拼上参数：

```http
GET /_search?timeout=10ms
```

###### 2.multi-index和

url上的index可以用逗号拼多个，还可以使用通配符比如：

```http
GET /test_index,test_index1/_search
```

###### 3.分页

在查询的url里加上size和from两个参数就可以实现分页：

```http
GET /test_index/_search?size=10&from=10001
```

或者把size和from放到body中：

```json
{
  "query": {
    "match_all": {}
  },
  "sort": [
    {
      // 7版本用这个字段排序会发出警告
      "_id": {
        "order": "desc"
      }
    }
  ], 
  "size": 1,
  "from": 0
}
```

使用from+size分页在大数据量的情况下是很慢的，可以使用scroll分页优化，查询时在url上加上scroll=1m(scroll快照的过期时间)这种参数，不是第一次查询时必须要有scroll_id字段：

```json
GET /test_index/_search?scroll=1m
{
  "query": {
    "match_all": {}
  },
  // 是es优化过的排序，如果没有要求顺序可以使用这个
  "sort": ["_doc"],
  "size": 1
}
```

第二次搜索的时候只需要传scroll的id就可以拿到下一页的数据了，连index都不需要指定：

```json
GET /_search/scroll
{
  "scroll": "1m",
  "scroll_id": "FGluY2x1ZGVfY29udGV4dF91dWlkDXF1ZXJ5QW5kRmV0Y2gBFHpvX2FsbjhCd2lENjB4ZTBPLS1iAAAAAAADjhsWZjVHelU4TlVSUmFjM1BONFFvY3llUQ=="
}
```

###### 4.query DSL

quer DSL的基本语法：

```json
{
    QUERY_NAME: {
        ARGUMENT: VALUE,
        ARGUMENT: VALUE,...
    }
}

{
    QUERY_NAME: {
        FIELD_NAME: {
            ARGUMENT: VALUE,
            ARGUMENT: VALUE,...
        }
    }
}
```

比如查找test_field字段中包含test1的doc：

```http
GET /test_index/_search
{
  "query": {
    "match": {
      "test_field1": "test1"
    }
  }
}
```

多个多个查询条件使用bool：

```http
GET /test_index/_search
{
  "query": {
    "bool": {
      "must": {
        "match": {
          "test_field1": "test1"
        }
      },
      "should": [{
          "match": {
            "name": "tom"
            }
        },{
          "bool": {
            "must": [
              {"match": {"personality": "good"}}
            ],
            "must_not": [
              {"match": {"rude": true}}
            ]
          }
        }
      ]
    }
  }
}
```

搜索的语法：

*   match all：查找所有
*   match：单field匹配
*   multi match：给定字段匹配多field
*   range：范围匹配，可以放在query里，也可以放在filter里
*   term：按照exact value模式匹配
*   terms：批量term
*   exist：es2.x的时候中用来指定field不能为空，现在被废弃了

如果不想计算结果的相关度还可以用constant_score，排序使用sort，constant_score+filter+sort的例子：

```http
GET /test_index/_search
{
  "query": {
    "constant_score": {
      "filter": {
        "range": {
          "age": {
            "gt": 10
          }
        }
      }
    }
  },
  "sort": [
    {
      "age": {
        "order": "desc"
      }
    }
  ]
}
```

因为string类型的field会被分词，如果sort的field是索引就会不准，这个时候要单独创建一个不分词的field，内容和分词的field一样，专门用来排序：

```json
PUT /website 
{
  "mappings": {
      "properties": {
          "title": {
              "type": "text",
              // 单独的row field进行排序
              "fields": {
                  "raw": {
                      "type": "string",
                      "index": "not_analyzed"
                  }
              },
              // 创建正排索引
              "fielddata": true
          },
          "content": {
              "type": "text"
          },
          "post_date": {
              "type": "date"
          }
      }
  }
}
```

需要排序的时候使用title.row字段进行排序。

校验搜索是否合法：`GET /{test_index}/_validate/query?explain`。