---
title: 1.habse基础
date: 2021-06-06 06:27:35
tags:
  - HBase
categories: 数据库
copyright: true
---

### HDFS和HBase的关系：

HDFS能做的事非常少，也就是创建文件，删除文件，大文件读取，追加数据，如果想要修改或者根据某些条件查询就不行，hbase就是做这个的。

hbase是一个nosql，他不负责存储数据，需要基于HDFS来实现，但是他不能执行复杂的条件查询，对海量数据的简单的增删改查。

### 架构特点

*   hbase基于多台regionServer来管理数据分片，regionServer是高可用的，region是存储在HDFS中，本省就有两个副本。

*   主从强一致。

*   支持MapReduce和spark这种分布式计算引擎。

*   支持java API。

*   支持协处理器，块缓存和布隆过滤器。

*   可以使用web端管理和运维。

### 使用场景

海量数据的简单增删改查，不支持索引，也不知道事务，不支持SQL语法。想要上面的功能也没必要用hbase，实现索引可以用es，实现海量数据事务可以用TiDB，实现海量数据的的实时分析可以用clickhouse或者druid。

### 数据模型和物理模型

针对数据模型有几个概念：

rowkey：每行都有一个rowkey，和mysql的主键差不多，也会根据这个排序，所以要把相似的数据放到一起，这样的话好查，比如这里存了订单和订单明细，rowkey可以是这样的order_1_111，order-detail_1_110，就是用户1在的111订单，和用户1的110订单明细。

列族：就是列名，两部分组成，列族+分号+列限定符（column qualifier），列族就是一系列的类的family，有点像表的意思。

列：他的列是要存储多个版本的值的，每个值都带着一个时间戳（版本）。

单元格：取到某行某列的某个时间戳对应的值就是一个单元格。

rowkey					order:base			order:detail			order:extent

order_1_110			xxx						xxx

order_1_111			x1(t1); x2(t2)		  xxx						xxx

实际存储上他会把数据根据列来拆分成行进行存储，所以叫列式存储，像下面这样：

rowkey				timestamp		列					值

order_1_110		t1					order:detail		 xxx

order_1_111		t2					order:base		  xxx

order_1_111		t3					order:extent		xxx

### 简单的语法

进入bin下执行**./hbase shell**连接hbase。

```shell
# 查看命令使用
help 'list'
# 创建工作空间
create_namespace 'stats'
# 修改工作空间
alter_namespace 'stats', {METHOD=>'set', NAME => 'stats1'}
# 修改工作空间
drop_namespace 'stats'
# 创建一张test表，制定工作空间是stats，指定一个列族cf，创建表时必须要有列族，可以是多个列族create 'test','cf1','cf2'
create 'stats:test','cf'
# 使用list获取所有表，也可以指定表，比如list 'test'
list
# 或者使用exist 表名
exists 'test'
# 禁用表，所有dml操作都需要先禁用表
disable 'test'
# 启用表
enable 'test'
# 表是否启用
is_enabled 'test'
# 查看表的明细
describe 'test'
# 添加列族
alter 'test', 'Extra'
# 删除列族
alter 'test', {NAME=>'Extra', METHOD=>'delete'}
# 删除表，这一步是基于disable之后的
drop 'test'
# 插入或者更新一行数据，put 表名,rowkey,列族：列名,值……
put 'test','row1','cf1:a','1','cf2:b'
# 指定返回行数
scan 'test', {LIMIT=>10}
# 获取列族或者列数据
scan 'test', {COLUMNS=>['cf1:a', 'cf2:b']}
# 根据rowkey前缀匹配
scan 'test', {STARTROW => 'row1'}
# 获取表数据，例子是倒叙，条件可以不带
scan 'test',{REVERSED=>true,STARTROW=>'row100~',ENDROW=>'row100',LIMIT=>5}
# 再来个正序
scan 'test',{STARTROW=>'row100',ENDROW=>'row100~',LIMIT=>5}
# 还可以根据row key获取一条数据
get 'test' 'row1'
# 删除数据，表名,rowkey,列族:列名，列族和列名都不是必须的，这样是删除最新的一个数据
# 把所有版本全部删除时deleteall
delete 'test','row1','cf1:a'
```

```properties
# 刷新memStore的阈值，默认是128M
hbase.hregion.memstore.flush.size
# 当所有memStore都超过了这么大的时候把memStore刷到hdfs中
hbase.regionserver.global.memstore.size.lower.limit
# HLog数量上限
hbase.regionserver.maxlogs
# 设置chunk大小，默认是2MB
hbase.hregion.memstore.mslab.chunksize
# 开启chunk pool，默认是0，可以设置0到1的数字
# 比如设置成0.3，就会把年轻代的大小 * 0.3分配给pool
hbase.hregion.memstore.chunkpool.maxsize
# pool中初始化多少个chunk
hbase.hregion.memstore.chunkpool.initialsize
# 每隔多久创建一个新的WAL日志文件，默认一小时
hbase.regionserver.logroll.period
# 每隔多久删掉过期的old WAL文件，默认1分钟
hbase.master.cleaner.interval
# old WAL文件过期时间，默认10分钟
hbase.master.logcleaner.ttl
# 合并HFile的阈值
hbase.hstore.compactionThreshold
# major compaction的周期（天），0是关闭
hbase.hregion.majorcompaction
# 当未合并的文件数量低于这个阈值时，停止合并，默认是3
hbase.store.compaction.min
# 合并文件的large线程池的线程数
hbase.regionserver.thread.compaction.large
# 合并文件的small线程池的线程数
hbase.regionserver.thread.compaction.small
# 合并的文件数超过这个值就会使用large线程池
hbase.regionserver.thread.compaction.throttle
```

