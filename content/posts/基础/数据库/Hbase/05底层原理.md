---
title: 5.底层原理
date: 2021-06-24 06:27:35
tags:
  - HBase
categories: 数据库
copyright: true
---

### HLog

每个HRegionServer都有一个HLog，也可以配置多个，所有region共享这个HLog，每次操作数据都会产生一个log，很像MySQL的redoLog，这条log的key就是**表名+region名+写入时间+sequenceid+clusterid**，内容就是对那个列改了什么value。

在hdfs上有两个专门的目录存放HLog："/hbase/WALs"和"/hbase/oldWALs"，WALs里存放的是还没过期的数据，就说还在memStore里，没有刷到hdfs中的，在这个目录下面对于每个HRegionServer都有一个自己的目录，类似这样：

**/hbase/WALs/hbase12.df.zszs.org,60020,1304970381600**

hbase12.df.zszs.org是HRegionServer的机器域名，60020是统一的端口号，最后面试创建的时间戳，在HRegionServer文件夹里就是一堆HLog文件了。有多个HLog文件是为了方便删除，因为大部分的数据都会刷到hdfs中，已经落盘的数据对应的HLog就没有存在的必要了。

什么时候创建一个新的HWAL日志文件呢？通过配置**hbase.regionserver.logroll.period**，这个默认是1小时，每个小时HBase后台的一个线程就会去创建一个新的日志文件。当数据被持久化到了hdfs中后，对应的日志文件就会被放到oldWALs中，然后HBase的后台还有一个线程根据参数**hbase.master.cleaner.interval(默认1分钟)**来检查文件在oldWALs文件夹里待的时间超过了**hbase.master.logcleaner.ttl(默认10分钟)**，就会把这个日志删掉了。为什么还要再等10分钟呢？据说是主要用于调试。

### memStore的写入

memStore使用的是双跳表机制来实现key的有序性，直接使用的java的ConcurrentSkipListMap，把rowkey+列族+列+timestamp当做跳表的key，当一个跳表满了的时候新进来的数据都会写入到另一个跳表中，这个跳表的数据慢慢刷到hdfs里，数据的value存储在chunk数组里。

### HFile

#### 逻辑组成

HFile逻辑上包含4部分：

*   Scanned block：Data Block（key-value数据），Leaf Index Block（索引树的叶子节点），Bloom Block（布隆过滤器）；
*   Non-scanned Block：Meta Block、Intermediate Level Data Index Blocks；
*   Load-on-open Block：这部分会在RegionServer打开HFile的时候直接加载到内存里去，包括FileInfo、布隆过滤器的MetaBlock、Root Index Block和Meta IndexBlock（在查找元素的时候都要从根节点开始）、Bloom Index Block（这个是布隆过滤器的索引，因为一个HFile可能会有很多布隆过滤器）；
*   Trailer Block：HFile版本和其他几个部分的偏移量以及寻址信息，寻址信息就包括Load-on-open Block的地址；

当HRegionServer打开HFile的时候会先读取文件的信息，这个时候就知道了文件有多少字节，然后就可以从末尾把Trailer Block读取出来，Trailer中有Load-on-open Block的位置信息，就可以把Load-on-open Block读取到内存中。

#### 查询

当从HFile里查找文件的时候，会现根据Load-on-open Block中的布隆过滤器索引拿到所有的过滤器，然后判断要查找的数据是否在这个文件中，如果在就根据LSM树来查找数据，也就是根据Scanned block中的Leaf Index Block最终拿到Data Block，data block中就是一个一个的key-vakue对，key由rowkey、列族、列、操作时间戳、keyType（keyType就是Put、Delete、DeleteColumn、DeleteFamily这些）组成。

LSM树和MySQL的聚簇索引差不多，当数据量少的时候只有一层，Load-on-open Block直接指向Data Block，当数据量变多后，会出现叶子节点，再多就会出现Non-scanned Block的Intermediate Level Data Index Blocks。

不管是get还是scan，底层都是scan，在查询时，首先会有3层scan（RegionScanner、StoreScanner、MemStoreScanner和StoreFileScanner），第一层先找到对应的region，第二层找到store，第三层在memStore找到具体的数据和通过布隆找对具体的HFile。

找到数据之后会在内存中进行合并筛选。

#### block cache

因为每次查找数据会涉及到几次磁盘IO，读取intermediate level index block发生一次IO，读取leaf index block发生一次IO，读取data block发生一次IO，最多就3次IO，但是如果是批量get并且只有每次只有3个IO，查询速度也会达到秒级，所以hbase有一个和MySQL的缓冲池类似的东西，block cache，它是以我们存储数据的block为单位存储，其实就是3个ConcurrentHashMap，map的key就是block的key，value可能就是一个地址的指向：

*   single-access：数据刚被读取的时候会在这个map里
*   multi-access：当数据被第二次使用的时候会从single-access移动到multi-access里
*   in-memory：比如在创建列族的时候指定了n_memory=true，这个列族的block就会存在在这个map中

这3个map都是LRU map，内存占用比是25%:50%:25%。

#### HFile合并

因为memStore有很多条件会生成一个新的HFile，每个HFile的元数据信息、LSM、data block的索引信息都会存在内存中，如果不控制HFile的数量，内存很快就会被撑爆的。

HFile的合并有两种，minor compaction和major compaction，minor就是选择一部分，major就是将所有文件合并：

1.  当HFile的数量超过**hbase.hstore.compactionThreshold**的时候会进行合并。
2.  后台线程定期去检查HFile是否超过**hbase.server.thread.wakefrequency * hbase.server.compactchecker.interval.multiplier**。
3.  每7天进行一次major compaction，设置**hbase.hregion.majorcompaction**为0就是不自动major compaction。

合并时，major的优先级高于minor，进行minor compaction时会最老的文件开始选择，低峰期当合并的文件和剩余文件的比例超过1.2或者高峰期两种文件数量超过5就停止合并，还有就是剩余未合并的文件数量小于**hbase.store.compaction.min(默认3)**就停止合并。用来合并有两个线程池，一个large一个small，当文件数大于某一个值的时候会使用large线程池，这两个线程池默认都只有一个线程，文件数和线程池中线程数量都可以配置。

合并的过程就是把文件读取，然后排序，重写到一个临时文件夹下，写完了就把文件sync到HFile目录下，删除旧文件。

### region分裂

当region中的store大小超过阈值之后就会进行region分裂，在0.94版本后这个阈值时hbase自己动态调整的，大表的阈值就高点，小表的阈值就低点，就是找到最大的store中间的rowkey所在的block，然后以这个block拆成两个store，进行迁移。

hbase会自动感知region个数，region负载，读写请求数量，storefile大小，memstore大小，移动数据的代价等等数据来判断是否有便宜进行调整。
