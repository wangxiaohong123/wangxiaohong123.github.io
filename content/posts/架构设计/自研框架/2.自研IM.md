---
title: 自研框架-自研im
tags:
  - 框架
categories: 自研框架
copyright: true
---

整体架构：
![IM系统架构](https://raw.githubusercontent.com/wangxiaohong123/p-bed/main/uPic/IM系统架构.png)

* 分发层启动之后会在zk上注册节点信息；
* 接入层启动之后也会在zk上注册节点信息，然后监听zk上的分发层机器信息，当感知到分发层有机器上线的时候就发起链接并保存分发层信息，然后发送一个注册消息，分发层收到注册消息之后保存接入层信息；
* 接入层每5s向分发层发送一次心跳，接入层和分发层都是通过IdelStateHandler配置的客户端如果30s没有读写请求就断开连接；
* 接入层在zk上的节点信息有当前连接的客户端数量(接入层每隔3s更新)，当客户端请求ipList服务的时候会挑一个连接数少的接入层服务器返回给客户端；
* 客户端和接入层建立连接之后会把连接信息发送给路由服务，路由服务校验token，再把用户信息和接入服务的机器信息存到redis中，然后返回响应给接入层，接入层收到消息后会维护用户和连接信息；

##### 1.技术栈

整体架构使用cloud alibaba，缓存使用redis，关系数据库使用MySQL，消息持久化使用MongoDB或者hbase，消息中间件使用RocketMQ或者kafka，接入层和分发层的定向交互使用zk，序列化协议使用protobuf。加密使用netty的SSL。

如果要支持多端的话还需要加很多东西。

##### 2.为什么要单独引入zk

分发层和接入层都是多服务器部署，如果分发层有服务器宕机，接入层可以自动重试，但是分发层发送推送消息请求时，如果接入层有机器宕机，需要马上感知下线的服务实例，把推送请求暂存，等到客户端重连后再进行推送，同时也需要根据session信息来选择要调用那台接入层机器，使用zk是合理的，虽然nacos也可以做到，但是麻烦一些。

##### 3.tcp接入层和websocket接入层要分开

因为tcp接入层面向的是c端用户，用户量比较大，要部署大量机器。websocket面向的一般是网页比如客服之类的，可能1台机器就足够了。而且两个协议的很多代码不一样，不能合并到一起处理。

##### 4.自定义协议

协议头共20个字节：

*   headLength：4byte，消息头长度，固定20
*   clientVersion：4byte，客户端SDK版本号
*   requestType：4byte，请求类型
*   sequence：4byte，请求序号
*   bodyLength：4byte，请求体长度

请求体的序列化和反序列化协议使用protobuf。

###### 4.1 protobuf demo

首先到github(`https://github.com/protocolbuffers/protobuf/releases/tag/v3.9.0`)下载对应系统的安装包然后解压。

```shell
# 进入解压后的文件夹执行
# 设置编译目录
./configure --prefix=[编译目录]
# 编译并安装
make
make install

# 配置环境变量
open ~/.bash_profile
# 添加配置
export PROTOBUF=[编译目录]
export PATH=$PROTOBUF/bin:$PATH

# 刷新配置文件
source .bash_profile
```

将proto文件编译成java文件：

```shell
# --java_out生成的java文件所在目录，目录需要手动创建
# AuthenticateRequest.proto是proto文件位置
protoc --java_out=./src AuthenticateRequest.proto
```

新建proto文件：

```protobuf
syntax = "proto3";

option java_package = "com.why.im.protocol";
option java_outer_classname = "AuthenticateRequestProto";

message AuthenticateRequest {
	string uid = 1;
	string token = 2;
	int64 timestamp = 3;
}
```

项目中引入maven依赖：

```xml
<!-- https://mvnrepository.com/artifact/com.google.protobuf/protobuf-java -->
<dependency>
    <groupId>com.google.protobuf</groupId>
    <artifactId>protobuf-java</artifactId>
    <version>3.9.1</version>
</dependency>
```

编写测试类：

```java
public class ProtoTest {

    public static void main(String[] args) throws InvalidProtocolBufferException {
        AuthenticateRequestProto.AuthenticateRequest request = createAuthenticateRequest();
        byte[] serializedAuthenticateRequest = request.toByteArray();
        System.out.println(new String(serializedAuthenticateRequest));

        AuthenticateRequestProto.AuthenticateRequest deserializedAuthenticateRequest =
                AuthenticateRequestProto.AuthenticateRequest.parseFrom(serializedAuthenticateRequest);
        System.out.println(deserializedAuthenticateRequest.toString());
    }

    private static AuthenticateRequestProto.AuthenticateRequest createAuthenticateRequest() {
        AuthenticateRequestProto.AuthenticateRequest.Builder builder =
                AuthenticateRequestProto.AuthenticateRequest.newBuilder();
        builder.setUid("test01");
        builder.setToken("test01_token");
        builder.setTimestamp(System.currentTimeMillis());
        
        return builder.build();
    }
}
```

##### 5.分发系统、接入系统、客户端宕机或异常退出怎么办

如果分发系统宕机，接入系统可以马上感知并执行channelInactive()方法，我们把维护的接入系统的连接信息删掉即可。

如果接入系统宕机，那么此时客户端也会断开连接，这个时候就算分发系统把消息发送到接入系统，接入系统也不能把消息推送到客户端，此时需要客户端执行重连逻辑，分发系统把消息暂存，直到再次感知到消息所属客户端重连并且认证成功，在找到对应的接入系统实例发送消息。

客户端断开连接，此时session应该马上失效，然后分发系统还是会感知到找不到消息所属人所在的客户端，还是会把消息暂存。

##### 6.离线消息

离线消息采用写扩散+redis+receiverId hash + sortedSet(消息时间戳作为分数)方式，使用receiverId hash防止key在redis集群上分布不均匀，使用sortedSet存储是为了方便读取和删除，也可以实现分段拉取。群聊消息采用写扩散方式，每个群成员都有对应的离线消息。在没收到ack的时候就需要存储离线消息，收到ack之后再删掉这条消息。**需要预估好离线消息的容量，否则redis满了之后可能自动清理离线消息数据**。这样做有个好处就是服务端为了保证消息不丢失只需要推送一次，失败了不需要重复推送。

##### 7.群消息

*   群消息不能再客户端生成sequenceId，需要把同一个群的消息路由到同一台分发层，分发层使用redis自增生产群的sequence，这样可以保证群消息的顺序。
*   功能层拉取到mq中的群消息之后交给线程池异步处理，然后直接返回ack(这里如果异步处理失败，不会返回ack，客户端会重新发送消息)。异步处理消息的逻辑是写到hbase中，rowKey采用groupId+消息id逆序(long最大值-消息id)，这样符合查消息的时候从后往前查。
*   群消息持久化之后就可以推送给群成员了，但是如果要推送的群消息很多，比如超过5条，这个时候可以给群成员推送一个拉取群消息的通知，客户端发起http调用获取群消息。

##### 8.timeline模型

timeline模型就是维护一个有序的消息列表，在这个系统里timeline可能会发生错乱和丢失，因为我们在推送消息的时候是随机选择的分发服务，mq中也没有路由，所以到客户端的顺序可能是乱掉的，所以在推送消息的时候可以使用客户端自增的sequenceId来路由，进入mq的时候也使用sequenceId路由，客户端实时根据sequenceId进行调整顺序。

目前redis没有一个成熟的数据类型支撑timeline模型，又支持海量数据又支持timeline模型，又支持高并发。阿里云提供了一个table store服务是专门的timeline模型。

##### 9.已读功能

首先每条消息都有一个状态，然后采取客户端主动拉取未读信息，如果是服务端推送的话会导致客户端收到大量的群消息已读通知，可能导致客户端崩溃，当客户端停留在某条消息上时，发送请求获取这条消息以后的有未读状态的消息的未读信息。并且采用定时查询，比如第一次2s，第二次4s，第3次8s。

##### 10.未读数功能

直接使用redis保存就可以了。但是未读数可能会出现不一致的情况，因为可能有多条未读消息同时更新，或者正在清空未读消息，但是此时有收到增加未读消息的请求，又或者更新总未读数成功，更新会话未读数失败，所以需要加分布式锁。如果加锁后修改未读数出现失败的情况可以执行重试操作，实在不行就还原操作。

##### 11.消息丢失

客户端或者服务端向对方发送消息的时候都会使用ack机制，就是说当收到对方的ack才算成功，消息丢失的情况：

1.   客户端发送消息因为网络问题失败：此时会被异常捕获，把消息放进延迟队列进行一定次数的重试，达到配置的重试次数还是失败就提示用户发送失败；
2.   客户端发送成功，但是没有收到消息的ack：当消息发送成功之后需要把消息暂存到延迟队列中，这个延迟时间可以长一点。定时检查这个队列中是否有等待ack的消息，如果有的话还是会进行消息重发。一直到收到ack之后把消息移出这个队列。
3.   分发层发送消息没有收到ack会直接存储离线消息，客户端重新上线后会拉取离线消息。

不管是服务端还是客户端防止消息丢失都需要重试，根据sequenceId去重。

##### 12.安全

DNS劫持，比如运营商的LocalDNS被黑了，把我们的域名指向了其他网站，家用路由器被劫持重置一下就可以了。运营商的可以使用云厂商的HttpDNS服务。

##### 13.消息id

服务端有一个单独的id服务(提供雪花算法和segement两种方式)，segement使用网易云信开源的id框架，客户端每次从服务端获取一批id并在客户端缓存。

##### 14.timeline模型

timeline模型是说每个用户只有一个队列存储离线消息， 哪怕是多个客户端也会只使用1个队列，这个模型要尽量保证插入消息的顺序，而且还要支持按端查找未读，并且还要可以及时清理所有端都拉取过的消息，目前没有一个现成的数据结构可以支持timeline模型，但是阿里有个timeline模型的产品叫openStore。我们也没有使用这个模型，因为我们只有一个端。

##### 15.sequenceId和消息id

sequenceId用来排序消息，为什么不用消息id呢？因为消息id是客户端批量拉取的，所以消息id并不能作为排序的标准。

单聊的sequence使用聊天双方的id+发送消息的时间戳，群聊的sequence由分发层统一赋值。
