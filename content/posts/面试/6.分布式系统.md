---
title: 面试-6.分布式系统
tags:
  - 面试专题
categories: 面试专题
copyright: true
---

##### 一 认证授权

###### 1.没有cookie的话session还能用吗

cookie的主要作用是保存sessionId，如果没有cookie比如浏览器禁用cookie可以把sessionId保存到别的地方，通过url拼接或者其他参数传过来，如果不想暴露sessionId可以加个密。

###### 2.分布式session方案

1.使用spring session + redis。2.jwt。

###### 3.什么是jwt

jwt是基于token的跨域认证解决方案。jwt由3个base64编码字符串以'.'拼接组成：

*   header：定义签名算法、token类型。
*   payload：实际数据。
*   signatrue(签名)：使用header中的签名算法处理header+payload+秘钥生成。

jwt是无状态的，服务端不需要维护，但是无状态就有个问题，jwt一旦生成，在没过期之前都是有效的，比如退出登录、注销用户并不会让旧jwt失效，解决的办法可以在redis里维护一个jwt黑名单，或者使用用户的某些信息作为生成token的秘钥，一旦注销或者退出登录就修改这些信息，这样就jwt就会自动失效。

###### 4.如何防止jwt被篡改

当秘钥没有泄露的时候，如果header和payload被篡改那么生成的签名是不同的，这个时候token验证不会通过，所以要想jwt不被篡改就要保证秘钥不能泄露，同时payload中要包含过期时间，不能让token永久有效。

###### 5.单点登录

单点登录就是登录过一个系统后就有权访问其他相关系统，比如登录淘宝之后可以直接跳转到天猫不需要重新登录。

SSO可以减少用户操作流程，统一的认证中心也让系统开发更方便。

###### 6.如何实现一个SSO系统

单点登录系统不需要维护用户信息，他必须得功能只是生成token、校验token的有效性、解析token。当用户登录成功之后从当前服务端调用SSO系统，验证也是在网关里调用SSO系统。

如果想要实现注销、退出登录时token自动失效需要再数据库维护一个用户id和token的对应关系，存储失效的用户的token，可以定期删除。

###### 7.什么事OAuth 2

OAuth 2是一种授权机制，用来给第三方应用颁发一个有时效的token，常见支付平台、开发平台、或者公司的系统调用内部的认证平台。

##### 二 分布式系统

###### 1.为什么要把系统拆分成分布式的？

首先如果是单块系统，随着代码越来越多，以后想要修改很容易改出问题，模块之间耦合越来越高，多人开发一个项目，很容易不小心操作别人的表，或者把别人的代码改出问题，如果代码提交到远程仓库发生冲突也不好解决，还有一个，比如说某个模块并发很高，需要加机器，但是代码都在一起，不能单独把摸个模块多部署几台机器。

###### 2.CAP和BASE理论

CAP中的C表示强一致性，A表示高可用，P表示分区容错行，CAP理论说的是分布式系统中只能保证CP或者AP，但是不会有CA。

BASE理论说的是最终一致，软状态和基本可用。软状态说的是不会影响结果的数据的中间状态，比如两个服务都可以查询数据，但是现在网络出现了问题导致一台服务查询不出来。当网络恢复后软状态消失数据还是会最终一致；基本可用说的是当系统出现不可预知的问题的时候可以牺牲一些可用性，比如响应时间增加，或者牺牲掉一些功能。

###### 3.分布式事务

*   两阶段提交/XA：一般用于一个系统去操作多个数据库，在系统中有个事务管理器的概念，第一个阶段由事务管理器去确定要操作的数据库是否都能成功，如果是，第二阶段就执行操作。在微服务中这种方案是不会被用到的，微服务里每个服务只操作自己的库，不会出现这种情况，操作别的数据库都是调别的服务的接口。
*   TCC：TCC就是引入了补偿的概念，需要手动实现回滚的代码，某个服务操作失败后将原来插入的数据删除，删除的数据重新插入，实现的逻辑非常复杂，一般用在保证业务绝对不能出错时会使用这种方案，比如资金交易。
*   本地消息表：这也是一个不太常用的方案，分布式事务的系统都会有一张消息表，比如A系统中调用B系统，A会在逻辑处理结束之后在消息表里插入一条待确认的消息，然后去访问B系统，可以使用mq来访问，B系统在收到消息之后会先插入进消息表，这时候如果消息表已经有了就不会去消费，然后开始业务处理，如果处理成功会调用A系统的接口或者使用zk协调，A系统收到通知后再去修改消息表的状态为完成，A系统会轮询查消息表，找到超时没完成的业务数据在发送给B系统，不断的去重试，不适合高并发，直接写到库里扛不住多少QPS；
*   可靠消息最终一致性：根据本地消息表改的，只使用mq，去掉了本地消息表，RocketMQ天然支持。可靠消息说得是提交时可靠和rocketmq的高可用，最终一致性说的是失败不会回滚，只会不断重试，A系统首先会提交一条prepared消息到rocketmq里去，在处理完业务后提交一个confirm消息，如果rocketmq长时间没收到confirm消息会去回调A系统，A系统可以选择重新发送或者回滚prepared消息，B系统收到消息后首先要保证幂等，如果消费失败可以回调A系统让他重发或者不提交offect，在重试一定次数还是失败需要标记或者持久化等待人工补偿。
*   最大努力送达方案：在mq后面可能有一个通知服务，甚至都不需要这个服务，消费的服务只要尝试最大次数就可以了，对一致性要求不严格的话可以使用；

###### 4.核心链路的事务

针对用户可能在公众号充值和在app消费的情况，使用seata的AT事务(画图)，发送漂流瓶同步到动态使用自研的最终一致性框架(画图)。

###### 5.seata的空悬挂、空回滚问题

在seata的tcc事务里，空悬挂说的是try方法的空悬挂，空回滚说的是cancel方法的空回滚，比如在执行try方法时突然卡柱，seata server拿到try方法超时通知执行cancel方法，cancel方法并没有要回滚的数据，出现空回滚，cancel执行完try方法继续执行，此时出现空悬挂产生脏数据的问题。

这个时候可以记录try方法的执行状态，如果没执行完就执行cancel方法就在数据库里插入一条空数据，当try方法操作数据库前发现这条空数据就表示这次事务已经失败，删掉这条空数据结束方法就可以了，同时当confirm或者cancel方法超时的时候seata server会不断发起调用，这个时候需要做好幂等处理。

###### 6.分布式id

分布式方案主要有以下几种：

*   基于单台数据库单张表主键自增，不能抗住并发。
*   雪花算法：第0位没意义，1-41位表示时间戳，接着5位机房(集群)id，接着5位机器id，最后12位自增，一台机器每ms可以产生4096个id，他只能保证同一台机器的id自增，就是局部自增，他还有时钟回拨问题。
*   UUID：很长的字符串，浪费空间，不是自增，存到MySQL中会频繁页分裂。
*   flicker及其变种：flicker也是基于数据库，要求是个集群，表中有最少2列，id和stub，stub可以代表项目也可以代表业务，通过replace和select last_insert_id获取新的id，这个方案要求自增的步长非常大，比如10000，客户端每次拿到最大id，然后用最大id减掉步长作为最小id，在内存中使用LongAdder这种自增，快用完的时候再去数据库获取，这个方案不支持扩容，并且如果号段没用完就重启造成了号段浪费。
*   redis自增：他和flicker一样的问题。
*   业务+时间戳：这个方案最省事，优先考虑。

雪花算法的时钟回调问题：把前半个小时每毫秒每台机器生成的最大的id保存到起来(比如map)，同时需要记录最后一次id，当本次生成的id比上次的id小说明发生时钟回拨，如果发生回调，找到之前生成的最大id继续开始加，每ms都是，直到map里没有说明时间正常了，从1开始加。

雪花算法实现整个分布式全局递增：需要全局递增的业务要保证跨服务每次调用到相同的发号器服务，从nacos获取发号器所有地址，根据业务+dubbo自定义负载均衡路由到相同的发号器。

###### 7.网关的技术选型

我们系统里的网关主要负责请求转发(动态路由)、token认证、用户状态过滤等。

*   zuul：使用简单，通过自定义过滤器扩展，1版本使用同步IO扛不住并发，2版本好像是使用netty了。
*   kong：基于nginx+lua，这就相当于一个简单的网关，和系统交互麻烦但是抗高并发能力很强。
*   gateway：gateway比zuul2更早，也是基于netty，也是自定义过滤器(实现GlobalFilter接口)实现扩展，但是他基于webflux开发，目前不支持数据库的动态路由。
*   shenyu：他也是基于webFlux开发的Apache开源网关，自带控制台，自带治理功能，使用SPI扩展。

当时考虑我们所有的服务都是统一使用Sentinel进行限流熔断，统一使用sentinel的控制台，所以选择gateway。

###### 8.怎么实现动态路由

首先通过ConfigService监听Nacos上的实力变化，当发生变化的时候发布RefreshRoutesEvent事件，gateway会消费这个事件更新路由。

###### 9.接口幂等性怎么保证？

出现重试有可能重复下单，首先插入逻辑最好有唯一索引保证，比如扣款记录中的订单id，这样能保证不重复扣款，如果要做统一幂等可以写一个接口执行结束后的拦截器，把post和put请求的所有参数都拼在一起当成一个key，去redis中查找，如果有结果就是重复访问，但是这样实现会有问题，解决不了超时重试的重复问题，而且我项目里并不是所有接口都要做幂等，所以我是在具体的接口中单独耦合的幂等，进入接口的时候redis中set数据，如果失败了就删除redis中的数据然后回滚，成功了就不管。

###### 8.分布式系统中的接口调用如何保证顺序性？

可以在被调用接口之前加一个接入服务，这个服务负责分发，把需要顺序执行的请求分到同一机器上。

###### 9.分布式锁

Redis：项目中用户钱包金额的查询和扣减都是用了分布式锁，因为在下单时，选择钱包抵扣是需要判断钱包余额的，支付失败的回调接口是反向扣减余额，redis分布式锁原理是通过setNx+过期时间实现，业务中使用try with resources封装的redisson，并且不指定持有锁时间，因为不指定时间redisson可以实现锁的自动续期。redis集群挂掉对分布式锁是有影响的，因为主从同步是异步的，很有可能还没同步数据，master就挂了，这时候别的线程也可以加锁了，官方推荐的是redLock，给集群中独立的redis进行setNx，超过半数成功就算成功，这种方式性能不好。

zk：使用zk加锁一般都用curator创建临时顺序节点加锁。zk和客户端出现网络问题或者脑裂还是会可能出现重复加锁问题。

如果想要抗高并发可以使用kv存储，不加锁，像redis、mongodb比如说库存，直接扣，如果扣完发现小于0就把数据加回去，然后返回库存不足。

###### 11.服务注册中心进行过技术调研吗？如何选型

zk只有leader能写，每个服务都监听zk上的服务列表，当服务列表有变化的时候zk会通知其他服务服务列表变更。eureka是一种peer to peer模式，每个节点都可以读写，zk是leader和follower这种。

一致性保障：zk选择了cp，保证强一致性和分区容错性，当leader挂了的时候，服务是不能访问的。eureka选择的ap，任何一个eureka挂掉了是不影响其他服务访问的，但是经过一段时间的心跳、注册、拉取注册表会达到最终一致性。

时效性上：zk的时效性很快，eureka的时效性非常慢，可能需要几十秒或者几分钟。

容量上：zk和eureka都不适合大量的服务，由于zk的时效性很高，如果服务变更会就会让zk回调大量的服务，可能瞬间带宽就被打满了，eureka相对来说好一点，当服务发送心跳时，他基于了三层内存队列来减少和其他eureka通信的资源消耗，只需要处理瞬间大量服务的心跳就可以了。

###### 12.nacos协议

nacos支持distro(ap+最终一致性)协议和raft(弱cp)协议

*   distro协议：注册的时候随机选择nacos节点，由转发节点根据ip+port计算目标nacos节点并转发，目标nacos节点把服务信息保存到内存然后通过转发节点返回结果，然后目标nacos节点再把刚才收到的服务信息发送给所有nacos节点；所有nacos节点都会定时向其他节点发送心跳和自己保存的服务信息，当其他节点发现两个服务信息不一致的时候会全量拉取；所以distro协议是最终一致性的，订阅节点信息时也是随机选择一个nacos节点，如果这个nacos节点发现没有这个服务实例信息回添加一个回调，等到实例信息同步完成回调订阅这个实例的服务。
*   raft协议：选出主节点，只能在主节点上写，写的时候会同步到从节点，当超过半数从节点同步成功就算成功，所以他是弱一致性，这个性能没有distro好。

###### 13.http和rpc的区别

首先他俩都是基于应用层的协议，rpc更倾向于调用方式而不是协议，他的协议可以定制，定制出来的header要比http小很多，序列化的方式也要比http好，http应该只支持json序列化，而且rpc不需要考虑那么多的状态码，一般用于系统内部的服务调用。

##### 三 dubbo

##### 1.dubbo工作原理

![dubbo主要工作流程](https://raw.githubusercontent.com/wangxiaohong123/p-bed/main/uPic/dubbo主要工作流程.png)

###### 2.dubbo支持哪些通信协议和序列化协议？

dubbo3.0之后应该只剩下hession2和jdk了。之前还有protobuf、json这些。

通信协议有dubbo、triple、grpc。

###### 3.dubbo底层的网络通信原理

![dubbo网络服务架构](https://raw.githubusercontent.com/wangxiaohong123/p-bed/main/uPic/dubbo网络服务架构.png)

##### 4.dubbo怎么保证组件的扩展性

内部组件和组件之间的调用，通过ScopeModel统一管理，必须全部是依托于接口，根据SPI动态查找配置或者默认的实现类。

###### 5.SPI是啥，dubbo的SPI机制是怎么玩的？

SPI是服务提供者接口，java内置的SPI的实现会把所有实现类全部加载返回一个集合，dubbo自己实现的SPI可以实现按需加载，还有IOC功能。

###### 6.dubbo支持哪些负载均衡、高可用以及动态代理策略？

负载均衡策略：默认的带权重随机；轮询策略；自动感知性能不好的机器就减少分发；一致性hash，把参数相同的请求分配到同一台机器；

集群容错策略：

*   failover（默认）：访问失败到一定次数去尝试别的机器；
*   failfast：只调用1次，失败就抛出异常；
*   failsafe：出现异常时忽略掉，并打印日志；
*   failback：出现异常自己记录，定时重发；
*   forking：一起调用多个机器，有一个成功就算成功；
*   broadcacst：逐个访问所有服务提供者；

动态代理策略：默认使用javassist生成字节码动态代理，可以使用SPI实现自己的动态代理策略。

###### 7.如何设计一个类似dubbo的rpc框架？架构上该如何考虑？

首先要有一个服务注册中心，比如zk、nacos或者自己实现一个；

上游服务和下游服务都需要实现接口的动态代理，rpc的所有核心逻辑在这个动态代理中实现；

还需要一个cluster层，获取本地注册表；

然后再来一个负载均衡组件，有多少种策略；

选择好了机器，知道了ip、端口号，调用哪个接口的那个方法，把这些信息交给协议层；

把数据组织一下，采用什么协议、序列化的方式；

收到请求之后就是反序列化，反序列化之后就知道调用自己那个接口的那个方法，通过反射去调用；

###### 8.生产环境的dubbo配置

超时时间(timeout)是1s，重试2次，dubbo优先考虑没有调用过的provider，他也没有配置autoRetriesNextServer这种参数，像权重、负载均衡策略这种参数根据业务定。

##### 四 sentinel

##### 五 Zookeeper

###### 1.说说zookeeper一般都有哪些使用场景？

*   分布式锁：多服务要保证绝对顺序可以使用zookeeper并在设置锁时添加顺序参数，消息的重复消费也可以使用zk；
*   分布式协调：比如一个服务发送了一个mq消息，但是他在后面可能需要知道消费者有没有消费完，这个时候消费者可以更新zk上的节点值，这样发送mq消息的服务监听了zk节点就会感知到消费者已经消费完这个消息了。
*   保存元数据信息：比如kafka的节点信息；
*   HA高可用：比如一个服务部署两个节点，只有一个使用，另一个容灾，运行的节点会在zk上创建一个临时节点，容灾节点在zk上注册监听临时节点，一旦运行的节点挂掉，zk会把临时节点删掉，容灾的机器监听到zk上临时节点没有了就会重新创建一个临时节点，这时候容灾机器就变成了运行的机器，当挂掉的节点重新启动去zk上看到了临时节点，就会把自己变成容灾节点；

###### 2.什么叫做zk的羊群效应

所有要获取锁的节点都监听现在持有锁的节点，一旦释放锁，会反向通知所有监听的节点，然后所有监听的节点又去抢这一把锁，存在很多不必要的网络开销，解决办法就是使用curator创建顺序节点。

##### 六 系统设计

###### 1.设计一个高并发的系统

高并发是一点一点演进的，并不是刚上线就有几万的QPS；

第一件事是系统拆分，系统拆分就会带着数据库拆分。尽量保持一个服务几万行代码或者一个人维护一到两个服务；

第二件事是设计缓存，缓存可以轻松抗几万并发，只要注意缓存数据库双写一致就可以；

第三件事设计MQ，MQ保证肖峰和让系统响应更快；

第四件事如果数据库还是很吃力需要分库分表，在操作数据库之前加一个异步服务去分发请求到不同的数据库；

第五件事如果读还是有很多走不了redis，比如LRU淘汰或者写多的情况，可以加几台es集群；

###### 2.设计一个高可用系统



###### 3.生产系统的连接池优化

durid举例：maxWait不能为0，0的意思是无线等待，高并发的时候可能一下就都在这等，就卡死了，一般在1200，最多让他等待一秒多点。connectionProperties中设置connectionTimeout是1200，socketTimeout是3000，意思是连接1200ms超时，发送语句后3s超时，这个是防止网络问题导致的一直重试，配置这个超时了会断开连接重连。maxActive设置成20，太多容易CPU负载过高。

###### 4.设计朋友圈系统

首先说一下图片或者视频处理，可以走一个异步发送，点击发送先在本地缓存，这样发朋友圈的人可以发完就看到自己刚发的朋友圈，然后朋友圈的媒体传到就近的cdn上，我们之前用的OSS，然后在保存朋友圈。后台要有一个朋友圈表和相册表，相册表用来实现查看自己相册功能。然后会有一个异步处理任务，还要有一张朋友圈时间线的表，里面放可以查看的朋友圈id，异步任务会把最新的圈子信息拿出来，根据好友关系插入到个人的朋友圈时间线中。

权限设置，可以在本地缓存，如果用户修改权限，这时候在刷新朋友圈的时候需要判断，根据权限是否显示朋友圈。

点赞可以放到set中，这样还能看到谁点赞了，然后个人的点赞放到score set中，在刷朋友圈的时候，需要在后台处理共同好友，比如用sismember。

###### 5.并发扩大十倍怎么扩容

有些服务直接加机器就会自己注册到服务注册中心，其他服务拉取注册表后就可以正常分担流量了，但是网关加机器需要修改nginx的负载均衡策略，还有的机器中包含消息队列的消费者，可能涉及到顺序消费，需要修改producer的分配messageQueue算法，像eureka这种横向扩容无效的服务可以加机器配置，数据库的话，如果数据量不大也可以加机器，如果并发高了之后数据量也上来了就要分库分表了。



