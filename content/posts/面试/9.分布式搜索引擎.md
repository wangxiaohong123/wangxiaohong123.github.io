---
title: 分布式搜索引擎
tags:
  - 面试专题
categories: 面试专题
copyright: true
---

##### 1.es原理(es是如何实现分布式的)

首先每个节点都有primary shard和replica shard，写数据只能写到primary shard上，读数据可以从primary和replica shard上读。

写数据的时候会随便选择一个协调节点，协调节点通过hash值把请求路由到具体的primary shard所在机器，然后primary shard把数据写到内存的indexing buffer，同时在OS cache中记录trans log，然后同步数据给replica shard，这个时候就可以返回成功了，但是数据还不能被查到。
每秒会有一个refresh操作：再OS cache中新建一个segement file，把indexing buffer刷到segement file中并建立好倒排索引，然后清空indexing buffer，这个时候数据才能被查询到，所以es是准实时的；OS cache中的trans log每5秒会被刷到硬盘中，所以默认情况下es可能会最多丢失5s的数据；当trans log太大或者每半个小时会执行一次flush操作，记录一个commit point然后把trans log清空。
删除的时候会把删除的数据写进del文件，当segement文件太多之后会进行merge操作，把segement文件合成一个大文件，这个时候会把del文件中的数据物理删除。

![es读写数据原理](https://raw.githubusercontent.com/wangxiaohong123/p-bed/main/uPic/es读写数据原理.png)

查询的时候也是根据id计算出数据在那个shard上，然后找对应shard查找，搜索的时候会查找所有shard把符合条件的document的id都返回到协调节点上，然后协调节点在根据doc id去查找对应document再进行筛选，最后返回结果。

##### 2.在大数据量的情况下如何提高查询效率

es的查询优化是没有银弹的！

首先在查询数据的时候，会先去cache中找，如果没有再把磁盘的数据读到filesystem cache中，然后把结果返回，下次再来查找的时候会直接把cache中的数据返回，所以filesystem cache的大小对查询的速度影响是最大的。如果走磁盘的话基本就上秒了。所以要把更多的内存给cache，并且尽可能减少es的document的field大小，就把要用来检索的字段存到es中。检索出来的数据的id再去MySQL或者hbase中查完整的数据。

如果数据量还是很大，可以统计热数据，每隔一段时间主动查询一下热数据，让热数据一直在OS cache中。

在模型设计的时候要避免多次查询，一些join操作把涉及到的数据存到一个document中。

他的分页性能非常差，需要把数据都拿到协调节点上，然后排序，在分页操作，比如查询10000\~10010的数据，他会从每个shard中都查询出来10010条数据进行排序然后找到10000\~10010的数据，所以翻页越深，性能越差。如果是上拉加载更多那样的可以用scroll api。

##### 3.生产环境的es部署架构？每个索引有多大？每个数据多少个分片？

我们生产环境部署了3台16g机器，48g的总内存，es进程每个不到8g，OS cache差不多有20多个g。日增数据量差不多八十万条，日增的数据大约120多兆，每个月增量数据大概两千多万，2g左右，两个索引，每个索引6个shard。

##### 4.分词器的二次开发

下载ik分词器源码，然后添加MySQL依赖，然后编写MySQL连接代码，还有查询分词、停用词代码，最后在Dictionary类中添加一个addStopWord()方法，不断查询MySQL调用Dictionary的addStopWord()和addWords()方法就可以了。