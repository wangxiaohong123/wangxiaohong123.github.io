---
title: 面试-4.MySQL
tags:
  - 面试专题
categories: 面试专题
copyright: true
---

##### 1.MyISAM和InnoDB的区别

MyISAM不支持事务，不支持外键，索引文件和数据文件分开，并且他没有行锁。

InnoDB有自己的redo log，他要求必须要有一个自增的主键，如果我们没有指定他会自己生成一个，默认情况会有一个根据主键的B+树索引，叫做聚簇索引，这个索引的叶子节点就是数据文件。

##### 2.索引为什么不用二叉树、平衡二叉树、B树，而是使用B+树

二叉树容易造成节点不平衡，影响查找速度；就算他改成了平衡二叉树，深度太大也会影响查找速度；B树的子节点也会存储数据，因为MySQL的数据页是16k固定大小，如果子节点存储数据，那么他能存储的子节点个数就会变少(子节点的个数也叫阶数)，也会影响深度，B+树就是让这个索引结构更矮更胖，深度减少相对的磁盘IO就会减少。

B+树种的叶子节点有两个前后指针，让叶子节点组成了一个双向链表，这像排序和范围查也更快。

##### 2.1 B+树能存储多少数据

以深度为3的B+树举例，首先算出第二层子节点的个数=16k / (6b + 8b)，其中6字节是指针大小，8字节是表的主键为bigint类型，得到第二层子节点的个数是1170个，这1170个子节点都会有1170个叶子节点，能够得到叶子节点的个数=1170*1170，假设一行数据是0.5k，如果不考虑数据页的其他信息，每个数据页能够存储16k / 0.5 = 32条数据，所以深度为3的B+树能够存储1170 * 1170 * 32 = 4380万条。

##### 3.事务

事务的特性(ACID)原子性、一致性、隔离性、持久用。可以理解成用原子性+隔离性+持久性来保证一致性，也就是使用AID来保证C。

###### 1）什么是脏读、脏写、不可重复读、幻读

*   脏写：事务A把id=10的数据对应的列x改成1，此时事务B对id=10的数据对应的列x改成2然后提交事务，然后事务A在执行后面的业务时发生异常把id=10的数据对应的列x回滚成了初始值0，事务B就发生了脏写。
*   脏读：事务A先读取一次id=10的数据对应的列x，然后去处理其他业务，此时事务B把id=10的数据对应的列x的值改成NULL，但是没有提交事务，然后事务A又回来读取一次id=10的数据对应的列x发现和之前的值不一样，事务A发生了脏读。
*   不可重复读：他和脏读的情况很像，只不过事务B提交了修改id=10的数据对应的列x的事务，此时事务A两次读到的数据是不一样的，我觉得这种情况可以算问题，也可以不算。
*   幻读：由于其他事物增加或者删除了一些数据，导致在同一个事物中前后两次查询的结果条数不一致。

###### 2）事务的隔离级别

*   读未提交：要求可以读到没提交事务的数据，脏读、不可重复读、幻读的问题还在。
*   读已提交：要求不能读到没提交事务的数据，会有不可重复读和幻读的问题。
*   可重复读：要求同一事物中多次读取同一条数据一致，会有幻读问题，MySQL里可重复读级别没有幻读问题！
*   串行化：只能有一个线程操作数据库。

###### 3）MySQL默认的隔离级别及原理

默认使用可重复读，在MySQL中每个事物的事务id是全局递增的。MySQL使用readView+undo log实现了MVCC(多版本并发控制)机制，在readView中有当前操作这个数据的事务id集合，还有集合中的最大、最小事务id，还有当前事物id，读取数据时会从undo log链里找到第一个创建数据的事务id比当前事务id小的undo log去读取数据；修改的时候会插入一条新的undo log，然后会增加独占锁。这样会保证读取数据的事务不会读到比自己id大的事务操作后的数据。

在MySQL中每条数据会有两个隐藏列，一个是插入数据的事务id，还有一个删除数据的事务id。

###### 4）项目里基于Spring的事务是怎么做的？事务传播是怎么设置的？

Spring的事务有声明式和注解式，现在使用的都是注解式，在需要开启事务的方法上增加@Transactional注解，一般传播属性使用的默认的required_new就可以。如果有业务需要考虑某块代码不需要回滚之类的根据业务设置。

##### 4.优化SQL的步骤

互联网公司的SQL都不会出现join这种，一般都是很简单的SQL，因为SQL的资源很宝贵，可以把复杂查询的操作放到业务里处理。

*   测试前会统一创建索引，索引不超过4个，多了会影响增删改性能，保证每个SQL都是使用索引，注意索引失效情况(最左匹配原则、查询条件使用函数或者计算、有些时候or、not null、不等于这些关键字也会染索引失效)。
*   查询时返回的字段尽可能少，第一是为了减少回表(如果查询的字段都在非聚簇索引中就不需要回表)，第二是减少占用网络带宽。
*   平时需要观察慢SQL，找到对应的慢SQL使用explain分析，保证type在index级别一下，rows尽可能小。

##### 5.in和exists的区别

比如select * from a where id in (select a_id from b where ……)会先执行后面in条件中的语句，select * from a where id exist (select a_id from b where ……)exists先执行前面的语句。

##### 6.锁，出现死锁了怎么办

行锁、表锁（MyISAM用的多）InnoDB的表锁是表级意向锁、页锁，查询数据时指定悲观锁是在后面增加 for update，指定添加共享锁需要再语句后面增加lock in share model。

发生死锁时，show engine innodb status，找到latest deadLock分析原因，测试环境重现之后修改问题，一般出现死锁时MySQL会自动回滚一个事务，如果死锁没有自动解决的话可以通过show processList找到对应的进程，kill掉。

##### 7.分库分表

###### 1）为什么要分库分表

分库分表是两个概念，并不一定同时存在。

当并发量很高，并且写请求跟多，肖峰之后还有几千的请求打到MySQL上，或者很多写请求对实时性要求比较高的时候需要分库，分担写的压力，当单表数据量达到几百万条的时候需要分表。

###### 2）分库分表的中间件

client层的sharding-jdbc（当当开源的）支持很多语法，分库分表，读写分离，分布式事务，现在shardingSphere也支持proxy层了；
proxy层的mycat，cobar改造的，功能也很全；
client层的优点是不需要独立部署，缺点是升级很麻烦，需要涉及到的系统重新发布，耦合度很高。中小型公司使用sharding，维护成本更低。

分库分表之后需要对业务进行大量改动，比如会出现垮裤跨表的复杂差查询或者分页，需要借助es或者clickHouse。

分库分表之后需要一个控制台来管理数据库的DDL操作，比如分了128个库，当有新需求的时候不可能手动去128个库里创建表，同时还要监听这些库表的状态，比如总共多少条数据、每张表多少条数据、监控状态、当需要迁移表的时候也要在控制台执行对应的脚本。

###### 3）数据库拆分

垂直：一张表的列拆分出来，数据的行数不变，把访问量多的列放到单独的一张表里，因为数据库的查询是要把数据读到缓冲池中，一行数据占的小可以让buffer pool多存写数据，减少刷盘。

水平：表结构不变，把不常用的数据拆到其他表中，能让查询更快。拆出来的表可以在一个库里叫不同的名字，也可以在不同的库里或者同时多个库，每个库里多个表。分库分表说得也是水平拆分，垂直拆分实在数据库设计的时候直接拆出来的。

###### 4）不停机单库单表迁移到分库分表

不停机迁移方案首先要有几个系统：数据迁移系统+canal系统+数据同步系统。canal程序监听新系统和旧系统的binlog，旧系统的binlog发送到数据迁移系统，新系统的binlog发送到数据同步系统用来写入es，数据迁移系统会全量查询就数据库的数据，同时还要运行一个数据对比程序，当新老数据库数据完全一致时修改nginx把请求转发到新系统，停掉老系统和数据迁移系统：

![全量+增量串行数据迁移](https://raw.githubusercontent.com/wangxiaohong123/p-bed/main/uPic/全量+增量串行数据迁移.png)

###### 5）动态扩容缩容分库分表方案

最简单的：数据库设计时就设计好16个库，每个库里16张表，或者32个库，都可以，一定要是2的倍数，这样才好扩容，比如刚开始有四台数据库服务器，每个服务器上4个库，每个库16张表，如果并发很高，比如一万并发，因为一台数据库服务器也就抗个2000左右的并发，这时候就需要更多的数据库服务器，或者数据量很大，单表一两千万，但是4 * 16张表的数据加起来让服务器磁盘满了，这两种情况就需要扩容了，扩容也很简单，在加四台机器，把每个机器抽出来两个库，放到新的服务器中就可以了，这里只需要修改分库分表中间件的路由方案即可，比如原来路由是和4取模，现在就需要跟8取模。

##### 8.读写分离

###### 1）原理

什么时候会使用主从数据库：一般就是大量读的情况，比如redis刚启动，没有预热数据，这时候大部分请求会打到数据库上，或者并发量很高，redis放不下数据频繁的LRU淘汰，也会有大量的请求打在MySQL上。

MySQL原生就支持主从复制，主库和从库的后台都有个IO线程，从库去请求主库，然后主库读出来binlog日志给从库，从库把日志写到relay日志，还有一个专门的线程根据relay日志变更数据。整个过程是串行的，**可能会出现主从延迟问题**。

###### 2）主从同步延时问题

从库的延时是跟主库的写并发有关，1000并发可能会有几毫秒的延时。

在从库上执行show status可以看到有一个seconds behind master，表示同步落后了主库多长时间；

如果是并发不高，只是丢数据的问题，可以开启semi-sync机制，这个机制是说至少有一台从库确定写入了relay log之后才算成功，这样可以保证数据99.99%不丢，主要就是保证延时导致的数据丢失。

如果是因为高并发首先要开启并行复制（5.7以后），这个东西作用不大，因为他是从库开启多个MySQL线程，每个线程对应一个库，根据28法则，高并发的时候大部分的请求可能都落在一个库中，但是开启这个复制多少会有点作用，如果真碰到读写分离出现延时导致数据出问题，先去修改代码，避免主库写完从库就去读，如果解决不了就分库分表，或者就个别接口是这样的，那么就让这个接口的读直连主库。

##### 9.mysql是怎么实现事物的

首先事物有4个特性，原子性，隔离性，持久性，一致性，个人觉得原子性+隔离性+持久性是为一致性服务的。

*   mysql中通过undo log实现的原子性，当数据库没有宕机的情况下想要回滚直接读取undo log即可，如果发生宕机但是事物没有提交，此时undo log的状态时uncommited，也会保证数据的原子性。
*   通过mvcc机制+锁实现的隔离性，mvcc可以实现多个线程读和一个线程写多个线程读情况下的隔离性，如果出现多个线程写就需要通过锁来控制只能有一个线程写。
*   bin log + redo log + 双写缓冲区实现持久性，其中bin log + redo log通过两阶段提交实现刷盘，双写缓冲区是指当缓存中脏页刷盘时会先同步写入双写缓冲区，在异步更新数据文件，这样可以提高性能，由随机写变成顺序写+异步随机写，同时如果刷盘时服务器宕机虽然双写缓冲区文件损坏，但只是少量数据，可以通过redo log文件进行修复，如果没有双写缓冲区就是数据文件损坏，需要通过bin log + redo log进行修复，很麻烦。

##### 10.索引失效

*   查询条件或者排序、分组条件中使用函数
*   不符合最左匹配原则
*   索引时数字格式的字符串，但是查询条件没有使用引号
*   表的数据量太小

其实就是数据库引擎会预估这条sql的执行计划成本，比如把1个数据页加载到内存需要消耗1，在内存中计算条件、排序什么的消耗0.2，通过计算使用2级索引的成本和全表扫描的成本比较，决定是否使用索引。
