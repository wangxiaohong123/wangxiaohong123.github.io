---
title: 8.自然语言处理-nlp
tags:
  - 深度学习
categories: 人工智能
copyright: true
---

标准数据集GLUE：由纽约大学、华盛顿大学和google联合推出的涵盖11个子任务的NLP数据集。

文本的标签太多，打不完；文本模型不是聚焦某个场景，更重要的是培养模型的学习能力，自监督更合适；有监督容易过拟合，不能做迁移学习，适合做一些针对性的事，自监督适合解决领域通用的问题。

### 一、文本预处理

#### 1 基本方法

##### 1.1 分词

精确模式：把句子精确的切开，适合文本分析。

```python
import jieba

content = "工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作"
# cut_all是False表示精确模式
print(jieba.lcut(content, cut_all=False))

# ['工信处', '女干事', '每月', '经过', '下属', '科室', '都', '要', '亲口', '交代', '24', '口', '交换机', '等', '技术性', '器件', '的', '安装', '工作']
```

全模式：把句子中所有可以成词的词语都扫出来，不能消除歧义

```python
import jieba

content = "工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作"
# cut_all是False表示精确模式
print(jieba.lcut(content, cut_all=True))

# ['工信处', '处女', '女干事', '干事', '每月', '月经', '经过', '下属', '科室', '都', '要', '亲口', '口交', '交代', '24', '口交', '交换', '交换机', '换机', '等', '技术', '技术性', '性器', '器件', '的', '安装', '安装工', '装工', '工作']
```

搜索引擎模式：在精确模式的基础上对长词再次进行切分，提高召回率。

```python
import jieba

content = "工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作"
print(jieba.lcut_for_search(content))

# ['工信处', '干事', '女干事', '每月', '经过', '下属', '科室', '都', '要', '亲口', '交代', '24', '口', '交换', '换机', '交换机', '等', '技术', '技术性', '器件', '的', '安装', '工作']
```

##### 1.2 命名实体识别(NER)

将人名、地名、机构名等专有名词称作命名实体。

##### 1.3 词性标注(POS)

以语法特征为依据，对词进行划分，就是划分出名词、动词啥的。

```python
import jieba.posseg as pseg

content = "工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作"
print(pseg.lcut(content))

# [pair('工信处', 'n'), pair('女干事', 'n'), pair('每月', 'r'), pair('经过', 'p'), pair('下属', 'v'), pair('科室', 'n'), pair('都', 'd'), pair('要', 'v'), pair('亲口', 'n'), pair('交代', 'n'), pair('24', 'm'), pair('口', 'n'), pair('交换机', 'n'), pair('等', 'u'), pair('技术性', 'n'), pair('器件', 'n'), pair('的', 'uj'), pair('安装', 'v'), pair('工作', 'vn')]
```

#### 2 张量表示

让语言类的文本可以作为计算机处理程序的输入。把词转成向量，也叫词嵌入。模型输入的维度=batch \* 句子长度 \* 词向量长度。因为多个词可能有相同的意思，将词转成词向量时要保证相似意思的词语的向量也接近。

##### 2.1 one-hot

独热编码，每个词都表示成有n个元素的向量，这个词向量中只有1个元素是1，其他元素都是0，n表示整个语料中不同词汇的总数。很耗内存，而且他体现不出词与词之间的联系，适合枚举类型不多的字符串，比如星期几。

```python
from tensorflow.keras.preprocessing.text import Tokenizer


vocab = {'周杰伦', '陈奕迅', '王力宏', '李宗盛'}
t = Tokenizer(num_words=None, char_level=False)
t.fit_on_texts(vocab)

for vc in vocab:
    zero_list = [0] * len(vocab)
    token_index = t.texts_to_sequences([vc])[0][0] - 1
    zero_list[token_index] = 1
    print(vc, "的one-hot编码：", zero_list)
    
    
王力宏 的one-hot编码： [1, 0, 0, 0]
陈奕迅 的one-hot编码： [0, 1, 0, 0]
李宗盛 的one-hot编码： [0, 0, 1, 0]
周杰伦 的one-hot编码： [0, 0, 0, 1]
```

##### 2.2 word2vec

需要通过模型，随机将单词变成向量 -> 通过模型计算损失 -> 反向更新权重和**向量**。预训练模型中不光有调好的权重，还有一些通用的词向量。Gensim是训练词向量的工具包。gensim实现word2vec：

首先将准备好的语料库分词

```python
import jieba
import jieba.analyse
import jieba.posseg as pseg
import codecs,sys
def cut_words(sentence):
    #print sentence
    return " ".join(jieba.cut(sentence)).encode('utf-8')


# 语料库文件
f=codecs.open('wiki.zh.jian.text','r',encoding="utf8")
# 分词之后的文件
target = codecs.open("zh.jian.wiki.seg-1.3g.txt", 'w',encoding="utf8")
print ('open files')
line_num=1
line = f.readline()
while line:
    print('---- processing ', line_num, ' article----------------')
    line_seg = " ".join(jieba.cut(line))
    target.writelines(line_seg)
    line_num = line_num + 1
    line = f.readline()
f.close()
target.close()
exit()
while line:
    curr = []
    for oneline in line:
        curr.append(oneline)
    after_cut = map(cut_words, curr)
    target.writelines(after_cut)
    print ('saved',line_num,'articles')
    exit()
    line = f.readline1()
f.close()
target.close()
```

建模代码：

```python
import logging
import os.path
import sys
import multiprocessing
from gensim.corpora import WikiCorpus
from gensim.models import Word2Vec
from gensim.models.word2vec import LineSentence
if __name__ == '__main__':
    
    program = os.path.basename(sys.argv[0])
    logger = logging.getLogger(program)
    logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')
    logging.root.setLevel(level=logging.INFO)
    logger.info("running %s" % ' '.join(sys.argv))
    # check and process input arguments
    if len(sys.argv) < 4:
        print (globals()['__doc__'] % locals())
        sys.exit(1)
    inp, outp1, outp2 = sys.argv[1:4]
    model = Word2Vec(LineSentence(inp), size=400, window=5, min_count=5, workers=multiprocessing.cpu_count())
    model.save(outp1)
    model.model.wv.save_word2vec_format(outp2, binary=False)
# 命令行执行，将分好的词建模，保存到wiki.zh.text.model
# python word2vec_model.py zh.jian.wiki.seg.txt wiki.zh.text.model wiki.zh.text.vector
# opencc繁体转简体命令
# opencc -i wiki_texts.txt -o test.txt -c t2s.json
```

#### 3 数据分析

##### 3.1 标签数量分布

理解为数据集句子的各个分类数量是否想差过大。

```python
import seaborn as sb
import pandas as pd
import matplotlib.pyplot as plt

# 设置图像风格
plt.style.use('fivethirtyeight')

# 获取训练集和测试集
train_data = pd.read_csv('data/train.csv', sep='\t')

# 获取标签数量分布
sb.countplot("label", data=train_data)
plt.title("train_data")
plt.show()
```

##### 3.2 句子长度分布

短文本和长文本的模型不同。

```python
import seaborn as sb
import pandas as pd
import matplotlib.pyplot as plt

# 设置图像风格
plt.style.use('fivethirtyeight')

# 获取训练集和测试集
train_data = pd.read_csv('data/train.csv', sep='\t')

# 增加长度列
train_data["length"] = list(map(lambda x: len(x), train_data["sentence"]))

# 长度数量分布
sb.countplot("length", data=train_data)
# 不展示横坐标
plt.xticks([])
plt.title("distribute_count")
plt.show()

# 长度分布图
sb.displot(train_data["length"])
plt.xticks([])
plt.title("distribute_length")
plt.show()
```

##### 3.3 词频统计与关键词词云

词云可以辅助评估语料质量，对违反预料标签含义的词进行修正，保证大部分的语料符合训练标准。

```python
import pandas as pd
import matplotlib.pyplot as plt
import jieba.posseg as pseg
from wordcloud import WordCloud
from itertools import chain


# 获取形容词列表
def get_abj(text):
    res = []
    for g in pseg.lcut(text):
        if g.flag == 'a':
            res.append(g.word)
    return res


# 绘制词云
def gen_word_cloud(word_list):
    # 微软雅黑字体，最多展示200个词，白色背景
    word_cloud = WordCloud(font_path="./SimHei.ttf", max_words=200, background_color='white')
    word_string = " ".join(word_list)
    word_cloud.generate(word_string)

    # 绘图
    plt.figure()
    plt.imshow(word_cloud, interpolation='bilinear')
    plt.axis('off')
    plt.show()


# 获取训练集和测试集
train_data = pd.read_csv('data/train.csv', sep='\t')
# 拿到正样本
p_train_data = train_data[train_data["label"] == 1]["sentence"]
# 提取形容词
train_p_abj = chain(*map(lambda x: get_abj(x), p_train_data))

# 生成词云
gen_word_cloud(train_p_abj)
```

#### 4 文本特征处理

增加一些普适性的特征或者对长度进行规范。

##### 4.1 增加n-gram特征

相邻的n个词或者字的共现特征，就是当前词出现和之前的几个词有关系。一般分为2元特征(bi-gram)和3元特征(tri-gram)。

```python
ngram_range = 2


def create_ngram_set(input_list):
    return set(zip(*input_list[i:] for i in range(ngram_range)))


input_list = [1, 3, 2, 6, 4, 9]
res = create_ngram_set(input_list)
```

##### 4.2 文本长度规范

模型的输入需要等尺寸的矩阵，因此在进入模型前需要对文本数值映射后的长度进行规范，对超长的句子进行截断，对短句子进行补齐。

```python
from keras.preprocessing import sequence

x_train = sequence.pad_sequences(x_train, cutlen)
```

##### 5 文本数据增强

一般都是将文本调用google翻译，在翻译回来，这样会产生和当前样本标签相同但是文本不同的句子，这种方法叫回译法。

如果句子很短有可能经过回译法之后得到和当前样本相同的句子，这个时候可以翻译成一个语言，在翻译成一个语言，在翻译成中文。

