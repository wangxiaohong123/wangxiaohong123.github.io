---
title: 1.机器学习介绍
tags:
  - 机器学习
categories: 人工智能
copyright: true
---

图灵在50年的时候提出机器思维：一个人在不知道对方是计算机的情况下进行**很长**时间的问答，如果无法判断对方是不是计算机就说明这个计算机有了人的思维。56年的时候麦卡锡提出了人工智能的术语。

柏拉图假说：不同的人工智能系统以不同的方式表示世界，视觉系统表示为形状和颜色，语言模型表示为语法和语义；但是随着参数规模、训练数据的不断扩大，不同模型对于现实的表征方式会越来越相似，所以现在的大语言模型或者文生视频这种其他模型都使用全模态(文本、视频、音频、深度图等)数据来训练，比单一模态数据性能提高20%。这就发展成现在很多模型都是在大语言模型上微调来的，比如qwen2-VL是基于qwen2的，CogVLM2机遇llama3 8b微调来的。

### 1 介绍

人工智能发展阶段

![人工智能发展阶段](https://raw.githubusercontent.com/wangxiaohong123/p-bed/main/uPic/人工智能发展阶段.png)

#### 1.1 人工智能

三要素：数据，算法，计算力，CPU核心数较少，基于冯诺依曼架构(存储程序，顺序执行)，适合逻辑控制，处理复杂的指令集；GPU核心数较多，适合**并行**处理大量简单且重复的指令，比如矩阵、向量计算；TPU是Google专门为机器学习设计的芯片。

机器学习是人工智能的实现途径，而深度学习是机器学习发展而来，也就是神经网络。195x年最开始人工智能最多就是和人下个黑白棋，198x年开始使用机器学习分辨垃圾邮件，201x年开始深度学习让机器可以识别图片。

现在的模型分大模型和普通模型，普通模型就像专攻一样，训练的数据有10种类别，他就只能做这10种类别的事。大模型的不光是模型的架构大，也是数据集的大，大模型也可能自己创造训练数据，训练自己，比如segment anything。

##### 1.1.1 当前人工智能目前主要应用领域

*   计算机视觉(CV)：机器感知环境的能力，比如物体检测和人脸识别。
*   自然语言处理(NLP)：自然语言处理又包括语音识别、机器翻译、文本挖掘和分类：
    *   文本挖掘和分类：主要是对文字的情绪分析和垃圾检测，目前尤其是中文不同的词在不同的场景有不同的语义是一个难点。
    *   机器翻译：方言、行话是难题。
*   语音：语音识别，语音合成什么的，音转文和文转音是一个领域，现在声纹识别和鸡尾酒会效应很难处理好。声纹和指纹差不多，每个人的声音都不一样，如果能提取出人的声纹那就可以实现声音支付了。鸡尾酒会是说人的大脑在和别人专心讨论的时候会自动降燥，专注和交流目标对话，目前计算机只能做到很多人说话，找不到应该重点听谁发出的声音。

#### 1.2 机器学习算法分类

算法方向分为ML机器学习，DL深度学习，RL强化学习。

根据数据集组成可以将机器学习的算法分为监督学习、无监督学习、半监督学习和强化学习。如果**输出是连续的成为回归，输出是有限个离散值叫分类**。

*   监督学习：输入数据由特征值和目标值组成；
*   无监督学习：输入数据只有特征没有目标值；
*   半监督学习：输入数据部分没有目标值，适合数据难标记或者标记成本高的情况，先用少量标记初步训练，然后用未标记数据训练；
*   强化学习：强化学习是一个决策问题，可以做连续的自动决策，包含5个元素：agent(代理体)，action(行动)，reward(奖励)，environment(环境)和observation(观察情况)，比如让程序中的某个角色模拟走路，这个角色就是agent，他需要决定先迈左脚还是右脚，迈出一步还要不要在迈一步，迈步就是action，地面就是environment，走了几步可以给他reward。强化学习的目标是获得最多的累积奖励；他是不断跟环境交互获取经验，不断进步的过程。通过设计奖励惩罚机制，每正确n步会产生奖励，模型的本质就是获取更多奖励。ALPHAGo、机械手臂、deepseek R1模型都是强化学习实现的。**强化学习更像有大脑在思考一样**。
*   对比学习：相当于计算机视觉上的自监督学习，通过**随机的图像增强**让一张图片随机变成别的样子，但是还属于同一类，这样来代替打标签的操作，同类越相似的思想。如果数据增强变成了核心，数据越离谱学到的东西可能就越多，越花里胡哨效果越好。

监督学习输入的特征都是独立同分布的，独立说的是每次抽样之间相互独立，没有影响，比如掷2次骰子的和大于8，第二次的结果和第一次相关，就不是独立的；同分布说的是每次抽样的样本服从同一个分布，比如掷骰子，每次得到任意点数的概率都是1/6。

##### 1.2.1 强化学习算法

###### 1.2.1.1 Proximal Policy Optimization(PPO)算法

核心思想：PPO 主要改进了策略梯度方法（如 REINFORCE）和信赖域策略优化（Trust Region Policy Optimization, TRPO），使其更加稳定、高效，并且易于实现。其核心思想包括：

1.  **克服策略更新的不稳定性**：
    -   在策略梯度方法中，策略的更新可能会导致较大的变化，从而影响学习稳定性。
    -   TRPO 通过优化约束（KL 散度约束）限制策略更新的幅度，但实现较为复杂。
    -   PPO 采用了一种更简单且高效的方法，即 **剪辑（Clipping）策略比率** 来约束策略的更新幅度。
2.  **使用信赖区域（Trust Region）控制策略更新幅度**：
    -   PPO 通过 **目标函数中的剪辑项** 限制策略的更新步长，从而避免策略发生剧烈变化，提高训练稳定性。

PPO 的两种变体：

1.  **PPO-Clip（剪辑版 PPO）**：
    -   直接对策略比率（ratio）进行剪辑，确保策略不会更新过大。
    -   目标函数： $L^{clip}(θ)=E[min⁡(r_t(θ)A_t,clip(r_t(θ),1−ϵ,1+ϵ)A_t)]$
    -   其中：
        -   $r_t(θ)= \frac{\pi_{\theta}(a_t | s_t)}{\pi_{\theta_{\text{old}}}(a_t | s_t)}$（新的策略与旧的策略的比率）
        -   $A_t$ 为优势函数（Advantage Function）
        -   ϵ 是超参数（通常设为 0.1~0.2）
2.  **PPO-Penalty（KL 散度惩罚版 PPO）**：
    -   在优化目标中添加 KL 散度（Kullback-Leibler Divergence）惩罚项，确保策略不会偏离过远： $L^{KL}(θ)=E[L^{clip}(θ)−βD_{KL}[π_{θold}∣∣π_θ]]$
    -   其中：
        -   $D_{\text{KL}}$ 表示旧策略与新策略之间的 KL 散度。
        -   β 是一个超参数，用于调整 KL 惩罚的权重。

###### 1.2.1.2 Q-learning和DQN

Q-learing包括2部分，瞬时奖励(做了1个动作就能获得的奖励)和记忆经验奖励(按照训练时的记忆，之后怎么做才能获得更大的奖励)，DQN是对Q-learning的扩展，使用神经网络计算Q-learning函数的参数。

*   Double DQN：解决DQN高估自己的问题；
*   Dueling DQN：让网络有举一反三的能力，比如去网吧1会挨揍，那么去网吧2也会挨揍；
*   Multistep DQN：让网络看的更远，计算的时候使用之后的n步；

###### 1.2.1.3 A3C

-   结合了 **Actor-Critic 方法** 和 **异步更新**。通过多个工作线程并行训练策略，使用 **Actor** 来表示策略函数，**Critic** 来评估状态的价值，从而提高样本效率和训练稳定性。

#### 1.3 分类模型评估和回归模型评估

分类模型评估的指标有准确率(预测正确的数占样本总数的比例)、精确率、召回率、F1-score、AUC

回归模型评估根据均方根误差(RMSE)指标来衡量$\sqrt{\sum_{i=1}^n(p_i-a_1)^2\over n}$，求和表示从i=1开始一直到i=n，p表示预测值，a表示真实值。

比如现在有5个样本：

```
真实值：100，120，125，230，400
预测值：105，119，120，230，410
```

{% raw %}
计算RSME：$\sqrt{{(100 - 105)^2 + (120 - 119)^2 + (125 - 120)^2 + (230 - 230)^2 + (400 - 410)^2 }\over 5} = 5.495$
{% endraw %}

模型评估的结果可以分为过拟合和欠拟合，欠拟合就是模型太粗糙，样本数据特征没有学出来；过拟合就是模型在训练样本中表现的过于优越，不是特征的也被学成了特征。**特征过多、错误数据或者同类样本分布不均都会导致过拟合**，一般过拟合模型就废了

#### 1.4 深度学习

深度学习是机器学习的分支，也叫深度结构学习、层次学习或者深度机器学习，是一类算法集合，由神经网络发展而来。深度学习是机器学习的子集，机器学习是实现人工智能的一种途径。传统机器学习依赖人工提取特征，深度学习依赖算法自动提取特征。

常见的应用领域：人脸相关、AR特效、手势识别、语音识别、机器翻译、OCR、自动驾驶、风控、安防、推荐系统等。

### 2 开发环境

直接安装anaconda，自带python、常见的科学计算库、conda环境管理和jupyter等工具，到官网`https://www.anaconda.com/download/success`下载对应版本即可

Jupyter是iPython的网页加强版，除了可以编程，还可以做一些笔记、展示，pycharm适合大工程，纯写代码，Jupyter适合目标不明确的时候，比如画图、数据展示，Jupyter就更方便。

进入到目标目录打开命令窗口之后使用conda选择虚拟环境然后输入`jupyter notebook`就可以在浏览器打开jupyter的开发界面了

![jupyter-start](https://raw.githubusercontent.com/wangxiaohong123/p-bed/main/uPic/jupyter-start.png)

### 3 常用科学库

科学计算库是用于数值计算、矩阵运算、自动微分等功能的工具库。

#### 3.1 matplotlib

用来创建静态、交互和实时得D图表。有3层结构：

1.   容器层：画板、画布、坐标系。
2.   辅助层：坐标标题、网格、图像标题、刻度。
3.   图像层：折线图、数点图、柱状图等等。

##### 基础用法：

```python
import matplotlib.pyplot as plt
import random

# 模拟x、y轴数据
x = range(60)
# 设置x轴刻度
x_ticks_label = ["11点{}分".format(i) for i in x]

y_shanghai = [random.uniform(15, 18) for i in x]
# 设置y轴刻度
y_ticks = range(40)

# 创建画布，figsize是长宽，dpi是清晰度
plt.figure(figsize=(20, 8),dpi=100)
# 显示网格，alpha设置网格线的透明度
plt.grid(True, linestyle="--", alpha=0.5)
# 设置标题等描述信息
plt.xlabel("时间")
plt.ylabel("温度")
plt.title("标题")

# 设置x轴刻度并且间隔为5，不能直接使用字符串，x[::5]是必须的
plt.xticks(x[::5], x_ticks_label[::5])
plt.yticks(y_ticks[::5])

# 绘制图像，如果要在一张图上展示多个图像就写多行plot()
# 如果想让多个数据在不同的坐标系中展示可以使用subplots()
# plot折线图，scatter散点图，bar柱状图，pie饼图
plt.plot(x, y_shanghai)

# 保存图像，必须要放到show()的前面，因为show()会释放资源
plt.savefig("test.png")

# 显示图像
plt.show()
```

#### 3.2 Numpy

numpy是一个开源的科学计算库，可以处理任意维度的数组和矩阵运算。比python原生的效率高，机器学习的样本数据量都很大，numpy更合适。

numpy快是因为使用numpy定义列表时要求元素类型必须相同，这样可以直接存储数据，python原声的列表可以存储不同类型的元素，他是通过内存地址指向真实的数据。

##### 3.2.1定义数组

```python
import numpy as np

# 创建一个2维数组，数组中元素类型是int32，dtype不是必须的参数
score = np.array(
    [[80,89,86,67,79],
     [78,97,89,67,81],
     [90,94,78,67,74],
     [91,91,90,67,69],
     [76,87,75,67,86],
     [70,79,84,67,84],
     [94,92,93,67,64],
     [86,85,83,67,80]]
, dtype=np.int32)

# array = [[80,89,86,67,79],
#      [78,97,89,67,81],
#      [90,94,78,67,74],
#      [91,91,90,67,69],
#      [76,87,75,67,86],
#      [70,79,84,67,84],
#      [94,92,93,67,64],
#      [86,85,83,67,80]]
# 下面2种方式是复制数组
# score = np.array(array)
# score = np.asarray(array)
```

numpy有两种方式定义数组，一种是array()，另一种是asarray()，array()可以创建数组也可以复制数组，asarray()只能复制数组。array()是深拷贝，asarray()是浅拷贝。

##### 3.2.2 生成值都为0或者1的数组：

```python
# 生成4行8列值都为1的数组
ones = np.ones([4, 8])
# 生成一个和ones格式相同，值都为1的数组
zeros1 = np.zeros_like(ones)
# 生成4行8列值都为0的数组
zeros = np.zeros([4, 8])
# 生成一个和zeros格式相同，值都为1的数组
ones1 = np.ones_like(zeros)
```

##### 3.2.3 生成等差数组：

```python
# start起始值，stop最大值，num生成的元素数量，endpoint是否包含stop值(默认true)
np.linspace(start, stop, num, endpoint)
# start起始值，stop最大值，step步长(默认1)
np.arange(start, stop, step, dtype)
```

##### 3.2.4 生成等比数组：

```python
# 生成10^x个，start, stop是指x的值，num默认50
np.logspace(start, stop, num)
```

##### 3.2.5 随机数组：

正态分布：正态分布是一种概率分布，是具有两个参数$\mu$和$\sigma$的连续行随机变量的分布，$\mu$是均值，决定峰值位置，$\sigma$叫标准差，决定图形的高矮胖瘦。$\mu=0，\sigma=1$的时候是标准的正太分布，$\sigma$的计算公式：$\sigma=\sqrt{\frac{\sum_{i=0}^N (x_i - \mu)^2} N}$

```python
# 生成均值是1.75，标准差为1的100个数据
x1 = np.random.normal(1.75, 1, 100)
```

##### 3.2.6 数组属性：

```python
# 几行几列
print(score.shape)
# 维数
print(score.ndim)
# 元素数量
print(score.size)
# 一个数组元素的字节数
print(score.itemsize)
# 元素类型
print(score.dtype)
```

#### 3.3 pandas

开源的数分库，用来做数据挖掘，基于numpy和matplotib。pandas的数组可读性更好，数据处理和文件读取更方便。他3种数据结构：

1.   series：一维数据结构

     ```python
     import pandas as pd
      
     # data是传入的数据，可以是ndarray、list等
     # index是唯一索引，并且需要和数据的长度相等，不传默认生成0~N
     pd.series(data=None, index=None, dtype=None)
     # 当data是字典的时候，key就是索引，value是数组的值
     ```

2.    dataFrame：二维数据结构

      ```python
      import pandas as pd
      
      # data是传入的数据，可以是ndarray、list等
      # index是唯一行索引，不传默认生成0~N
      # columns是唯一列索引，不传默认生成0~N
      pd.DataFrame(data=None, index=None, columns=None)
      ```


3.    multiIndex/panel：多个索引的二维数组：

      ```python
      import pandas as pd
      
      arrays = [[1, 2, 3, 4], ['g', 'r', 'b', 'y']]
      # names表示索引
      pd.MultiIndex.from_arrays(arrays, names=('number', 'color'))
      ```

##### 3.3.1 基础使用

1.   索引操作：pandas读取数据的顺序是先列后行，和别人不一样，比如`arr[2][3]`表示读取第3行第2列，但是他不支持通过索引值获取数据，只能通过索引的具体值来获取，比如`arr['次数']['2024']`；先行后列使用loc：`arr.loc[['2024']['次数']]`；通过索引使用iloc：`arr.iloc[[:3][2]]`表示前3行第2列的所有数据。

2.   赋值操作：直接使用等号赋值。

3.   排序操作：

     ```python
     # 值排序，by可指定多个列名，就会根据指定的索引排序，ascending true表示生序
     arr.sort_values(by=, ascending=)
     # 根据索引排序
     arr.sort_index()
     ```

4.   运算操作：

     ```python
     # 次数列的值加1
     arr['次数'].add(1)
     
     # 次数列的值减1
     arr['次数'].sub(1)
     
     # 计算次数是否大于23，会按行输出true或者false
     arr['次数'] > 23
     # 筛选出次数大于23的行
     arr[arr['次数'] > 23]
     # 筛选出次数大于23并且小雨30的行
     arr.query("次数 > 23 & 次数 < 30")
     ```

5.   通缉运算：

     ```python
     # 以列为维度计算平均值，标准差，最大值，最小值等
     arr.describe()
     
     # 其他的统计函数和numpy用法一致
     
     # 累积求和，每行都会变成累计的和
     arr.cumsum()
     # func是自定义函数，axis=0表示列，axis=1表示行
     arr.apply(func, axis=0)
     ```

6.   画图操作：

     ```python
     import pandas as pd
     import matplotlib.pyplot as plt
     
     # 拿到图像需要的数据很重要
     # 比如先把数据排序
     arr.sort_index()
     # 获取数据的某一列，这里拿到的列还是有行的索引的，相当于横纵坐标
     times = arr['次数']
     
     # line折线图，bar柱状图，barh横向柱状图，hist直方图，pie饼图，scatter散点图
     times.plot(kind='line')
     ```

7.   文件操作：pandas支持很多格式的文件读取和存储，比如CSV，JSON，HTML，XLS，SQL，SAS，HDF5等

     ```python
     # path_or_buffer文件的路径，usecols要读取的列明数组，不写读所有
     # read_json就是读取json数据
     data = pd.read_csv(path_or_buffer, usecols = '')
     
     # 将数据写进cvs文件
     # sep数据分隔符，columns要写的列，header是否写进列索引(可以写列的数组)，是否写进行索引，mode w重写 a追加
     data = pd.to_csv(path_or_buffer, sep = ',', columns = [], header = True, index = True, mode = 'w')
     ```

##### 3.3.2 高级使用

1.   处理缺失值，首先需要知道缺失值是NaN还是什么符号

     ```python
     # 如果返回true说明没有缺失值或者缺失值不是NaN
     np.all(pd.notnull(arr))
     
     # 如果缺失值不是NaN，比如是问号可以先把问号替换成NaN
     arr.replace(to_replace = '?', value = np.nan)
     
     # 删除存在缺失值的行
     arr = arr.dropna()
     # 替换缺失值,inplace表示直接替换还是生成一个新对象
     # 可以for循环替换所有列的缺失值
     arr["列名"].fillna(要替换的值, inplace = True)
     ```

2.   数据离散化：离散化是为了简化数据结构，他可以减少连续属性值的个数。比如有一个身高列，值可能是165，174，160，180等等，离散化之后会变成150～165，165～170等等，对应的值就是处在这个区间的数量。

     ```python
     # 10表示分成10组
     qcut = pd.qcut(arr['列明'], 10)
     
     # 打印分组之后每个区间的数量，没有意义，就是为了看的
     qcut.value_counts()
     
     # 也可以指定分组区间bins，bins是个数组定义分组的区间
     cut = pd.qcut(arr['列明'], bins)
     
     # one-hot编码：把列中的每个类型变成一个boolean列
     # prefix相当于列的前缀
     pd.get_dummies(data, prefix = None)
     
     # 使用one-hot就可以将上面分好组的数据变成多列的数组
     pd.get_dummies(cut, prefix = '身高')
     ```

3.   数据合并：当需要多张表时将他们合并到一起方便分析

     ```python
     # axis=0表示按列索引合并。1表示按行索引合并。不管是按行还是按列都需要保证索引值相同
     pd.concat([data1, data2], axis = 0)
     
     # merge就像sql的join，how指定是左连接还是右连接，on指定条件的索引
     pd.merge(data1, data2, how='inner', on=None)
     ```

4.   交叉表和透视表：交叉表是用来计算2列数据的分组个数，他属于特殊的透视表。透视表是按照指定的索引和列进行聚合和汇总。

     ```python
     pd.crosstab(data['列1'], data['列2'])
     # 创建透视表
     # values聚合值的列
     # index行索引
     # columns列索引
     # aggfunc汇总函数
     # fill_value缺失值填充为0
     pivot = pd.pivot_table(data, values='销售额', index='日期', columns='城市',aggfunc='sum',fill_value=0)
     ```

5.   分组：

     ```python
     # key:分组的列，可以是多个
     # as_indes:是否将分组的列设置为索引
     arrar.groupby(key, as_index=False)
     ```

#### 3.4 seaborn

seaborn比matplotlib画图更高效，api更简单。seaborn分为单变量和双变量，单变量就是直方图、核密度估计曲线等，双变量就是散点图、树密度估计图等。

单变量图绘制：

```python
import seaborn as sns

# a 要观察的数据，可以是series、一位数组或者列表
# bins 条形数量
# hist 是否是直方图
# kde 是否绘制高斯核密度曲线

sns.distplot(a, bins=None, hist=True, kde=True, rug=False, fit=None, color=None)
```

双变量绘制：

```python
import seaborn as sns

# kind 图形类型
# stat_func 两个变量的关系
# ratio 中心圆和侧边图的比例
# space 中心圆和侧边图的间隔
sns.jointplot(x, y, data=None, kind='scatter', stat_func=None, color=None, ratio=5, space=0.2, dropna=True)
```

