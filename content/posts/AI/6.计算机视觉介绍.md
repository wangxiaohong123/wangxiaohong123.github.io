---
title: 6.计算机视觉(CV)介绍
tags:
  - 计算计视觉
categories: 人工智能
copyright: true
---

图像是由长度、宽度、颜色通道组成的矩阵，一般是3个通道RGB，每个通道有多个像素点，像素点的值是0到255，这个数值表示的是亮度，比如0就是最暗-黑色，255是最亮-白色。

计算机视觉的挑战：部分遮掩(比如模型识别不出来被遮住头的猫)，背景混入(无法识别和背景颜色相似的目标)。

### 一、主要流程

#### 1. 数据采集与预处理（Data Acquisition & Preprocessing）

获取高质量的视觉数据，并进行规范化处理。比如图像/视频采集、数据增强（如翻转、旋转、缩放）、数据标准化（如均值归一化、缩放到 [0,1]）、去噪、去模糊。常用工具OpenCV、Pillow（PIL）、Albumentations。

常用的开源基准数据集有2种，VOC和COCO，里面包含了一些人、动物、物品等带b标注图片。

#### 2. 特征提取（Featrue Extraction / Backbone）

从原始图像中提取有代表性的特征，经典网络VGG、ResNet、MobileNet、Darknet。输出的是高维的特征图。

#### 3. 特征表示与学习（Featrue Representation & Learning）

对特征进行降维或压缩、将特征输入到分类器或检测器中、学习特征与任务目标之间的映射关系。常见模型：

*   分类任务：Softmax 分类器
*   检测任务：RPN（Region Proposal Network）
*   分割任务：上采样模块（如 U-Net 中的 Decoder）

#### 4. 任务特定头（Task-Specific Heads）

根据不同任务设计不同的网络头（Head）。常见任务头：

*   图像分类：全连接层 + Softmax
*   目标检测：边界框回归 + 分类头
*   语义分割：像素级分类头
*   实例分割：目标检测头 + 分割头（如 Mask R-CNN）

#### 5. 推理与后处理（Inference & Post-processing）

将模型的输出转换为可解释的结果，常见后处理方法：

*   NMS（非极大值抑制）：去除冗余边界框
*   Softmax：转换为类别概率
*   阈值化：过滤低置信度的检测结果
*   CRF（条件随机场）：优化分割边界

##### 5.1 NMS

**非极大值抑制（Non-Maximum Suppression, NMS）** 是一种常用于目标检测任务中的后处理技术，主要目的是从多个重叠的候选框（Bounding Boxes）中选择出最佳的框，减少冗余框，确保每个目标只保留一个检测框。流程：

1.   对所有候选框按**置信度分数**进行降序排序。
2.   选择**当前置信度最高的框**，将其加入保留框列表 `Keep` 中。
3.   计算当前选择的框与**所有剩余框**之间的IoU。
4.   对于与当前框 **IoU 大于设定阈值**（例如 0.5）的其他框，将它们从候选框列表中**移除**。因为这些高重叠的框很可能指向同一个目标，保留多个会导致冗余。
5.   从剩余的候选框中，**选择下一个置信度最高的框**，重复步骤 **2 → 3 → 4**。

**如果目标靠的太近可能造成误删**，改进版有Soft-NMS(调整置信度而不是直接丢弃框)和Multi-Class NMS(针对不同类别分别进行 NMS)。

### 二、图像分类

特征提取：backbone。从给定类别集合中为图像分配标签的任务。

#### 1. AlexNet

使用8层卷积神经网络赢得2012年imageNet冠军。5个卷积层+2个隐藏层+1个输出层，**大尺度卷积核计算开销大**。模型：

```python
import tensorflow as tf

net = tf.keras.Sequential([
    # 卷积层：96个卷积核，卷积核大小是11 * 11，步长是4，激活函数是rule
    tf.keras.layers.Conv2D(filters=96, kernel_size=11, strides=4, activation='relu'),
    # 池化层：窗口大小是3*3，步长是2
    tf.keras.layers.MaxPooling2D(pool_size=3, strides=2),
    # 卷积层：256个卷积核，卷积核大小是5 * 5，padding是same，激活函数是rule
    tf.keras.layers.Conv2D(filters=256, kernel_size=5, padding='same', activation='relu'),
    # 池化层：窗口大小是3*3，步长是2
    tf.keras.layers.MaxPooling2D(pool_size=3, strides=2),
    # 卷积层：384个卷积核，卷积核大小是3 * 3，padding是same，激活函数是rule
    tf.keras.layers.Conv2D(filters=384, kernel_size=3, padding='same', activation='relu'),
    tf.keras.layers.Conv2D(filters=384, kernel_size=3, padding='same', activation='relu'),
    tf.keras.layers.Conv2D(filters=256, kernel_size=3, padding='same', activation='relu'),
    tf.keras.layers.MaxPooling2D(pool_size=3, strides=2),
    # 伸展成1维向量
    tf.keras.layers.Flatten(),
    # 隐藏层：4096个神经元
    tf.keras.layers.Dense(4096, activation='relu'),
    # 随机失活
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(4096, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    # 输出层：10个神经元，激活函数是softmax
    tf.keras.layers.Dense(10, activation='softmax')
])

# 1张图片，大小是227 * 227，通道数是1
X = tf.random.uniform((1, 227, 227, 1))
y = net(X)
# 输出模型结构
net.summary()
```

#### 2. VGG

分为VGG16和VGG19，一个16层一个19层。他的卷积核只有3\*3，比AlexNet小很多，常用作特征提取，**小适度卷积核无法捕捉大尺度特征**。模型：

```python
import tensorflow as tf


def vgg_block(num_conv, num_filters):
    """
    构建卷积块
    :param num_conv: 卷积层个数
    :param num_filters: 卷积核个数
    """
    blk = tf.keras.Sequential()
    for _ in range(num_conv):
        blk.add(tf.keras.layers.Conv2D(num_filters, (3, 3), padding='same', activation='relu'))
    blk.add(tf.keras.layers.MaxPooling2D(pool_size=2, strides=2))
    return blk


def vgg(conv_arch):
    net = tf.keras.Sequential()
    for (num_conv, num_filters) in conv_arch:
        net.add(vgg_block(num_conv, num_filters))

    # 全联接层
    net.add(tf.keras.Sequential([
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(4096, activation='relu'),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(10, activation='softmax'),
    ]))
    return net


# 创建模型
net = vgg(((2, 64), (2, 128), (3, 256), (3, 512), (3, 512)))
```

#### 3. GoogLeNet

引入Inception代替之前的卷积+激活函数，他的错误率很接近人眼。Inception不需要堆叠多层卷积核，他的核心思想是**同一层同时使用不同大小的卷积核（1×1、3×3、5×5）和池化层**，然后将它们的输出拼接起来，Inception包含下面几个部分：

1.   1*1卷积：减少特征图的通道数，降低后续大尺寸卷积的计算开销
2.   3*3卷积：捕获中等尺度的特征
3.   5*5卷积：捕获较大尺度的特征
4.   3*3池化：减少特征图的空间尺寸，保留主要特征
5.   1*1卷积：在池化之后进行降维

模型：

```python
import tensorflow as tf


class Inception(tf.keras.layers.Layer):
    # 设置模块的构成
    def __init__(self, c1, c2, c3, c4):
        super().__init__()
        # 1*1
        self.p1_1 = tf.keras.layers.Conv2D(c1, kernel_size=1, activation='relu', padding='same')
        # 3*3
        self.p2_1 = tf.keras.layers.Conv2D(c2[0], kernel_size=1, activation='relu', padding='same')
        self.p2_2 = tf.keras.layers.Conv2D(c2[1], kernel_size=1, activation='relu', padding='same')
        # 5*5
        self.p3_1 = tf.keras.layers.Conv2D(c3[0], kernel_size=1, activation='relu', padding='same')
        self.p3_2 = tf.keras.layers.Conv2D(c3[1], kernel_size=5, activation='relu', padding='same')
        # 3*3池化+1*1卷积
        self.p4_1 = tf.keras.layers.MaxPooling2D(pool_size=3, padding='same', strides=1)
        self.p4_2 = tf.keras.layers.Conv2D(c4, kernel_size=1, activation='relu', padding='same')

    # 前向传播过程
    def call(self, inputs):
        p1 = self.p1_1(inputs)
        p2 = self.p2_2(self.p2_1(inputs))
        p3 = self.p3_2(self.p3_1(inputs))
        p4 = self.p4_2(self.p4_1(inputs))
        outputs = tf.concat([p1, p2, p3, p4], axis=-1)
        return outputs


def aux_classifier(x, filter_size):
    """
    辅助分类器
    :param x: 输入数据
    :param filter_size: 卷积核个数
    """
    x = tf.keras.layers.AveragePooling2D(pool_size=5, strides=3, padding='same')(x)
    x = tf.keras.layers.Conv2D(filter_size=filter_size[0], kernel_size=1, strides=1, padding='valid', activation='relu')(x)
    x = tf.keras.layers.Flatten()(x)
    x = tf.keras.layers.Dense(units=filter_size[1], activation='relu')(x)
    x = tf.keras.layers.Dense(units=10, activation='softmax')(x)
    return x


# B1模块
inputs = tf.keras.Input(shape=(224, 224, 3), name='input')
# 64个卷积核，每个卷积核大小7*7
x = tf.keras.layers.Conv2D(64, kernel_size=7, strides=2, padding='same', activation='relu')(inputs)
x = tf.keras.layers.MaxPooling2D(pool_size=3, strides=2, padding='same')(x)

# B2模块
x = tf.keras.layers.Conv2D(64, kernel_size=1, strides=1, padding='same', activation='relu')(x)
x = tf.keras.layers.Conv2D(192, kernel_size=3, strides=1, padding='same', activation='relu')(x)
x = tf.keras.layers.MaxPooling2D(pool_size=3, strides=2, padding='same')(x)

# B3模块
x = Inception(64, (96, 128), (16, 32), 32)(x)
x = Inception(128, (128, 192), (32, 96), 64)(x)
x = tf.keras.layers.MaxPooling2D(pool_size=3, strides=2, padding='same')(x)

# B4模块
x = Inception(192, (96, 208), (16, 48), 64)(x)
aux_output_1 = aux_classifier(x, [128, 1024])
x = Inception(160, (112, 224), (24, 64), 64)(x)
x = Inception(128, (128, 256), (24, 64), 64)(x)
x = Inception(112, (114, 288), (32, 64), 64)(x)
aux_output_2 = aux_classifier(x, [128, 1024])
x = Inception(256, (160, 320), (32, 128), 128)(x)
x = tf.keras.layers.MaxPooling2D(pool_size=3, strides=2, padding='same')(x)

# B5模块
x = Inception(256, (160, 320), (32, 128), 128)(x)
x = Inception(384, (192, 384), (48, 128), 128)(x)
x = tf.keras.layers.GlobalAvgPool2D()(x)
output = tf.keras.layers.Dense(units=10, activation='softmax')(x)

model = tf.keras.Model(inputs=inputs, outputs=[output, aux_output_1, aux_output_2])
```

#### 4. ResNet

15年出来的真神，10年了大家还在用。网络越深获取的信息越多，特征也越丰富，但是实践中网络越深，训练效果反而越差，针对这个问题，何凯明提出了残差网络(ResNet):同样是增加网络层数，但是增加之前向保存当前网络，增加完1层之后将结果和之前相比，选择2个里最优的，**不过残差网络保存的网络变多，需要的显存也更大。**

```python
import tensorflow as tf
from tensorflow.keras import layers, models


def residual_block(x, filters, kernel_size=3, stride=1, downsample=False):
    """
    定义一个残差块（Residual Block）
    - x: 输入张量
    - filters: 卷积核的数量
    - kernel_size: 卷积核大小，默认3x3
    - stride: 步幅，默认1
    - downsample: 是否下采样，用于改变通道数和尺寸
    :return 输出张量
    """
    # 保存输入（用于跳跃连接）
    shortcut = x

    # 主路径 - 第一层卷积
    x = layers.Conv2D(filters, kernel_size, strides=stride, padding='same', use_bias=False)(x)
    x = layers.BatchNormalization()(x)
    x = layers.ReLU()(x)

    # 主路径 - 第二层卷积
    x = layers.Conv2D(filters, kernel_size, strides=1, padding='same', use_bias=False)(x)
    x = layers.BatchNormalization()(x)

    # 如果需要下采样或通道不匹配，调整 shortcut
    if downsample:
        shortcut = layers.Conv2D(filters, 1, strides=stride, use_bias=False)(shortcut)
        shortcut = layers.BatchNormalization()(shortcut)

    # 将输入（shortcut）和输出相加
    x = layers.Add()([x, shortcut])
    x = layers.ReLU()(x)
    return x


def build_resnet(input_shape, num_classes):
    """
    构建 ResNet-18 模型
    - input_shape: 输入图像的形状 (H, W, C)
    - num_classes: 输出类别数量

    :return ResNet 模型
    """
    inputs = layers.Input(shape=input_shape)

    # 初始卷积层和池化层
    x = layers.Conv2D(64, 7, strides=2, padding='same', use_bias=False)(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.ReLU()(x)
    x = layers.MaxPooling2D(pool_size=3, strides=2, padding='same')(x)

    # 残差块阶段
    x = residual_block(x, 64)
    x = residual_block(x, 64)

    x = residual_block(x, 128, stride=2, downsample=True)
    x = residual_block(x, 128)

    x = residual_block(x, 256, stride=2, downsample=True)
    x = residual_block(x, 256)

    x = residual_block(x, 512, stride=2, downsample=True)
    x = residual_block(x, 512)

    # 全局平均池化层
    x = layers.GlobalAveragePooling2D()(x)

    # 全连接层输出分类结果
    outputs = layers.Dense(num_classes, activation='softmax')(x)

    # 创建模型
    return models.Model(inputs, outputs)


# 构建模型
resnet_model = build_resnet((224, 224, 3), 10)

# 编译模型
resnet_model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# 输出模型结构
resnet_model.summary()
```

### OpenVC

OpenCV 是一个计算机视觉的工具库，安装：

```shell
# 3.4.2之后有一些算法申请专利用不了了
pip install opencv-python==3.4.1.15
# 两个工具的版本要一致
pip install opencv-contrib-python==3.4.1.15
```

基本操作：

```python
import cv2

# 读取图像,读取进来的数据是一个3维数组(numpy的ndarray)：像素点是2维的 + 通道的维度
# IMREAD_GRAYSCALE表示读取方式是灰度图,灰度图的通道只有1
img = cv2.imread('xxx.jpg', cv2.IMREAD_GRAYSCALE)
# 输出图片数组的长度：长、宽、通道数
print(img.shape)
# 截取图像
cat = img[0:200, 0:200]
# 改变图像大小，这个不是截取而是拉伸或者压缩图像
dog = cv2.resize(img, (200, 200))
# 提取颜色通道
b, g, r = cv2.split(img)
# 合并通道
cv2.merge((b, g, r))
# 边界填充，填充类型有很多，比如BORDER_REPLICATE表示复制最近像素
cv2.copyMakeBorder(img, 10, 10, 10, 10, borderType=cv2.BORDER_REPLICATE)

# 读取视频
open1 = False
vc = cv2.VideoCaptrue('xxx.mp4')
if vc.isOpened():
    open1, frame = vc.read()
# 视频读取就是把每帧都读取成图片数组
while open1:
    ret, frame = vc.read()
    if frame is None:
        break
    if ret:
        # 3通道转成灰度图
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        cv2.imshow('result', gray)
        # 每0.1s播放下一帧，27是esc按键，意思是按esc退出播放
        if cv2.waitKey(100) & 0xFF == 27:
            break
vc.release()
cv2.destroyAllWindows()
```

它还可以处理图像的阈值、平滑、腐蚀、膨胀、开闭运算等等。

##### 信用卡数字识别

##### OCR识别

##### 图像特征

##### 全景图象拼接

##### 停车场车位识别

##### 答题卡判卷

##### 背景建模

##### 光流估计

##### DNN模块

##### 目标追踪

##### 卷积原理

##### 疲劳检测



### 四、图像分割

图像分割是目标检测的进阶，就是为图像中的每个像素进行分类，可以用在自动驾驶，医疗影像诊断，图片美化等领域。主要分2类，语义分割和实例分割，实例分割分割不仅仅要区分类别，还要区分不同类别中的不同个体。

常用的开源数据集有VOC、coco、城市风光数据集(自动驾驶模型专用)。

#### 1 评价指标

1.   像素精度
2.   平均像素精度
3.   平均交并比

#### 2 语义分割

##### 2.1 FCN

FCN使用全卷积架构，是目前图像分割算法的基础框架。 除了全卷积层还多了一个上采样层，上采样层使用**反卷积**可以将低分辨率的特征图恢复到原图相同的尺寸。

FCN还引入的跳跃连接，让不同层次的特征图进行融合，可以保留高层语义信息和底层空间信息，提高分割精度和保留更多细节。

##### 2.2 Unet

采用**对称的编码器-解码器（Encoder-Decoder）** 结构，类似英文字母 "U" 的形状。编码器就是卷积层，卷积核是3*3，通道数递增，解码器就是采样层，通道数递减，适合高分辨率图像分割。

#### 3 实例分割

##### 3.1 MaskRCNN

就是在faster-RCNN的基础上增加一个分支，实现目标检测的同时分割目标像素。

#### 4 Segment Anything

23年Facebook开源的图片分割**大模型**，这是第一个视觉大模型，基于VIT增加了数据引擎和提示词编码器：

1.   数据引擎：由模型自己创建训练的标记数据。
2.   提示词系统：通过提示告诉模型要分割哪些物体，而不是和传统模型一样只能自动分割样本数据中的物体。