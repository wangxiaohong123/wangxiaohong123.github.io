---
title: 2.机器学习基础
tags:
  - 机器学习
categories: 人工智能
copyright: true
---

机器学习就是模仿人类处理问题，人类是先从经验归纳，机器学习也是自动从数据中获取模型，用模型对位置数据进行预测。白话讲就是从历史数据里找规律，有了规律之后根据输入的数据输出结果，这个规律就是模型。比如说某个样子的物体是猫，那下次看见类似的也可以把他当成猫。

机器学习不能解决的问题，跨域：机器学习学习的是历史数据的特征，新的数据不可能和历史数据有一样的特征分布

### 1. 计算机视觉

用摄像机、电脑或者其他设备模拟生物视觉，让计算机理解图片或者视频中的内容。可以分为三类：

1.   图像分类：将图像结构化成类别信息，用实现确定好的类别来描述图片。
2.   目标检测：关注特定的物体目标，要求获取这一目标的类别和位置信息，他和分类的区别是，分类将图片当做整体，目标检测可以在一张图片中获取多种目标及位置。
3.   图像分割：分割是在检测的基础上还需要获取像素信息。

应用领域：人脸识别、视频监控、智能驾驶、图片识别（以图搜图，图片鉴黄），比如抖音送礼的眼镜特效属于人脸识别里的人脸关键点定位。

### 2. 自然语言处理

语言模型是用来计算下一个句子概率的模型。

### 3 时间序列

时间序列是一种 **有序的、依赖时间的结构化数据**，其核心任务是研究和预测数据随时间变化的规律。深度学习为时间序列任务提供了更强大的建模工具，尤其是在处理复杂非线性关系或长时间依赖时。他是一个跨领域的问题，还会涉及到统计学等。

### 3. 机器学习的工作流程：

获取数据->对数据进行基本处理->特征工程->机器学习(模型训练，也是算法应用的过程)->模型评估，如果模型评估没有达到要求需要从对数据进行基本处理重新进行一遍。

#### 3.1 获取数据

拿到的数据类似于表格，一行就是一个样本，一列就是一个特征，涉及到判断的列不叫特征，叫目标值，不是所有数据都有目标值。数据分为训练数据和测试数据，一般比例为3/7或者2/8。

#### 3.2 数据基本处理

修改数据的空值、异常值、类型转换等。

#### 3.3 特征工程

对数据的进一步处理。包括特征提取(比如将文本或者图片转换成可以用于机器学习的数字)，特征预处理(通过一些函数将数据转换成适合算法模型的特征数据)和特征降维(降低特征个数)。

##### 3.3.1 特征预处理

将数据转换成机器更好识别，更好处理的数据。当特征数据的单位或者大小相差较大，或者某个特征的方差相比其他的方差大出好几个数量级，这种情况可能这个特征对结果的影响比较大，使得算法无法学习到其他特征。

###### 3.3.1.1 归一化

把原始数据映射到某个区间内，默认0~1。计算公式为：
$$
X' = \frac {x - min}{max - min}\\

X'' = X' * (mx - mi) + mi
$$
上面的公式中，X''就是归一化处理后的特征值，max和min表示初始特征值的最大值和最小值，mx和mi表示想要将特征值映射到区间的最大值和最小值。

归一化的时候如果出现一条特征统计不正确，比如有1条数据比其他的大了很多倍或者小了很多倍，这种情况对其他数据的影响很大，所以这种方法鲁棒性较差，只适合精确小数据场景。

```python
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

data = pd.read_csv('./data/dating.txt')
# 实例化转化器
transfer = MinMaxScaler(featrue_range=(3, 5))
# 将目标列转换到指定区间，这里是3~5
ret_data = transfer.fit_transform(data[["列1名称", "列2名称"]])
print("转化后的数据:\n", ret_data)
```

###### 3.3.1.2 标准化

```python
import pandas as pd
from sklearn.preprocessing import StandardScaler

data = pd.read_csv('./data/dating.txt')
# 实例化转化器
transfer = StandardScaler()
# 将目标列转标准化
ret_data = transfer.fit_transform(data[["列1名称", "列2名称"]])
print("转化后的数据:\n", ret_data)
print("每一列的方差为:\n", transfer.var_)
print("每一列的平均值为:\n", transfer.mean_)
```

第一步就是去均值，将平均值变成0，然后在比上标准差，这样能让所有维度的数相差不大：$X' = \frac{x - avg}{\sigma}$。

###### 3.3.2 特征提取

就是把数据转换成数字，主要包括字典特征提取，文本特征提取（统计单词次数），图像特征提取。

*   字典特征提取：

    ```python
    from sklearn.featrue_extraction import DictVectorizer
    
    
    def dict_demo():
        """
        字典特征提取
        :return: None
        """
        data = [{'city': '北京', 'temperatrue': 100},
                {'city': '上海', 'temperatrue': 60},
                {'city': '深圳', 'temperatrue': 30}]
    
        # 字典特征提取
        # 非sparse矩阵，比sparse矩阵遍历慢，费空间
        transfer = DictVectorizer(sparse=False)
        new_data = transfer.fit_transform(data)
    
        print("列名：\n", transfer.get_featrue_names_out())
        print(new_data)
        
    # 控制台输出
    属性名：
     ['city=上海' 'city=北京' 'city=深圳' 'temperatrue']
    [[  0.   1.   0. 100.]
     [  1.   0.   0.  60.]
     [  0.   0.   1.  30.]]
    ```

*   英文文本特征提取：

    ```python
    from sklearn.featrue_extraction.text import CountVectorizer
    
    
    def text_count_demo():
        """
        英文文本特征提取
        :return: None
        """
        text = ["life is short,i like python", "life is too long,i dislike python"]
        # 可以使用stop_words属性指定停用词
        transfer = CountVectorizer()
        new_data = transfer.fit_transform(text)
        print("特征名:\n", transfer.get_featrue_names_out())
        print(new_data)
        
    # 控制台输出，默认就是sparse矩阵，(0, 2)	1 表示在第一句话中，输出的特征名中的第二个单词出现1次，也就是在第一句话中is单词出现了1次。
    特征名:
     ['dislike' 'is' 'life' 'like' 'long' 'python' 'short' 'too']
      (0, 2)	1
      (0, 1)	1
      (0, 6)	1
      (0, 3)	1
      (0, 5)	1
      (1, 2)	1
      (1, 1)	1
      (1, 5)	1
      (1, 7)	1
      (1, 4)	1
      (1, 0)	1
    ```

*   中文文本特征提取：中文使用jieba分词现将中文分开，然后在使用CountVectorizer提取特征

*   Tf-idf文本特征提取：如果某个词或者某个短语在一篇文章中出现的概率比其他文章高，就认为这个词或短语具有很好的类别区分能力，适合是用来分类。Tf就是词频，idf是逆向文档频率，idf分2步得到，第一步先用总文件数除以包含该词的文件数，第二步将第一步得到的商取10为底的对数。Tf * idf就是最终的重要程度。

##### 3.3.3 特征降维

减少特征的数量，一般都是将相关性的特征去掉一些或者合并成1个，比如湿度和降雨量可能降维之后就剩一个降雨量。降维有2种方式：特征选择和主成分分析。

###### 3.3.3.1 特征选择

就是提取主要特征，主要有2中实现：

1.   过滤式，探究特征和目标值之间的关联，使用低方差特征过滤(方差相差小表示很相近，就删掉)或者相关系数。

     ```python
     import pandas as pd
     from sklearn.featrue_selection import VarianceThreshold
     from scipy.stats import pearsonr
     from scipy.stats import spearmanr
     
     
     def var_thr():
         """
         特征降维：低方差特征过滤
         :return:
         """
         data = pd.read_csv('csv文件')
         # 实例化,方差小于1的会被过滤
         transfer = VarianceThreshold(threshold=1)
         # 转换
         transfer_data = transfer.fit_transform(data.iloc[:, 1:10])
         #
         #
     
     
     def pea_demo():
         """
         特征降维：相关系数
         :return:
         """
         x1 = [12.5, 43, 15.3, 14.4]
         x2 = [21.2, 23.1, 44.2, 19.5]
         # 皮尔逊相关系数
         ret = pearsonr(x1, x2)
         # 斯皮尔曼相关系数
         # ret = spearmanr(x1, x2)
         
         # 无论哪个相关性公式都会输出2个值
         # 第1个值大于0表示正相关，小于0表示负相关，等于0表示没有关系，|r|=1表示完全相关，|r|<0.4是低度相关，0.4 <=|r|<0.7是显著相关，0.7 <=|r|是高度相关
         # 第二个值当样本数超过500时有参考意义，越接近0表示相关性越强
         print(ret)
     ```

2.   embedded嵌入式，算法自动选择特征，比如决策树、L1正则化。

###### 3.3.3.2 主成分分析

降维时舍弃原有数据，创建新变量:

```python
from sklearn.decomposition import PCA


data = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]
# n_components传整数表示多留多少特征，小数表示保留特征的百分比
transfer = PCA(n_components=2)
transfer_data = transfer.fit_transform(data)
print("保留2列特征的结果:", transfer_data)

# 保留2列特征的结果: [[ 5.19615242  0.        ]
#  [-0.          0.        ]
#  [-5.19615242  0.        ]]
```

#### 3.4 机器学习

选择合适的算法对模型进行训练。使用梯度下降算法时涉及到3个概念：

1.   epoch：将整个训练集完整送入模型进行1次训练，称为1代训练；
2.   batch：将训练集分成多个小数据集，每个批次会被模型单独进行1次向前传播和反向传播；
3.   iteration：每次模型完成一个 Batch 的前向传播和反向传播，进行一次参数更新，称为一次 Iteration；

#### 3.5 模型评估

对训练好的模型进行评估，训练的好就上线，不好就重来一遍。

##### 3.5.1 分类评估方法

###### 3.5.1.1 混淆矩阵

测试集和预测结果可以组成四种情况：

|              | 预测结果正例 | 预测结果假例 |
| ------------ | ------------ | ------------ |
| 真实结果正例 | 真正例TP     | 伪反例FN     |
| 真实结果假例 | 伪正例FP     | 真反例TN     |

混淆矩阵中，真正例TP和真反例TN表示预测结果正确，通过混淆矩阵可以计算出几个指标：

*   准确率：$\dfrac {真正例TP + 真反例TN} {真正例TP + 真反例TN + 伪正例FP + 伪反例FN}$
*   精确率：$\dfrac 真正例TP {真正例TP + 伪正例FP}$
*   召回率：也叫查全率，$\dfrac 真正例TP {真正例TP + 伪反例FN}$
*   F1-score：用来反映模型的稳健性：$\dfrac {2真正例TP} {2真正例TP + 伪正例FP + 伪反例FN}$

上面这些指标在样本不均衡的时候是无法对模型进行评估的，比如一共有100条结果，99条真，1条假，如果机器不进行判断将所有结果都预测为真，精确率也能达到99%，召回率已经达到100%了，虽然模型在胡说，一般正例和反例的比例低于1/4就说名样本是**不均衡**的，不均衡的样本应该使用ROC曲线和AUC指标进行评估。

###### 3.5.1.2 ROC与AUC

ROC曲线由TPR和FPR组成，TPR就是召回率，当做纵坐标；FPR是$\dfrac 伪正例FP {伪正例FP + 真反例TN}$​，当做横坐标，ROC可以看出来分类器是否在胡说。

![ROC](https://raw.githubusercontent.com/wangxiaohong123/p-bed/main/uPic/ROC.jpg)

AUC是随机取一堆正负样本，正样本得分大于负样本得分的概率，AUC的范围在[0,1]之间，越接近1效果越好。

##### 3.5.2 交叉验证，网格搜索

交叉验证是说将数据集进一步划分，原来只分成训练集和测试集，现在需要将训练集分成验证集和训练集，比如将训练集分成4份，叫做**4折交叉验证**，然后对数据进行4次的测试，第一次前1/4当作验证集，第二次事1/2到1/4当作验证集……，将4次结果的平均值当作结果，这样可以**让模型更加准确，但是不能提高准确率**。

提高准确率需要使用网格搜索，算法中需要手动指定的参数叫做**超参数**，比如`KNeighborsClassifier(n_neighbors=5)`中的5就是超参数，网格搜索就是预先定义n个超参数，每组超参数都采用交叉验证来评估，最后选出最优的组合。

##### 3.5.3 欠拟合和过拟合

过拟合是在训练上表现好，测试上表现不好，这个时候就是模型过于复杂，出现过拟合；欠拟合就是训练和测试表现都不好，模型过于简单，出现欠拟合。

过拟合可能是因为特征过多，解决的办法是重新清洗数据、增加训练数据量、正则化和减少特征维度；解决欠拟合就是增加特征项。

###### 3.5.3.1 正则化

正则化是通过在损失函数中增加惩罚项对模型的参数进行约束，正则化分为2种

*   L1正则化：可以让一些模型参数变成0，比如$w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4^2$会变成$w_1x_1 + w_2x_2 + w_3x_3$​，也叫lasso回归；
*   L2正则化：认为一些参数是有用的，不会变成0，会变成一个比较小的常数值比如$w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4^2$会变成$w_1x_1 + w_2x_2 + w_3x_3 + b$。也叫岭回归（Ridge 回归）。

##### 3.5.4 损失函数

损失函数是用于衡量模型预测结果与真实结果之间差异的函数。损失过大的时候需要使用正规方程或者梯度下降进行优化。

###### 3.5.4.1 正规方程

$w = (x^Tx)^-1x^Ty$，X是特征值矩阵，y是目标值矩阵，-1表示逆矩阵。他是根据损失函数求偏导推到出来的。他可以求出误差最小的系数，但是求解逆矩阵是非常困难的，所以正规方程一般用到小数据集里。正规方程不能解决拟合问题。

###### 3.5.4.2 梯度下降

在单变量中，梯度表示某一点切线的斜率，也就是函数的微分；在多变量函数中，梯度是一个向量，梯度的方向就是函数在给定点上升最快的方向。$\theta_{i+1} = \theta_1 - \alpha{\frac \partial{\partial \theta_1}}J(\theta)$。α表示步长，即每次下降多少，太小容易增加计算复杂度，太大容易错过最低点，并且他没法保证找到的是最小值。

两种方法的demo：

```python
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, SGDRegressor
from sklearn.metrics import mean_squared_error

# 获取数据
boston = load_boston()

# 数据基本处理-分割
x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.2)

# 特征工程-标准化
transfer = StandardScaler()
x_train = transfer.fit_transform(x_train)
x_test = transfer.fit_transform(x_test)

# 机器学习-正规方程
# estimator = LinearRegression()
# 机器学习-梯度下降
estimator = SGDRegressor()
estimator.fit(x_train, y_train)
print("模型的偏置是:\n", estimator.intercept_)
print("模型的系数是:\n", estimator.coef_)

# 模型评估
y_pre = estimator.predict(x_test)
print("预测值是:\n", y_pre)
ret = mean_squared_error(y_test, y_pre)
print("均方误差:\n", ret)
```

##### 3.5.5 模型的保存和加载

使用joblib包实现模型的保存和加载：

```python
import joblib
# 保存模型
joblib.dump(estimator, '路径/xxx.pkl')
#加载模型
estimator = joblib.load('路径/xxx.pkl')
```

### 4 迁移学习

迁移学习是一种机器学习技术，它的核心思想是将从一个任务中学到的知识迁移到另一个相关的任务中，从而加速学习过程或提高模型的性能。说白了就是把别人训练好的模型，拿来改一改变成适合自己的。主要包含2个步骤：

预训练模型：一般情况下，预训练模型都是大模型，具备复杂的网络结构，众多的参数量以及足够大的数据集下进行训练产生的模型，常见的预训练模型：BERT及其变体、GPT、GPT-2、roBERTa、transformer-XL、XLNet、XLM、DistilBERT、ALBERT、T5、XLM-RoBERTa。

预训练模型的使用：首先安装依赖：

```shell
pip3 install tqdm boto3 requests regex sentencepiece sacremoses
```

使用pytorch.hub加载模型：

```python
import torch


# 选定预训练模型的来源
source = 'huggingface/pytorch-transformers'

# 选定加载模型的哪一部分,常用的4个选项：
# tokenizer：映射器
# model：不带头的模型，这个头不是多头注意力的头，而是表示输出层，不带头的意思是仅对输入文本进行特征表示
# modelWithLMHead：带语言模型头的模型
# ModelForSequenceClassification：带类型头的模型
# modelForQuestionAnswering：带问答头的模型
part = 'tokenizer'
# 加载模型的名称
model_name = 'bert-base-chinese'
# 加载预训练模型映射器
tokenizer = torch.hub.load(source, part, model_name)

# 将测试文本转换成模型可以处理的张量
token_tensor = torch.tensor([model.encode('测试文本')])
print("token_tensor:", token_tensor)
# 推理阶段不需要计算梯度，使用torch.no_grad()禁用梯度计算
with torch.no_grad():
    encoded_layers, _ = model(token_tensor)
# 输出结果，这是隐藏层的输出，不是最终的输出
print(encoded_layers)
```

微调(Fine-tuning)：根据给定的预训练模型，改变他的部分参数或者小分部结构，然后在小部分的数据集上训练，让整个模型更好的适应特定任务。所以微调也是一个训练过程。

### 5 下游任务

**下游任务**是指利用一个预训练模型在某些特定的应用场景中执行的任务，比如问答系统，机器翻译等。

1.   llama：Meta开源的语言模型，他可以让很少的权重参数达到大模型的效果。
2.   LoRA：正常的微调大模型需要重跑权重参数，LoRA可以实现微调时冻住大模型的权重参数，单独跑个小模型然后合并到大模型中。
3.   Self-Instruct：把样本数据变成方便理解，更能突出重点的过程叫润色，比如 “取快递”润色成“您好，请问我的快递在哪里”，Self-Instruct可以让都有一个标准的格式。

PEFT：Hugging Face将llama、LoRA和Self-Instruct整合到了一起。

