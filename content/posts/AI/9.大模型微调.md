---
title: 9.大模型微调
tags:
  - 大模型应用开发
categories: 人工智能
copyright: true
---

大语言中的大型主要体现在参数规模和训练数据量，一般参数规模达到1B(10亿)量级才叫大模型，只有达到这个量级才会有机遇Scaling law的涌现现象，涌现现象是大模型的魅力，有点像初中物理学的液态变固态。

模型的参数数量跟显存占比的计算，以OPT-6.7B举例：OPT-6.7B就是6.7Billion个参数，假设参数的类型是Float16，即每个参数占用16位（2字节）的显存。总显存占用=参数总量×每个参数的显存占用。总内存 = 67亿 \* 2 = 134亿字节。转换成GB就是134亿 / 1024 / 1024 / 1024 = 12.5GB的显存。

使用大显存的GPU加载整个模型可以加快训练速度，部署时也可以提高响应速度，但是可以只使用CPU+内存的方式训练或者部署，只不过这种方式的训练很慢，因为训练时需要大量的矩阵相乘操作。但是使用部署后的模型只是一个前向传播操作，CPU+内存的方式不会比GPU慢很多，除非是有并发量的批量推理，GPU的优势会很明显。

**现在是一个信息过载的时代，搜什么都会出现一堆，大模型工具的使用可以帮助我们筛选出有用的信息**

模型分类：自回归(`CAUSAL_LM`，文本生成任务，比如GPT)、序列分类(`SEQ_CLS`，情感分析、文本分类)、token级分类(`TOKEN_CLS`，命名实体识别NER)、问答任务(`QUESTION_ANS`)

#### 大模型应用4个阶段

##### 1提示词工程

面向的是终端用户，大模型时代的沟通手段，通过提示词从大模型挖掘知识。就是如何通过对话框跟大模型更好交流。大模型都是概率模型，很多能力他都没有，比如数学运算，但是我们可做到通过描述让他理解。这也是为什么基于注意力机制的模型很容易回答错误一个描述很复杂的小学数学应用题。

##### 2AI智能体（AI Agent）

基于ReAct范式，就是大模型自主判断应该使用哪些工具，比如chatGPT+联网搜索。分为3类：

1.   行动代理（Action agents）：自主决定使用工具，比如OpenAI的Function Call；
2.   模拟代理（Simulation agents）：通常设计用于模拟角色扮演，在模拟的环境中运行，比如生成式智能体，CAMEL。以后可能会应用在游戏领域，类似于美剧西部世界；
3.   自主智能体（Autonomous agents）：独立执行实现长期目标，比如Auto-GPT，manus（国产的，好像是Claude套壳）。

基于大模型开发应用的开发人员，比如自动客服，虚拟助手。

##### 3大模型微调（Fine-tuning）

在预训练模型的基础上，使用较小的数据集进一步训练来调整模型参数。当有和目标相关的较小的数据集，并且希望模型在这个任务上表现更好的时候使用。

未来是面向基础模型编程。

##### 4预训练技术（Pre-training）

使用大量的未标记数据（比如维基百科内容）来训练一个初步的模型，为后续的微调提供基础模型。适合有资源的大厂，有大量的数据集，数据清洗做的好，然后大力出奇迹。

能自己预训练模型的都是顶级大厂，因为需要的资源实在是太大了，比如LLaMA-65B就需要780G显存。

#### RAG

把我们从外部拿到的数据通过处理之后变成向量数据库中的知识。

### 微调技术路线

20年之前大家都不知道怎么去做微调，OpenAI发表了一篇论文提出调整prompt，让模型能更好的理解输入也能有很好的效果，再加上几年之后的文生图让prompt被大家熟知。

但是prompt有个缺点就是相同的prompt换一个模型或者换一个语言描述效果就会差很多，不管是在LangChain里或者在应用的对话框中都有这个问题。

1.   全量微调（FFT）：所有系数都进行调整，原来的VC相关的模型用的多一点，训练成本高，容易造成灾难性遗忘。
2.   高效微调（PEFT）：分为有监督微调（SFT）、基于人类反馈的强化学习（RLHF）、基于AI反馈的强化学习（RLAIF）。

#### PEFT

高效微调技术：Adapter Tuning(2019 Google) -> Prefix Tuning(2021 Stanford) -> Prompt Tuning(2021 Google) -> P-Tuning V1(2021 TsingHua, MIT) -> P-Tuning V2(2022 TsingHua, BAAI )

传统的模型微调是很容易的，比如分类的卷积网络中可以直接选择冻结卷积层，训练softmax层直接增加类别，2018年google的bert出来之后模型就已经不是CNN那种神经网络了，都是在叠加transformer的层数，整个模型看起来又宽又高，包括到今天的大语言模型中，哪部分参数干了哪些事也是未知的。

##### Adapter Tuning

bert是先做了一个语义理解能力很强的模型，然后在下游的具体任务上逐个的去微调模型，这中做法的成本相当高，做微调时需要把整个bert都加载到显存中。19年的时候google提出在transformer的Feed-forward layer层后增加一个adapter层，adapter层负责将高维向量转成低维，计算后在转回高维，只训练降维后的特征提升效率。

##### Prefix Tuning

21年的时候斯坦福大学发表了一篇论文，Prefix Tuning，这个是在整个transformer前增加一个Prefix模块，仅训练Prefix，冻结Transformer全部参数，这样可以不将整个模型都加载到显存中，可以降低算力和训练时间。

上面的微调都是有几类任务就需要微调几次，很麻烦。

##### Prompt Tuning

相当于是Prefix Tuning的简化版，只在输入层增加一些token，让模型能够更好的理解用户的输入。解决了Prefix Tuning实现困难的问题，更容易训练，而且效果很接近Prefix Tuning。相当于在模型之前增加一个新的模型，并且不需要区分任务种类，只需要增加一个通用的就可以。

##### P-Tuning V1

清华唐杰团队、杨志林和MIT一起发表的论文，优化了冻结模型的数据可能会导致模型过拟合的问题，他和Prefix Tuning的区别是P-Tuning更灵活，只在Input embedding层增加一些参数，且使用长短记忆网络加MLP进行初始化。

##### P-Tuning V2

Prompt Tuning和P-Tuning V1在模型参数较小的情况下表显不好，V2改善了一下。

上面的方法都没法兼顾高效和高质量，比如Adapter会增加模型深度，训练成本高，其他的会生成额外的token，会减少我们输入的token长度。

#### LoRA

低秩适配技术：LoRA(2021 微软) -> QLoRA(2023 华盛顿大学) -> AdaLoRA(2023 微软)

通过低秩分解将权重更新表示为两个较小的矩阵（更新矩阵），这些新矩阵可以在适应新数据的同时保持整体变化数量较少进行训练，原始矩阵保持冻结状态，并且不接受任何调整。最终结果通过原始权重和适应后的权重组合得到。

实际上是在原始的预训练模型旁增加一个附加的网络通路，可以看成外挂了矩阵A和矩阵B相乘来模拟征秩。

<img src="https://raw.githubusercontent.com/wangxiaohong123/p-bed/main/uPic/LoRA原理.png" alt="LoRA原理" style="zoom:25%;" />

假设模型某一层的矩阵大小是d_in × d_out，那么lora外挂的2个矩阵就是d_in × r和r × d_out，这个r就是秩。

适配时会将模型的原始权重+lora的适配矩阵，$ W_{final}=W_{pretrained}+\frac{α}{r}⋅(A×B) $，α是一个缩放因子，通过α和r控制低秩矩阵对原始权重的影响，α越大影响越大(微调越激进)。

相比PEFT的优势

1.   更少的可训练参数：LoRA通常只需要训练非常少的额外参数，相较于全参数微调或者某些PEFT方法，可以极大地减少需要更新的参数数量。
2.   更高的训练效率：由于减少了需要更新的参数，LoRA能够更快地收敛，并且对计算资源的需求更低。
3.   无推理延迟：LoRA不会增加推理时的延迟，因为低秩矩阵可以在运行时动态地与原模型权重相加或相乘，无需改变模型结构或重新存储整个模型。
4.   灵活的模块化适应：LoRA允许创建轻量级、特定任务的适配器，这些适配器可以在不修改基础模型架构的情况下进行互换，便于多任务学习和任务切换。
5.   稳健的知识保留：通过冻结预训练权重，LoRA有助于减轻灾难性遗忘的问题，即在学习新任务时丢失之前学到的知识。

##### AdaLoRA

使用SVD提升矩阵低秩分解性能，动态调整不同权重矩阵的本征秩r。

SVD（奇异值分解）是一种矩阵分解技术，可以将任意一个矩阵分解为：左奇异向量矩阵、奇异值矩阵和右奇异向量矩阵。可以实现降维、数据压缩、噪声过滤、提取文本数据的潜在语义结构等。

##### QLoRA

在LoRA的基础上增加了量化，引入了 **NormalFloat（NF4）**，这是一种基于统计分布设计的非对称、4-bit 数据类型。NF4 能更好地适应权重的分布特性，在精度损失最小的前提下实现更高的压缩率。

### Hugging Face Transformers使用

开发环境：miniconda，python3.11

 Transformers库提供了几千个预训练模型，并且对Jax、PyTorch、TensorFlow等支持很友好，活跃度非常高。

#### pipelines库

通过pipelines可以让我们很方便的调用现成的模型。他是先将输入变成模型能理解的向量，向量可以解决不同的语言的词表达相同意思的问题。转成向量之后交给模型，在经过一个后处理输出结果，不同的模型后处理不同。

![transformers-pipeline](https://raw.githubusercontent.com/wangxiaohong123/p-bed/main/uPic/transformers-pipeline.jpg)

```python
from transformers import AutoTokenizer, AutoModel
from transformers import pipeline


# 手动拉取tokenizer和模型
tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")
model = AutoModel.from_pretrained("bert-base-chinese")

# 保存修改的tokenizer和模型
tokenizer.save_pretrained("路径")
model.save_pretrained("路径")

# 使用pipeline调用gpt2模型
prompt = "吧啦吧啦一段话"
generator = pipeline(task="text-generator", model="gpt2")
# 生成2句回应，每个句子最大长度为16
print(generator(prompt, num_return_sequences=2, max_length=16))
```

#### datasets库

用于快速加载、处理的公共数据集。

```python
from datasets import load_dataset
import torch
from torch.utils.data import DataLoader

# 加载GLUE中的SST-2情感分析任务数据集
dataset = load_dataset("glue", "sst2")
# 加载本地 CSV 文件
dataset = load_dataset("csv", data_files={"train": "data/train.csv", "test": "data/test.csv"})

# 查看训练集
print(dataset["train"][:5])

# 转换为 PyTorch Dataset
train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8)
```

#### Trainer库

封装了完整训练循环的高级类，可以简化模型训练、评估、预测等流程。

```python
from transformers import TrainingArguments, Trainer


# 训练
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)
trainer.train()

# 保存模型，后续可以通过 from_pretrained() 方法重新加载
trainer.save_model(model_dir)
# 保存训练状态，一般和try-except结合，防止训练中断
trainer.save_state()
```

#### evaluate库

提供一系列标准评估指标，如准确率、F1、BLEU、ROUGE 等。

```python
import evaluate
import numpy as np

# 加载评估指标-准确率
accuracy = evaluate.load("accuracy")

# 计算准确率
labels = small_eval_dataset[:]["label"]
preds = np.argmax(predictions.predictions, axis=-1)
acc = accuracy.compute(predictions=preds, references=labels)
print(f"Accuracy: {acc['accuracy']}")
```

#### PEFT库

提供微调支持。

```python
from peft import LoraConfig, PeftModel, PeftConfig, TaskType, get_peft_model

peft_config = LoraConfig(
    task_type=TaskType.SEQ_CLS,   # 任务类型：序列分类
    inference_mode=False,         # 训练模式
    r=8,                          # LoRA秩
    lora_alpha=16,                # 缩放因子
    lora_dropout=0.1              # dropout率
)

# 将 PEFT 层添加到原始模型中
model = get_peft_model(model, peft_config)
```

#### Transformers微调demo

##### 文本分类

```python
from transformers import AutoTokenizer
from datasets import load_dataset
import random
import pandas as pd
import datasets
from transformers import AutoModelForSequenceClassification
import numpy as np
import evaluate
from transformers import TrainingArguments, Trainer


def show_random_elements(dataset, num_examples=10):
    """
    查看数据集
    :param dataset: 目标数据集
    :param num_examples: 输出条数
    :return:
    """
    assert num_examples <= len(dataset), f"数据集长度 {len(dataset)} 小于 num_examples {num_examples}"
    picks = []
    for _ in range(num_examples):
        pick = random.randint(0, len(dataset) - 1)
        while pick in picks:
            pick = random.randint(0, len(dataset) - 1)
        picks.append(pick)

    df = pd.DataFrame(dataset[picks])
    for column, typ in dataset.featrues.items():
        if isinstance(typ, datasets.ClassLabel):
            df[column] = df[column].transform(lambda i: typ.names[i])
    print(df)


# 下载数据集
dataset = load_dataset("yelp_review_full")
# 查看下载的数据集格式
show_random_elements(dataset["train"])

# 数据预处理
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")


def tokenize_function(examples):
    """
    对超过长度的文本进行截断,长度不够的进行填充
    """
    return tokenizer(examples["text"], padding="max_length", truncation=True)


# datasets的map方法支持在整个数据集上应用预处理函数
tokenized_datasets = dataset.map(tokenize_function, batched=True)
# 删除原始文本，节省内存
tokenized_datasets = tokenized_datasets.remove_columns(["text"])
# 查看预处理后的数据集
show_random_elements(tokenized_datasets["train"], num_examples=1)

# 数据抽样，全量跑的时间太长，测试的时候就取一点
small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))

# 加载模型,数据集是5分类，所以这里指定num_labels也是5
# AutoModelForSequenceClassification 是专门为序列分类任务设计的,相对于AutoModel，AutoModelForSequenceClassification多了一些处理，比如分类操作，将概率转化为标签
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=5)
model_dir = "models/bert-base-cased-finetune-yelp"

# evaluation_strategy是评估策略，每完成1个epoch（遍历一次整个训练集）进行一次评估
# batch size是16，如果是单核GPU的话就是每次向模型输入16条数据；epoch是3，整个数据集会被训练3次
# logging_steps 默认值为500，根据我们的训练数据和步长，将其设置为每30步记录一次日志，比如只有1核GPU，就是每训练30 * 16条数据后记录一次日志
# 完整的参数：https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/trainer#transformers.TrainingArguments
# 配置在源码中的定义：https://github.com/huggingface/transformers/blob/v4.36.1/src/transformers/training_args.py#L161
training_args = TrainingArguments(output_dir=model_dir,
                                  eval_strategy="epoch",
                                  per_device_train_batch_size=16,
                                  num_train_epochs=3,
                                  logging_steps=30)

# 简单的加载一个准确率指标
metric = evaluate.load("accuracy")


def compute_metrics(eval_pred):
    """
    统计模型能力，这里只根据准确率评估
    :param eval_pred: 模型输出
    :return: 概率最高的标签
    """
    # labels是实际的标签
    # logits是2维数组，行数是batch size，列数是分类大小，就是说每行都是对一个样本的所有标签的分数预测
    logits, labels = eval_pred
    # 获取样本得分最高的标签，就是模型预测的标签
    predictions = np.argmax(logits, axis=-1)
    # 使用模型预测的标签和真实标签比较，得到准确率
    return metric.compute(predictions=predictions, references=labels)


# 开始训练
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)
trainer.train()

# 保存模型，后续可以通过 from_pretrained() 方法重新加载
# 训练完就要保存，要不后面抛异常就白训练了
trainer.save_model(model_dir)
# 保存训练状态，一般和try-except结合，防止训练中断
# trainer.save_state()

small_test_dataset = tokenized_datasets["test"].shuffle(seed=64).select(range(100))
test_results = trainer.evaluate(small_test_dataset)
print(f"Test Accuracy: {test_results['eval_accuracy']:.4f}")
```

##### 量化

量化就是对模型中系数（权重、激活值等）的压缩，用较少的信息表示数据，同时尽量不降低太多准确性。量化主要分2种：

1.   后训练量化（PTQ）：在模型训练后降低系数的精度，就好比打印东西，本来是10页，为了省纸改小字体变成5页了。没有降低训练成本，可以节省部署资源。GPTQ、AWQ、BitsAndBytes(BNB)等。
2.   量化感知训练（QAT）：训练时就考虑量化，效果更好，并且微调时训练成本更低，PyTorch Quantization Toolkit、NVIDIA TensorRT等。

当权重很大时，训练过程中矩阵相乘后产生的小数位会不断加长，这个时候量化后的精度过低就会导致精度丢失，也是梯度消失，所以模型的系数越大，参数的数据类型也越大。

**如果需要加载量化后的模型(比如bitsandbytes)必须要在Intel CPU上**。

##### LoRA微调demo

```python
import torch
from transformers import GPT2Tokenizer, AutoConfig, AutoModelForCausalLM, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model
from datasets import load_dataset, ClassLabel, Sequence


model_id = "facebook/opt-125m"
# 加载模型
# 如果要加载量化后的模型可以使用model = AutoModelForCausalLM.from_pretrained(model_id, load_8bit=True)
# 这种写法会使用bitsandbytes，但是必须要在Intel GPU基础上，下面的代码使用torch_dtype=torch.float16也可以减少内存的占用
# 目前bitsandbytes要比quanto或者pytorch自带的量化工具效率高
model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map="cpu")
tokenizer = GPT2Tokenizer.from_pretrained(model_id)

# 配置 LoRA
peft_config = LoraConfig(
    r=8, # LoRA的秩
    lora_alpha=16, # 适应的比例因子
    target_modules=["q_proj", "v_proj"], # 对哪些模块加 LoRA，可选的还有k_proj、out_proj
    lora_dropout=0.1, # 随机失效的神经元概率，防止过拟合，仅作用于lora的适配矩阵
    bias="none", # 不使用偏置，可选"all"、"lora_only"
    task_type="CAUSAL_LM" # 表示这是因果(自回归)语言模型任务
)
# 自动包装模型，适配 LoRA（无需 prepare_model_for_int8_training）
model = get_peft_model(model, peft_config)
# 看一下模型大小
print(f"{model.get_memory_footprint() / (1024 ** 3):.2f}GB")

# 数据集处理
dataset = load_dataset("Abirate/english_quotes")
tokenized_dataset = dataset.map(lambda samples: tokenizer(samples["quote"]), batched=True)
from transformers import DataCollatorForLanguageModeling
# 数据集收集器，处理语言模型数据，mlm设置为不使用掩码语言模型
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)

# 微调
model_dir = "models"
training_args = TrainingArguments(output_dir=f"{model_dir}/{model_id}-lora",
                                  learning_rate=2e-4, # 学习率，定义梯度下降更新参数时的步长
                                  per_device_train_batch_size=16,
                                  fp16=True, # 使用混合精度模型，上面设置了存的时候使用int8精度，但是计算的时候还是使用f16
                                  max_steps=2, # 最大训练步长
                                  logging_steps=30,
                                 save_safetensors=False) # 有些模型会出现多个模块复用权重，训练时重复保存相同的权重会报错，需要把save_safetensors关掉
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    data_collator=data_collator
)
trainer.train()

# 保存和使用
model.save_pretrained(f"{model_dir}/{model_id}-lora-int8")

lora_model = trainer.model
text = "two things are infinite:"
inputs = tokenizer(text, return_tensors="pt")
out = lora_model.generate(**inputs, max_new_tokens=48)
print(tokenizer.decode(out[0], skip_special_tokens=True))
```
