---
title: 4.tensorFlow
tags:
  - 深度学习
categories: 人工智能
copyright: true
---

PyTorch和TensorFlow是深度学习框架，但它们也具备科学计算库的特性。

#### Tensorflow

TensorFlow是谷歌的深度学习框架，可以用在计算机视觉、音频处理、推荐系统、自然语言处理等场景。向下支持做的很烂，**现在用的少。**

##### 一、安装

mac(m1/m2)安装：

```shell
# 如果提示Could not find a version that satisfies the requirement tensorflow-macos (from versions: none)升级pip
python3 -m pip install --upgrade pip

pip3 install tensorflow-macos
# 苹果为支持 M1/M2 芯片上的图形加速，提供的插件
pip3 install tensorflow-metal
```

其他版本安装：

```shell
# CPU版本
pip3 install tensorflow
# GPU版本
pip3 install tensorflow-gpu
```

验证安装结果：

```python
import tensorflow as tf
print(tf.__version__)
```

##### 二、基础

###### 1. 张量（Tensor）

张量就是多维数组，他和numpy可以相互转换:

```python
import tensorflow as tf
import numpy as np


# 创建int32类型的0维张量，就是被封装到tensor对象里的常量
rank_0_tensor = tf.constant(4)
print(rank_0_tensor)
# 创建1维张量
rank_1_tensor = tf.constant([2, 3, 4])
print(rank_1_tensor)

# 转换成numpy数组
np1 = np.array(rank_1_tensor)
print(np1)
# 转换成numpy数组
np2 = rank_1_tensor.numpy()
print(np2)
```

###### 2. 常用函数

```python
import tensorflow as tf


a = tf.constant([[1, 2], [3, 4]])
b = tf.constant([[5, 6], [7, 8]])

#张量加法
print(tf.add(a, b))
# 张量乘法
print(tf.multiply(a, b))
# 矩阵相乘
print(tf.matmul(a, b))
# 求和
print(tf.reduce_sum(a))
# 平均值
print(tf.reduce_mean(a))
# 最大值
print(tf.reduce_max(a))
# 最大索引
print(tf.argmax(a))
```

维度和长度不可变的张量叫变量，声明方式不同：

```python
tf.Variable([[1, 2], [3, 4]])
```

###### 3. 常见激活函数

*   Sigmoid/logistics：输入小于0时值趋近于0，输入大于0时值趋近于1，当输入很大或者很小的时候由于很熟的斜率非常小，会出现**梯度消失**现象(5个隐藏层内就会出现)，所以使用不多，一般用于二分类。
*   tanh(双曲正切曲线)：他和Sigmoid不同的是中心点，Sigmoid输入0时输出0.5，tanh输入0时输出0。所以他也会有**梯度消失**现象。
*   RELU：他的表达式就是max(0, x)，它的优点就是速度快，没有**梯度消失**现象。他是部分线性函数。
*   LeakRelu：relu函数有神经元死亡的问题，LeakRelu的表达式是max(0.1x, x)，这样输出0的数量会小很多，减少神经元死亡次数。
*   SoftMax：用在多分类过程中的输出层，输出每种分类的概率。

参数

参数分两个，权重和偏置。偏置的初始化可以设置成0，权重的初始化：

*   随机初始化：从正态分布中随机取值；
*   标准初始化：从均匀分布中随机取值，$\dfrac {-1}{\sqrt d}到\dfrac {1}{\sqrt d}$中随机取，d是输入参数的数量；
*   Xavier/Glorot初始化：思想是激活值和状态值梯度的方差在传播过程中保持一致。也是从正态分布或者均匀分布中随机取样，只不过标准差由输入和输出神经元个数决定；
*   He初始化：何恺明提出的。他和Xavier的区别是正向传播时激活值的方差保持不变，反向传播时状态值的梯度方差保持不变。

###### 4. 模型构建方式

Sequential：可以构建简单的序列模型：

```python
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Input


model = Sequential([
    # 定义输入形状，旧版本是Dense的input_shape属性
    Input(shape=(4,)),
    # 隐藏层1
    Dense(10, activation="relu"),
    # 隐藏层2
    Dense(10, activation="relu"),
    # 输出层
    Dense(3, activation="softmax")
])
```

function API构建：将层作为可调用对象并返回张量，可以构建复杂的序列模型

```python
from tensorflow.keras import Model
from tensorflow.keras.layers import Dense, Input


inputs = Input(shape=(4,), name="input")
# 隐藏层1
x = Dense(10, activation="relu", name="layer1")(inputs)
# 隐藏层2
x = Dense(10, activation="relu", name="layer2")(x)
outputs = Dense(3, activation="softmax", name="output")(x)
model = Model(inputs=inputs, outputs=outputs, name="functional_model")
```

Model子类方式：

```python
from tensorflow.keras import Model
from tensorflow.keras.layers import Dense


class MyModel(Model):
    def __init__(self):
        super(MyModel, self).__init__()
        # 隐藏层1
        self.layer1 = Dense(10, activation="relu", name="layer1")
        # 隐藏层2
        self.layer2 = Dense(10, activation="relu", name="layer2")
        # 输出层
        self.layer3 = Dense(3, activation="softmax", name="output")

    def call(self, inputs):
        x = self.layer1(inputs)
        x = self.layer2(x)
        output = self.layer3(x)
        
        
model = MyModel()
```

###### 5. keras

用来训练、测试和生成模型。包含模块：

| 名称          | 功能                                                         |
| ------------- | ------------------------------------------------------------ |
| activation    | 激活函数，没有激活函数神经网络只能处理线性问题               |
| application   | 包含许多预训练模型，可以用来做基础网络或者特征提取           |
| callbacks     | 训练过程中执行特定任务的回调                                 |
| datasets      | 提供了几个常用的数据集，供机器学习和深度学习实验使用(大设备或者夸设备训练使用) |
| layers        | 定义和创建神经网络层                                         |
| losses        | 提供损失函数                                                 |
| metrics       | 评估模型性能                                                 |
| models        | 创建和训练神经网络模型的类                                   |
| optimizers    | 优化器，提供优化算法，比如梯度下降                           |
| preprocessing | 数据加载、预处理和增强                                       |
| reguarizers   | 提供正则化方法，防止过拟合                                   |
| utils         | 一些数据处理、可视化的工具                                   |

###### 6. demo

```python
import seaborn as sns
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras import utils
import ssl


# 如果报SSL验证错误加上下面的代码跳过SSL验证
ssl._create_default_https_context = ssl._create_unverified_context
# 加载数据
iris = sns.load_dataset("iris")

# 提取特征值和目标值
x = iris.values[:, :4]
# 目标值转数字
y = LabelEncoder().fit_transform(iris.values[:, 4])

# 数据处理-训练集和测试集划分
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)
# 数据处理-特征值类型转换
x_train = np.array(x_train, dtype=np.float32)
x_test = np.array(x_test, dtype=np.float32)
# 数据处理-目标值热编码
y_train_ohe = utils.to_categorical(y_train, num_classes=3)
y_test_ohe = utils.to_categorical(y_test, num_classes=3)

# 构建模型
model = Sequential([
    # 定义输入形状，旧版本是Dense的input_shape属性
    Input(shape=(4,)),
    # 隐藏层1
    Dense(10, activation="relu"),
    # 隐藏层2
    Dense(10, activation="relu"),
    # 输出层
    Dense(3, activation="softmax")
])
# 可视化模型
# utils.plot_model(model, show_shapes=True)

# 模型编译
# 优化器:adam;损失函数:categorical_crossentropy(交叉熵);评价指标:accuracy
model.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])
# 模型训练
model.fit(x_train, y_train_ohe, epochs=10, batch_size=1, verbose=1)
# 模型评估
loss, accuracy = model.evaluate(x_test, y_test_ohe, verbose=1)
print("准确率是:\n", accuracy)
```

#### PyTorch

PyTorch的Tensor和NumPy的ndarray在功能和接口上非常相似，但PyTorch并不是基于NumPy实现的。PyTorch对于tensorflow的优势在于调试和复杂网络结构，目前主流的做法是使用PyTorch实现原型，使用Tensorflow进行分布式训练和部署，不过PyTorch的部署上也越来越完善了。20年之后没啥人用tensorflow，除了Google自己的项目。

CPU版本安装：

```shell
pip3 install torch torchvision
```

验证安装结果：

```python
import torch

print(torch.__version__)
# 查看GPU是否支持
print(torch.cuda.is_available())
```

##### 一、基础

```python
import torch
# 创建一个类型是long的5行3列的全0张量
torch.zeros(5, 3, dtype=torch.long)
# 通过数据直接创建张量
x = torch.tensor([1.1, 3.5])
# 批量将数组转成张量
x_train_test, y_train_test, x_valid_test, y_valid_test = map(torch.tensor, (x_train, y_train, x_valid, y_valid))
# 通过已有张量，创建新张量
x = x.new_ones(5, 3, dtype=torch.float)
# 张量加法
res = torch.add(x, y)
# 张量加法,结果存到y中
y.add_(x)
# 改变张量的形状，将5行3列改成3行5列
# 改变形状要保证元素个数一致
y = x.view(3, 5)
# 将张量转成numpy数组
# 转换之后n和x指向同一个内存地址，操作n，x的值也会改变
n = x.numpy()

# 将numpy数组转成torch张量
# 转换之后a和b指向同一个内存地址
import numpy as np
a = np.ones(5)
b = torch.from_numpy(a)

# 定义一个GPU设备
device = torch.device('cuda')
# 在GPU上创建一个张量
# 之前的张量默认在CPU上，和g1没法互通
g1 = torch.ones(5, device=device)
# 将x转到GPU上
x = x.to(device)
# 将g1转到CPU上
g2 = g1.to('cpu', torch.double)
```

###### 1 autograd

Autograd是PyTorch的自动微分引擎，通过动态计算图，根据链式法则自动追踪和计算每个张量的梯度。因为梯度就是损失函数对每个参数的导数，也就是微分，所以Autograd叫自动微分引擎。核心概念：

1.   Tensor： Autograd 追踪计算的核心数据结构，如果一个 Tensor的`requires_grad=True`，PyTorch将**追踪该张量的所有运算**，以便后续进行梯度计算。
2.   计算图：前向传播时发生张量之间的计算会构建计算图，在调用`.backward()`时，Autograd 将沿着计算图自动进行梯度计算。
3.   backward()：触发反向传播的核心函数，从目标张量开始，**反向计算所有相关张量的梯度**。
4.   grad_fn属性：每个Tensor都有grad_fn属性，表示当前Tensor是如何得到的，如果Tensor是手动创建的比如`torch.rand(5, 3, requires_grad=True)`，grad_fn为None，如果Tensor是通过计算得到的，grad_fn将记录计算的操作。
5.   requires_grad_()：动态设置Tensor的requires_grad属性。

###### 2 回归任务demo

```python
import torch
import pandas as pd
import numpy as np
from sklearn import preprocessing
# 过滤警告
import warnings

warnings.filterwarnings("ignore")

featrues = pd.read_csv('temps.csv')
# 将星期几转成独热编码
featrues = pd.get_dummies(featrues)

# 标签
labels = np.array(featrues['actual'])
# 删除特征中的标签
featrues = featrues.drop('actual', axis=1)
featrues_list = list(featrues.columns)
featrues = np.array(featrues)

# 标准化处理，因为样本数据中有的列数很大(年份)有的列数很小(温度),防止模型在学习的时候把年份当做重要特征需要对数据进行标准化
input_featrues = preprocessing.StandardScaler().fit_transform(featrues)

input_size = input_featrues.shape[1]
hidden_size = 128
output_size = 1
batch_size = 16
my_nn = torch.nn.Sequential(
    torch.nn.Linear(input_size, hidden_size),
    torch.nn.Sigmoid(),
    torch.nn.Linear(hidden_size, output_size),
)
cost = torch.nn.MSELoss(reduction='mean')
optimizer = torch.optim.Adam(my_nn.parameters(), lr=0.001)

# 损失值
losses = []
for i in range(1000):
    batch_loss = []
    # MINI-Batch方法来进行训练
    for start in range(0, len(input_featrues), batch_size):
        end = start + batch_size if start + batch_size < len(input_featrues) else len(input_featrues)
        x = torch.tensor(input_featrues[start:end], dtype=torch.float, requires_grad=True)
        y = torch.tensor(labels[start:end], dtype=torch.float, requires_grad=True)
        prediction = my_nn(x)
        # 计算损失
        loss = cost(prediction, y)
        # 反向传播更新权重
        optimizer.zero_grad()
        loss.backward(retain_graph=True)
        optimizer.step()
        batch_loss.append(loss.data.numpy())

    # 打印损失
    if i % 100 == 0:
        losses.append(np.mean(batch_loss))
        print(i, np.mean(batch_loss))

# 测试
xx = torch.tensor(input_featrues, dtype=torch.float)
predict = my_nn(xx).data.numpy()
```

###### 3 图像分类demo

```python
import os
import torch
import torch.nn as nn
import torch.optim as optim
# transforms数据量少的时候做增强
# models现成的模型，比如resnet
# datasets读取数据
from torchvision import datasets, transforms, models
import time
import copy
import warnings

warnings.filterwarnings("ignore")

data_dir = './flower_data/'
train_dir = data_dir + 'train'
valid_dir = data_dir + 'valid'

# 定义图像预处理操作
data_transforms = {
    'train': transforms.Compose([
        # 统一图片的尺寸
        transforms.Resize([128, 128]),
        # 数据增强-随机在145到45度间旋转
        transforms.RandomRotation(45),
        # 数据增强-数据增强-随机选择64 * 64的区域裁剪
        transforms.CenterCrop(64),
        # 数据增强-50%概率水平翻转
        transforms.RandomHorizontalFlip(p=0.5),
        # 数据增强-50%概率垂直翻转
        transforms.RandomVerticalFlip(p=0.5),
        # 亮度、对比度、饱和度啥的，不重要
        transforms.ColorJitter(brightness=0.2, contrast=0.1, saturation=0.1, hue=0.1),
        # 将图片转成标量
        transforms.ToTensor(),
        # 数据集比较小的时候均值和标准差不太准，直接大数据集的来用，3个数值对应通道数R-G-B
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'valid': transforms.Compose([
        transforms.Resize([64, 64]),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
}

batch_size = 32

# 指定数据集所在文件夹和与处理操作
image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'valid']}
# 指定读取数据操作
dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True) for x in
               ['train', 'valid']}
# 下面2个计算准确率使用
dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'valid']}
class_names = image_datasets['train'].classes

# 使用resnet训练好的特征
featrue_extract = True

# 是否使用GPU
train_on_gpu = torch.cuda.is_available()
device = torch.device("cuda:0" if train_on_gpu else "cpu")


# 迁移学习-冻结权重参数，一般数据集小的时候会全部冻结
def set_param_requires_grad(model, featrue_extracting):
    if featrue_extracting:
        for param in model.parameters():
            param.requires_grad = False


def initialize_model(num_classes, use_pretrained=True):
    # 使用resnet模型,18是层数,条件好的可以选大点的层数
    model_ft = models.resnet18(pretrained=use_pretrained)
    set_param_requires_grad(model_ft, featrue_extract)
    num_ftrs = model_ft.fc.in_featrues
    # 输出层的类别数根据自己数据定义
    model_ft.fc = nn.Linear(num_ftrs, num_classes)
    return model_ft, 128


model_ft, input_size = initialize_model(102, use_pretrained=featrue_extract)
model_ft = model_ft.to(device)

# 要跟新权重的名称
params_to_update = model_ft.parameters()
if featrue_extract:
    params_to_update = []
    for name, param in model_ft.named_parameters():
        if param.requires_grad:
            params_to_update.append(param)

# 优化器
optim_ft = optim.Adam(params_to_update, lr=1e-2)
# 学习率衰减，每10个epoch衰减为原来的1/10
scheduler = optim.lr_scheduler.StepLR(optim_ft, step_size=10, gamma=0.1)
criterion = nn.CrossEntropyLoss()


def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, filename='best.pt'):
    """
    模型训练函数
    :param model: 模型
    :param dataloaders: 数据读取操作
    :param criterion: 损失函数
    :param optimizer: 优化器
    :param num_epochs: 迭代次数
    :param filename: 模型名
    """
    since = time.time()
    # 标记效果最好的1次
    best_acc = 0
    model.to(device)
    # 训练过程中打印的一些指标
    val_acc_history = []
    train_acc_history = []
    train_losses = []
    val_losses = []
    # 学习率
    LRs = [optimizer.param_groups[0]['lr']]
    best_model_wts = copy.deepcopy(model.state_dict())

    for epoch in range(num_epochs):
        print('Epoch {}/{}'.format(epoch, num_epochs - 1))
        print('-' * 10)

        for phase in ['train', 'valid']:
            if phase == 'train':
                model.train()
            else:
                model.eval()

            running_loss = 0.0
            running_corrects = 0

            for inputs, labels in dataloaders[phase]:
                inputs = inputs.to(device)
                labels = labels.to(device)

                optimizer.zero_grad()
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                _, preds = torch.max(outputs, 1)
                # 训练集更新权重
                if phase == 'train':
                    loss.backward()
                    optimizer.step()

                # 计算损失
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

            epoch_loss = running_loss / len(dataloaders[phase].dataset)
            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)

            time_elapsed = time.time() - since
            print('time elapsed {:.0f}m {:.0f}s}'.format(time_elapsed // 60, time_elapsed % 60))
            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))

            # 如果当前模型比最好的强就替换
            if phase == 'valid' and epoch_acc > best_acc:
                best_acc = epoch_acc
                best_model_wts = copy.deepcopy(model.state_dict())
                state = {
                    'state_dict': model.state_dict(),
                    'best_acc': best_acc,
                    'optimizer': optimizer.state_dict(),
                }
                torch.save(state, filename)
            if phase == 'valid':
                val_acc_history.append(epoch_acc)
                val_losses.append(epoch_loss)
            if phase == 'train':
                train_acc_history.append(epoch_acc)
                train_losses.append(epoch_loss)

            print('Optimization learning rate: {:.7f}'.format(optimizer.param_groups[0]['lr']))
            LRs.append(optimizer.param_groups[0]['lr'])
            scheduler.step()

        time_elapsed = time.time() - since
        print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))
        print('Best val Acc: {:4f}'.format(best_acc))

        # 训练完之后用最好的一次当模型的最终结果
        model.load_state_dict(best_model_wts)
        return model, val_acc_history, train_losses, train_acc_history, val_losses, LRs


# 训练模型
model, val_acc_history, train_losses, train_acc_history, val_losses, LRs = train_model(model_ft, dataloaders, criterion, optim_ft)


# 如果训练的时候断了，加载训练好的模型
model_ft, input_size = initialize_model(102, use_pretrained=featrue_extract)
model_ft = model_ft.to(device)
checkpoint = torch.load('best.pt')
best_acc = checkpoint['best_acc']
model_ft.load_state_dict(checkpoint['state_dict'])
```

###### 4 自定义dataloader

图像分类demo里每个分类单独放到了一个文件夹里，很多时候拿到的数据不是分好类的，所有数据都存在一个文件夹里，但是会有一个文件名和类别的对照表，对照表可能是txt格式，excel或者json等等。

比如对照表是txt文件，每行是文件名 + " " + 类别的读取：

```python
import numpy as np
import os
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import torch
import warnings

warnings.filterwarnings("ignore")


class FlowersDataset(Dataset):
    def __init__(self, root_dir, ann_file, transform=None):
        """
        :param transform: 自定义预处理操作
        """
        self.ann_file = ann_file
        self.root_dir = root_dir
        self.img_label = self.load_annotations()
        self.img = [os.path.join(self.root_dir, img) for img in list(self.img_label.keys())]
        self.label = [label for label in self.img_label.values()]
        self.transform = transform

    def __len__(self):
        return len(self.img)

    def __getitem__(self, idx):
        image = Image.open(self.img[idx])
        label = self.label[idx]
        if self.transform:
            image = self.transform(image)
        label = torch.from_numpy(np.array(label))
        return image, label

    def load_annotations(self):
        data_infos = {}
        with open(self.ann_file) as f:
            samples = [x.strip().split(' ') for x in f.readlines()]
        for filename, gt_label in samples:
            data_infos[filename] = np.array(gt_label, dtype=np.int32)

        return data_infos


# 定义图像预处理操作
data_transforms = {
    'train': transforms.Compose([
        # 统一图片的尺寸
        transforms.Resize([128, 128]),
        # 数据增强-随机在145到45度间旋转
        transforms.RandomRotation(45),
        # 数据增强-数据增强-随机选择64 * 64的区域裁剪
        transforms.CenterCrop(64),
        # 数据增强-50%概率水平翻转
        transforms.RandomHorizontalFlip(p=0.5),
        # 数据增强-50%概率垂直翻转
        transforms.RandomVerticalFlip(p=0.5),
        # 亮度、对比度、饱和度啥的，不重要
        transforms.ColorJitter(brightness=0.2, contrast=0.1, saturation=0.1, hue=0.1),
        # 将图片转成标量
        transforms.ToTensor(),
        # 数据集比较小的时候均值和标准差不太准，直接大数据集的来用，3个数值对应通道数R-G-B
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'valid': transforms.Compose([
        transforms.Resize([64, 64]),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
}

train_dataset = FlowersDataset(root_dir='./flower_data/train', ann_file='./flower_data/train.txt', transform=data_transforms['train'])
val_dataset = FlowersDataset(root_dir='./flower_data/valid', ann_file='./flower_data/valid.txt', transform=data_transforms['valid'])

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)
val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)
```

