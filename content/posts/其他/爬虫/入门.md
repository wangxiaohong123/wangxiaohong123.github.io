---
title: 爬虫-1.入门
tags:
  - 爬虫
categories: 其他
copyright: true
---

任何语言都可以进行爬虫，但是python最方便，所以一般人都用python进行爬虫。

爬虫的时候只要不破坏网站、不侵犯用户隐私时不犯法的。

robots协议(君子协议):一般网站的主url后拼接robots.txt输出的内容为不允许爬的，爬的时候需要遵守。

### 一 爬虫用到的模块

有的时候我们拿到的资源是部分url，需要自己拼域名，不过部分url前面会出现有斜线'/'和没斜线的情况，有斜线的拼主域名就可以，没有斜线的是相对路径，需要把当前路的访问url取然后在拼接，非常麻烦，使用urllib就很简单：

```python
from urllib.parse import urljoin

herf = urljoin("当前访问url", "要拼接的url")
```

###### a.time模块

time模块常用的两个功能：

```python
import time

# 获取当前的秒级时间戳
t = time.time()
# 毫秒时间戳
t = int(time.time() * 1000)

# 让线程休眠5s
time.sleep(5)
```

###### b.os模块

```python
import os

# 创建目录
os.makedirs("/url/subUrl")

# 该url是否是已存在路径
os.path.isDir("url")
```

###### c.json模块

```python
import json

# 将json字符串转换成字典
d = json.loads(s)

# 把字典转成json字符串
s = json.dumps(dic)
```

###### d.random模块

```python
import random

# 生成10到20的随机数
i = random.randint(10, 20)

# 生成1到3的随机小数
f = random.uniform(1, 3)
```

###### e .requests模块(需要安装)

```python
import requests

# 请求头伪装，有的网站会根据请求头判断是否是爬虫，我们就是要尽可能装成浏览器
header = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36"
}

# get请求
url = "https://www.sogou.com/web"
param = {
    "query": "周杰伦"
}
resp = requests.get(url, headers=header, params=param)

# post请求
url = "https://fanyi.baidu.com/sug"
data = {
    "kw":"hello"
}
resp = requests.post(url, data=data)

# 文本格式获取响应
print(resp.text)
# json格式获取响应
print(resp.json())
# 获取字节内容，可以用这种方式获取媒体然后输出到图片
```

##### f.多线程和协程

进程是资源单位，线程是执行单位，爬虫中多线程用处不大，很多网站会限流而且线程过多容易把网站搞挂。

创建进程代码：

```python
from multiprocessing import Process
from concurrent.futrues import ProcessPoolExecutor

def work(name):
    print(123456, name)
    
if __name__ == '__main__':
    # 创建进程
    # target制定方法名，args指定参数，必须是元祖，只有一个参数时结尾必须有逗号
    p = Process(target=work, args=("进程1",))
    # 启动进程
    p.start()

    # 创建进程池
    with ProcessPoolExecutor(8) as pp:
        pp.submit(work, "进程1")
```

创建线程代码：

```python
from threading import Thread
from concurrent.futrues import ThreadPoolExecutor

def work(name):
    print(123456, name)
    
if __name__ == '__main__':
    # 创建线程
    # target制定方法名，args指定参数，必须是元祖，只有一个参数时结尾必须有逗号
    t = Thread(target=work, args=("线程1",))
    # 启动线程
    t.start()

    # 创建线程池
    # with表示等待线程池中的任务执行完成，执行完会关闭线程池
    with ThreadPoolExecutor(8) as t :
        t.submit(work, "线程1")
```

多线程是在同一进程中，协程针对同一个线程，可以理解为一个线程内有多个任务，当一个任务发生阻塞比如IO的时候切换到其他任务执行。协程中有一个事件循环，他负责判断当前任务是否阻塞，是否应该切换其他任务。

```python
import asyncio
import time


async def func1():
    print("func1开始")
    # time.sleep()不能在协程中使用
    await asyncio.sleep(1)
    print("func1结束")

    return 1

async def func2():
    print("func2开始")
    await asyncio.sleep(2)
    print("func2结束")

    return 2

async def func3():
    print("func3开始")
    await asyncio.sleep(1)
    print("func3结束")

    return 3

async def main():
    t1 = asyncio.create_task(func1())
    t2 = asyncio.create_task(func2())
    t3 = asyncio.create_task(func3())

    # 把所有任务放进列表
    # 然后挂起，等待所有任务执行结束
    res, pending = await asyncio.wait([t1, t2, t3])
    for re in res:
        print(re.result())
    print("任务执行结束")

if __name__ == '__main__':
    # 创建协程对象
    f = main()
    # 第一种方式直接run
    # asyncio.run(f)

    # 第二种方式使用事件轮训执行
    # 创建事件轮训对象
    event_loop = asyncio.get_event_loop()
    # 运行协程对象，直到执行结束
    event_loop.run_until_complete(f)
```

官方给出执行协程有两种写法，第一种直接run，第二种创建事件轮训对象，其中run里会自动创建事件轮训对象，其实两种写法差不多，但是run中执行完对象会关闭event_loop对象，一般的写法是一个总协程任务中封装所有的协程任务，然后run总协程任务。

协程中很多包不能用，因为他是异步的，所以爬虫的包需要更换，常用的是http模块和file相关的模块：

```python
pip install aiohttp
pip install aiofiles
```

```python
import aiohttp
import aiofiles


async def func1(url):
    print("func1开始")
    async with aiohttp.ClientSession() as session:
        # 其他参数headers、cookie什么的大部分跟requests差不多
        async with session.get(url) as resp:
            # 等待请求结果
            content = await resp.content.read()
            
            # 下载文件
            async with aiofiles.open("filename", mode="wb") as f:
                await f.write(content)
    print("func1结束")
```

**协程可以使用信号量控制被挂起任务的数量。**

### 二 数据解析

##### 1.re解析

用正则表达式解析出来我们想要的数据，用chatGPT生成正则表达式就可以。一般常用的就是`.*(贪婪匹配)`和`.*?(惰性匹配)`，一个尽可能多的匹配结果，一个尽可能少的匹配结果，如果要提取的标签中的内容，一般都是多个div标签嵌套，我们只需要最里面的div中的内容，所以惰性匹配更好用。不过一般网页内容不通过正则提取，正则一般是提取js中的url之类的，比如`url:".*?"`

python中正则使用re模块：

```python
import re

# 返回的是个列表
# r""表示字符串里的内容不转译
res = re.findall(r"\d+", "我有10000000，我捐1块")
print(res)

# finditer返回一个迭代器，里面是match对象，不是字符串，需要调用group()获取内容
# ()包起来的东西叫分组，使用?P<分组名>定义，获取分组匹配的内容在group()制定分组名
res = re.finditer(r"(?P<number>\d+)", "我有10000000，我捐1块")
for re in res:
    print(re.group("number"))

# 搜索，返回第一个匹配的结果
res = re.search(r"(?P<number>\d+)", "我有10000000，我捐1块")
print(res.group("number"))

# 先声明正则，再去搜索，可以在循环中使用
# re.S表示 .也匹配\n
obj = re.compile(r"(?P<number>\d+)", re.S)
res = obj.search("我有10000000，我捐1块")
print(res.group("number"))
```

##### 2.bs4解析

bs4是用来把文本解析成想要的格式，然后通过api获取标签内容：

```python
# bs4需要安装
# pip install bs4

from bs4 import BeautifulSoup

# 以html格式解析字符串，html.parser可以换成lxml(需要安装)
main_page = BeautifulSoup("假设这里是我们拿到的页面源代码", "html.parser")
# 根据标签定位到我们想要提取内容的标签
a_list = main_page.find("ul", attrs={"class":"ul-class"}).find_all("a")

# 遍历拿到的标签
for a in a_list:
    # 获取标签的属性值
    print(a.get("href"))
    # 获取标签间的文本
    print(a.text)
```

##### 3.xpath解析

xpath需要安装lxml模块：

```
pip install lxml
```

使用的时候xpath比bs4灵活很多：

```python
from lxml import etree

# 把字符串按html格式解析
# 后面的type是为了让pycharm知道解析出来的结果是什么对象，这样写为了调用main_page的时候有提示
page = etree.HTML("h5源码") # type: etree._Element

# 拿到根节点，xpath()返回的是列表
root = page.xpath("/html")

# 拿到html节点下的body节点下的div节点下的p节点中的文本
texts = page.xpath("/html/body/div/p/text()")
# 如果是两个斜线//，表示查找所有子节点，不管p节点中有多少子节点
# texts = page.xpath("/html/body/div/p//text()")
for text in texts:
    print(text)

# xpath里索引从1开始，如果想去最后一个可以写成p[last()]
# 查找页面所有<div><p>xxxxxxxxx</p></div>里第一个p节点中的文本
texts = page.xpath("//div/p[1]/text()")

# @表示选择属性的全值匹配
# 查找页面中的任意节点，并且节点的id是someId的文本
texts = page.xpath("//*[@id='someId']/text()")
```

### 三 请求头和代理

##### 1.User-Agent

User-Agent是标识当前发起请求的设备信息，如果代码发起的请求失败第一反应是带上这个信息。

##### 2.cookie

网页相关的使用的多，app使用的一般是jwt，里面放的是一些用户信息，服务端用来确认用户已经登录和拿到一些用户信息。

cookie有点特殊，早些年cookie里面有sessionId，由服务端返回，现在的cookie有一部分是js生成的，可能没有什么有效信息，纯校验用的。这种cookie由非对称加密生成，为了反爬可能每次请求客户端都会通过cookie传一个加密的字符串，这个加密的字符串可能是客户端生成，也可能服务端生成。

使用requests模块的session可以自动处理cookie。

##### 3.referer

有些网站要求请求带上referer字段来标记当前请求的上一个请求是什么，作用可能是防盗链，为的是溯源和防止爬虫，所以抓包的时候需要注意这个字段，如果发现这个字段需要在requests发送请求时的header中带上referer字段。

##### 4.代理

有些时候防止请求频繁被封ip可以使用ip代理，通过代理的服务器转发请求，这样爬虫的目标服务器拿到的就是代理服务器的ip，然后在请求时加上proxies：

```python
import requests

dic = {
    "http" : "http://223.96.90.216:8885",
    "https" : "https://223.96.90.216:8885"
}
resp = requests.get(url, headers=header, proxies=dic)
```

其实免费的代理服务器并不好找。还有一种隧道代理，可用性比这个ip代理高。
