---
title: 7.kafka源码-producer初始化
date: 2021-08-06 06:27:35
tags:
  - 大数据
categories: kafka
copyright: ture
---

### producer初始化

初始化producer，直接new KafkaProducer就可以，初始化的代码都在这个类里：

```java
KafkaProducer<String, String> producer = new KafkaProducer<>(Properties);
```
在这个构造方法里首先把我们配置的Properties转成**ProducerConfig**，kafka自己的配置文件，这个文件里有所有的Producer配置和默认值，还有一些属性的名字和注释在CommonClientConfigs里：

```java
this(new ProducerConfig(ProducerConfig.addSerializerToConfig(properties, keySerializer, valueSerializer)),
     keySerializer, valueSerializer, null, null);
```

然后调用另一个构造方法初始化核心组件：

```java
KafkaProducer(ProducerConfig config, Serializer<K> keySerializer, Serializer<V> valueSerializer, Metadata metadata, KafkaClient kafkaClient) {
    try {
        // 我们自己的配置
        Map<String, Object> userProvidedConfigs = config.originals();
        this.producerConfig = config;
        this.time = Time.SYSTEM;
        // 获取client.id属性，默认是空串
        String clientId = config.getString(ProducerConfig.CLIENT_ID_CONFIG);
        if (clientId.length() <= 0)
            // 空串的话会走到这里，producer-自增数字作为clientId，线程安全的
            clientId = "producer-" + PRODUCER_CLIENT_ID_SEQUENCE.getAndIncrement();
        this.clientId = clientId;
        // 核心组件，用来决定你的消息会发送到那个topic的那个分区里的
        this.partitioner = config.getConfiguredInstance(ProducerConfig.PARTITIONER_CLASS_CONFIG, Partitioner.class);
        // retry.backoff.ms，重试间隔，默认100ms
        long retryBackoffMs = config.getLong(ProducerConfig.RETRY_BACKOFF_MS_CONFIG);
        // 序列组件
        if (keySerializer == null) {
            this.keySerializer = ensureExtended(config.getConfiguredInstance(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,
                                                                             Serializer.class));
            this.keySerializer.configure(config.originals(), true);
        } else {
            config.ignore(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG);
            this.keySerializer = ensureExtended(keySerializer);
        }
        if (valueSerializer == null) {
            this.valueSerializer = ensureExtended(config.getConfiguredInstance(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,
                                                                               Serializer.class));
            this.valueSerializer.configure(config.originals(), false);
        } else {
            config.ignore(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG);
            this.valueSerializer = ensureExtended(valueSerializer);
        }

        // 拦截器组件
        userProvidedConfigs.put(ProducerConfig.CLIENT_ID_CONFIG, clientId);
        List<ProducerInterceptor<K, V>> interceptorList = (List) (new ProducerConfig(userProvidedConfigs, false)).getConfiguredInstances(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG,
                                                                                                                                         ProducerInterceptor.class);
        this.interceptors = new ProducerInterceptors<>(interceptorList);
        ClusterResourceListeners clusterResourceListeners = configureClusterResourceListeners(keySerializer, valueSerializer, interceptorList, reporters);
        // max.request.size，每个请求的最大大小，默认1M
        this.maxRequestSize = config.getInt(ProducerConfig.MAX_REQUEST_SIZE_CONFIG);
        // buffer.memory，缓冲池大小，默认32M
        this.totalMemorySize = config.getLong(ProducerConfig.BUFFER_MEMORY_CONFIG);
        this.compressionType = CompressionType.forName(config.getString(ProducerConfig.COMPRESSION_TYPE_CONFIG));

        // max.block.ms缓冲区满了的阻塞时间，默认一分钟，超过1分钟会抛出异常
        this.maxBlockTimeMs = config.getLong(ProducerConfig.MAX_BLOCK_MS_CONFIG);
        // request.timeout.ms，请求超时时间，默认30s
        this.requestTimeoutMs = config.getInt(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG);
        this.transactionManager = configureTransactionState(config, logContext, log);
        // 这个应该是重试次数
        int retries = configureRetries(config, transactionManager != null, log);
        int maxInflightRequests = configureInflightRequests(config, transactionManager != null);
        short acks = configureAcks(config, transactionManager != null, log);

        this.apiVersions = new ApiVersions();
        // 核心组件，缓冲池
        this.accumulator = new RecordAccumulator(logContext,
                                                 config.getInt(ProducerConfig.BATCH_SIZE_CONFIG),
                                                 this.totalMemorySize,
                                                 this.compressionType,
                                                 config.getLong(ProducerConfig.LINGER_MS_CONFIG),
                                                 retryBackoffMs,
                                                 metrics,
                                                 time,
                                                 apiVersions,
                                                 transactionManager);
        // broker地址
        List<InetSocketAddress> addresses = ClientUtils.parseAndValidateAddresses(config.getList(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG));
        // 核心组件，维护broker的元数据信息
        if (metadata != null) {
            this.metadata = metadata;
        } else {
            this.metadata = new Metadata(retryBackoffMs, 
                                         // metadata.max.age.ms，默认5分钟强制刷新一次
                                         config.getLong(ProducerConfig.METADATA_MAX_AGE_CONFIG),
                                         true, true, clusterResourceListeners);
            // 如果metadata是空需要拉取一下元数据
            this.metadata.update(Cluster.bootstrap(addresses), Collections.<String>emptySet(), time.milliseconds());
        }
        ChannelBuilder channelBuilder = ClientUtils.createChannelBuilder(config);
        Sensor throttleTimeSensor = Sender.throttleTimeSensor(metricsRegistry.senderMetrics);
        // 核心组件，网络通信组件，这里初始化了一个selector
        KafkaClient client = kafkaClient != null ? kafkaClient : new NetworkClient(
            new Selector(config.getLong(ProducerConfig.CONNECTIONS_MAX_IDLE_MS_CONFIG),
                         this.metrics, time, "producer", channelBuilder, logContext),
            // 这个是元数据的信息，负责元数据的增改，唤醒主线程
            this.metadata,
            clientId,
            // max.in.flight.requests.per.connection，同时发送的消息数
            // 这个最大就是5
            maxInflightRequests,
            // reconnect.backoff.ms，重新建立连接的等待时长
            config.getLong(ProducerConfig.RECONNECT_BACKOFF_MS_CONFIG),
            // reconnect.backoff.max.ms，建立连接的最大时长
            // 连接失败的时候会重试，每次间隔reconnect.backoff.ms成倍增加，直到超过max.ms
            config.getLong(ProducerConfig.RECONNECT_BACKOFF_MAX_MS_CONFIG),
            // send.buffer.bytes，# 发送缓冲区的大小
            config.getInt(ProducerConfig.SEND_BUFFER_CONFIG),
            // receive.buffer.bytes，接收缓冲区的大小
            config.getInt(ProducerConfig.RECEIVE_BUFFER_CONFIG),
            // request.timeout.ms，发送的超时时间
            this.requestTimeoutMs,
            time,
            true,
            apiVersions,
            throttleTimeSensor,
            logContext);
        // 核心组件，负责把缓冲池里的消息发送到broker上
        this.sender = new Sender(logContext,
                                 client,
                                 // 这个是元数据的信息，负责元数据的增改，唤醒主线程
                                 this.metadata,
                                 this.accumulator,
                                 maxInflightRequests == 1,
                                 // max.request.size单次请求的最大字节
                                 config.getInt(ProducerConfig.MAX_REQUEST_SIZE_CONFIG),
                                 acks,
                                 // 重试次数
                                 retries,
                                 metricsRegistry.senderMetrics,
                                 Time.SYSTEM,
                                 // 发送的超时时间
                                 this.requestTimeoutMs,
                                 // retry.backoff.ms，重试的最大时间
                                 config.getLong(ProducerConfig.RETRY_BACKOFF_MS_CONFIG),
                                 this.transactionManager,
                                 apiVersions);
        // 线程名：kafka-producer-network-thread|clientId
        String ioThreadName = NETWORK_THREAD_PREFIX + " | " + clientId;
        // 把sender放到线程里，启动
        this.ioThread = new KafkaThread(ioThreadName, this.sender, true);
        this.ioThread.start();
    } catch (Throwable t) {
        close(0, TimeUnit.MILLISECONDS, true);
        throw new KafkaException("Failed to construct kafka producer", t);
    }
}
```

看拉取元数据的方法：

```java
// 首先创建一个Cluster对象，把我们配置的bootstrap.servers参数传进去封装成node
// Cluster就是集群的元数据
this.metadata.update(Cluster.bootstrap(addresses), Collections.<String>emptySet(), time.milliseconds());
```

在Metadata类中：

```java
/**
 * 这个方法被加了一个锁
 * 每次更新版本号都会加1、更新刷新数据的时间戳
 */
public synchronized void update(Cluster newCluster, Set<String> unavailableTopics, long now) {
    Objects.requireNonNull(newCluster, "cluster should not be null");

    this.needUpdate = false;
    this.lastRefreshMs = now;
    this.lastSuccessfulRefreshMs = now;
    this.version += 1;

    if (topicExpiryEnabled) {
        // Handle expiry of topics from the metadata refresh set.
        for (Iterator<Map.Entry<String, Long>> it = topics.entrySet().iterator(); it.hasNext(); ) {
            Map.Entry<String, Long> entry = it.next();
            long expireMs = entry.getValue();
            if (expireMs == TOPIC_EXPIRY_NEEDS_UPDATE)
                entry.setValue(now + TOPIC_EXPIRY_MS);
            else if (expireMs <= now) {
                it.remove();
                log.debug("Removing unused topic {} from the metadata list, expiryMs {} now {}", entry.getKey(), expireMs, now);
            }
        }
    }

    // 监听器处理
    for (Listener listener: listeners)
        listener.onMetadataUpdate(newCluster, unavailableTopics);

    String previousClusterId = cluster.clusterResource().clusterId();

    // 这个是false，目前来看在启动的时候是不会刷新元数据的
    if (this.needMetadataForAllTopics) {
        // the listener may change the interested topics, which could cause another metadata refresh.
        // If we have already fetched all topics, however, another fetch should be unnecessary.
        this.needUpdate = false;
        this.cluster = getClusterForCurrentTopics(newCluster);
    } else {
        this.cluster = newCluster;
    }

    // The bootstrap cluster is guaranteed not to have any useful information
    if (!newCluster.isBootstrapConfigured()) {
        String newClusterId = newCluster.clusterResource().clusterId();
        if (newClusterId == null ? previousClusterId != null : !newClusterId.equals(previousClusterId))
            log.info("Cluster ID: {}", newClusterId);
        clusterResourceListeners.onUpdate(newCluster.clusterResource());
    }
	// 唤醒其他update线程
    notifyAll();
    log.debug("Updated cluster metadata version {} to {}", this.version, this.cluster);
}
```

### sender线程初始化

sender是一个runnable类，运行sender就是在线程里执行sender的run方法，线程设置了一下工作线程和线程名：

```java
public KafkaThread(final String name, boolean daemon) {
    super(name);
    configureThread(name, daemon);
}
```

然后启动，sender就不断的通过KafkaClient发送请求了。

### selector初始化

初始化KafkaClient的时候有一行初始化selector的代码：

```java
new Selector(config.getLong(ProducerConfig.CONNECTIONS_MAX_IDLE_MS_CONFIG),
             this.metrics, time, "producer", channelBuilder, logContext)
```

Selector的构造方法：

```java
public Selector(int maxReceiveSize,
                long connectionMaxIdleMs,
                Metrics metrics,
                Time time,
                String metricGrpPrefix,
                Map<String, String> metricTags,
                boolean metricsPerConnection,
                boolean recordTimePerConnection,
                ChannelBuilder channelBuilder,
                MemoryPool memoryPool,
                LogContext logContext) {
    try {
        // java nio的selector
        this.nioSelector = java.nio.channels.Selector.open();
    } catch (IOException e) {
        throw new KafkaException(e);
    }
    // 最大可以接收数据的大小
    this.maxReceiveSize = maxReceiveSize;
    this.time = time;
    // 保存的是brokerId和kafkaChannel，他是对java的Channel进行封装
    this.channels = new HashMap<>();
    // 这个应该是放无法连接的Channel
    this.explicitlyMutedChannels = new HashSet<>();
    this.outOfMemory = false;
    // 成功发送出去的请求
    this.completedSends = new ArrayList<>();
    // 已经处理完的响应
    this.completedReceives = new ArrayList<>();
    // broker收到但是还没处理的响应
    this.stagedReceives = new HashMap<>();
    this.immediatelyConnectedKeys = new HashSet<>();
    this.closingChannels = new HashMap<>();
    this.keysWithBufferedRead = new HashSet<>();
    // 建立连接的broker
    this.connected = new ArrayList<>();
    // 还没建立连接的broker
    this.disconnected = new HashMap<>();
    // 连接失败的broker
    this.failedSends = new ArrayList<>();
    this.sensors = new SelectorMetrics(metrics, metricGrpPrefix, metricTags, metricsPerConnection);
    this.channelBuilder = channelBuilder;
    this.recordTimePerConnection = recordTimePerConnection;
    this.idleExpiryManager = connectionMaxIdleMs < 0 ? null : new IdleExpiryManager(time, connectionMaxIdleMs);
    this.memoryPool = memoryPool;
    this.lowMemThreshold = (long) (0.1 * this.memoryPool.size());
    this.log = logContext.logger(Selector.class);
}
```

再看KafkaChannel的核心组件：

每个broker id对应一个网络连接，一个网络连接对应一个KafkaChannel，底层对应的是SocketChannel，SocketChannel对应的是最最底层的网络通信层面的一个Socket，套接字通信。

Send组件：要交给这个底层的Channel发送出去的请求，可能会不断的变换的，因为发送完一个请求需要发送下一个请求

NetworkReceive组件：这个Channel最近一次读出来的响应，先暂存在这里，也是会不断的变换的，因为会不断的读取新的响应数据

TransportLayer：封装了底层的Java NIO的SocketChannel

