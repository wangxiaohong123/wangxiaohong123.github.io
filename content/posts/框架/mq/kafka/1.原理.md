---
title: 1.kafka原理
tags:
  - 大数据
categories: kafka
copyright: ture
---

### 数据写入，如何保证单机每秒几十万

微批处理技术：spark streaming这种流式计算的时候会使用这种思想，收集数据后统一处理，提高吞吐量，比如9ms可以收到1000条数据，然后花1ms写入磁盘，这样只花10ms就可以处理完1000条数据，要比收到一条就花1ms写一条快100倍（1000条 * 1ms = 1s）。

使用微批处理提高吞吐还是会有一定的延迟，但是kafka可以保证高吞吐，低延迟，kafka通过os cache + 磁盘顺序写实现高吞吐低延迟，写os cache和磁盘顺序写的性能基本上和写入内存差不多，假设0.01ms写一条数据，那么1s就可以处理10万条数据。

### 零拷贝实现高性能读取

正常从把消息发送给客户端需要先从os cache中读取数据，如果os cache中没有就去从磁盘里读取，这步是需要从用户态切换到内核态的，然后把数据拷贝到应用进程的内存，在从内核态切换回用户态，把数据拷贝到socket cache中，在发送给网卡把数据发射出去，多余的是两次用户态和内核态之间的切换，和两次拷贝：

![](https://tva1.sinaimg.cn/large/008i3skNly1gt8js6clx3j30ki0g7js4.jpg)

kafka使用linux的sendfile，直接把数据从os cache发送给网卡，非常高效：

![](https://tva1.sinaimg.cn/large/008i3skNly1gt8juq2re2j30jm0fowey.jpg)

### 消息存储

kefka使用NIO的ByteBuffer一二进制的方式保存消息，比java对象保存方式节约40%的空间，每条消息时封装在log entry中，可以理解成一个消息条目，首先会有一个消息集合（record batch）的概念，里面包含多个日志，每个消息集合会记录自己的offect和总消息大小、创建时间戳等信息，具体的消息格式：

*   消息总长度
*   时间戳增量：相对于所属的record batch的增量
*   offset增量：相对于所属的record batch的增量
*   key长度
*   key：用来做消息负载均衡使用
*   value长度
*   value
*   header个数
*   header：自定义的消息元数据，key-value格式

时间戳和offset都是采用增量的方式存储，可以减少磁盘空间的占用。每个topic会把消息均匀的存储到每个partition上，每个partition也会有一个冗余副本。

每个分区会在自盘上有一个对应的目录，格式是**topic-分区号**，例如：order-topic-0。每个分区的日志会被拆成多个，并且每个日志文件有自己的索引文件：

00000000000005367851.index

00000000000005367851.log

00000000000005367851.timeindex

.log是存放数据的，文件名是起始的offset，.index是位移索引，另一个是时间戳索引。每个文件的大小可以通过log.segment.bytes（默认1G）配置，达到参数大小会重新创建一个文件，这个过程叫log rolling，正在被写入的文件叫active log segment，其他的日志文件叫log segment file。当日志写入达到一定大小的时候会在索引文件写入一条索引，通过log.index.interval.bytes（默认4k）配置，索引是按照位移和时间戳升序，这样可以使用2分查找快速定位数据。

数据默认保存7天，通过log.retention.hours设置。

### ISR（in-sync replica）机制

靠partition的多副本机制可以保证高可用，但是不能保证消息不丢失，kafka会维护一张ISR列表，只有和leader的消息是同步的flower才会出现在这个列表中，如果leader宕机，会从ISR列表中的flower选举一个leader，ISR要求最少有一个flower和leader的消息同步，并且消息要同时在leader和flower都写成功才算成功。

在0.9版本之前，通过replica.lag.max.messages（默认4000）设置那些flower会被踢出去，但是这样会有一个问题，就是在高并发的时候，如果1s内并发上完，所有flower都不满足这个条件就会被踢出去，等到flower同步之后又来一万，所有flower就又会被踢出去，一直这样反反复复。在0.9之后使用replica.lag.time.max.ms（默认10秒）替代之前的配置，表示超过多少秒flower还没完成同步就把他踢出ISR列表。

### LEO（log end offset）和HW（high watermark）

不管是leader还是flower都是副本，每个副本都有一个leo和hw，leo表示当前消息的结束偏移量，也就是下一新条消息的偏移量，hw表示能被消费到的消息的偏移量，每个leader会存储所有flower的leo值，当flower来fetch消息的时候会带上自己的leo，leader更新维护的flower的leo，返回自己的hw，flower拿到返回的leo和自己的取小，更新本地的hw。

**leader切换时的数据丢失1：高水位（非常极端的场景）**：

比如说broker往leader里写入一条数据，flower进行同步成功，但是第一次拉取数据leader返回的HW是0，所以flower的HW还是0:

![](https://tva1.sinaimg.cn/large/008i3skNly1gt9gqs6t0oj30wu09a3yy.jpg)

等到下一次去fetch数据的时候，leader判断flower的LEO和自己的一样就会更新HW，然后返回，当flower收到响应还没更新的时候宕机重启了，此时会根据HW调整LEO，LEO变成了0，刚要从leader哪里fetch数据，结果leader宕机了，此时flower被选举成新的leader，然后旧leader重启，fetch数据后发现leader没有数据，就会把自己的消息也删掉。

![](https://tva1.sinaimg.cn/large/008i3skNly1gt9h1f73epj311c0joabe.jpg)

**leader切换导致数据丢失2：数量巧合**

假设flower比leader落后10条消息，此时leader宕机，flower被选举成新leader，在收到10条消息后，leader重启变成flower，fetch数据的时候发现HW相同，就会以为数据是同步的。。。这样不止会丢掉10条消息，leader的新10条消息还没有同步，这样消息时错乱的。

**0.11.x的leader epoch机制**

0.11.x新增了一个epoch，里面是自己当leader时的版本号和消息的offset，比如刚启动的时候是[epoch:0,offset:0]，如果flower重启看到自己没有epoch是不会截断数据的，直接去更新，只有当发现自己的offset比leader大的时候才会删除数据，这样可以解决高水位的消息丢失问题，对于第二种情况，他会leader重启之后变成flower，发现自己的offset是1，但是新的leader是从1开始写的，所以判定自己多了一条数据，就会截断这条数据，并不能解决数据丢失，但是最起码数据不是乱的。

### broker

生产者/消费者和broker使用基于nio的长连接通信，broker和broker之间使用自定义的tcp协议通信。每个broker上都有一个acceptor线程和多个processor线程（默认3个），processor通过nio的selector轮询多个连接，收到请求后发送到一个全局队列，队列大小是500，可以通过queued.max.requests设置，队列由KafkaRequestHandler线程池消费，处理完结果会放到processor自己的响应队列中，通过processor返回响应。

### controller

controller负责broker的宕机感知、新节点、选举、负载均衡迁移等等，broker启动的时候回向zk发送注册临时节点的请求，zk可以保证只有一个broker能成功，谁先注册成功谁就是controller，然后controller监听zk上的broker信息变更，他会把zk上的信息拉取到本地，取出第一个作为leader，然后分配每个partion在那台机器上，然后在把所有的flower写到ISR列表里，最后把所有信息推送给所有broker。

删除topic的时候，controller会通知所有所有partition所在的机器，把副本状态设置成OfflineReplica，然后controller把所有副本状态改成ReplicaDeletionStarted，接着在通知所有broker删除磁盘文件，删除成功后副本的状态为ReplicaDeletionSuccessful，所有副本都删完了状态就变成了NonExistentReplica。

总的来说他就是负责集群的整体控制，主要是通过zk的临时节点。