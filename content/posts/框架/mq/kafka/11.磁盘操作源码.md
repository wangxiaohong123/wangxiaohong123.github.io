---
title: 11.kafka源码-磁盘操作
date: 2021-09-21 06:27:35
tags:
  - 大数据
categories: kafka
copyright: true
---

之前启动的时候看到了跟磁盘有关的两个组件，一个是LogManager，还有一个是ReplicaManager，而且在初始化ReplicaManager的时候LogManager是被当做参数传进去的，先看一下ReplicaManager都实例化了那些东西：

```scala
// 配置的brokerId
private val localBrokerId = config.brokerId
// 所有的partition信息
private val allPartitions = new Pool[(String, Int), Partition](valueFactory = Some { case (t, p) =>
    new Partition(t, p, time, this)
})
// 这个就是一把锁
private val replicaStateChangeLock = new Object
val replicaFetcherManager = new ReplicaFetcherManager(config, this, metrics, jTime, threadNamePrefix)
// 高水位的检查，只有当所有的副本都保存了消息，高水位才会指向最新的消息
// 消费者只能消费到高水位之前的消息
private val highWatermarkCheckPointThreadStarted = new AtomicBoolean(false)
val highWatermarkCheckpoints = config.logDirs.map(dir => (new File(dir).getAbsolutePath, new OffsetCheckpoint(new File(dir, ReplicaManager.HighWatermarkFilename)))).toMap
private var hwThreadInitialized = false
val stateChangeLogger = KafkaController.stateChangeLogger
// isr列表相关
// 当follower没有落后太多的时候才会出现在isr列表
private val isrChangeSet: mutable.Set[TopicAndPartition] = new mutable.HashSet[TopicAndPartition]()
private val lastIsrChangeMs = new AtomicLong(System.currentTimeMillis())
private val lastIsrPropagationMs = new AtomicLong(System.currentTimeMillis())
// 延迟调度机制，实践论的算法
val delayedProducePurgatory = DelayedOperationPurgatory[DelayedProduce](
    purgatoryName = "Produce", config.brokerId, config.producerPurgatoryPurgeIntervalRequests)
val delayedFetchPurgatory = DelayedOperationPurgatory[DelayedFetch](
    purgatoryName = "Fetch", config.brokerId, config.fetchPurgatoryPurgeIntervalRequests)

// 本地存储的leader数
val leaderCount = newGauge("LeaderCount", new Gauge[Int] {
        def value = {
            getLeaderPartitions().size
        }
    })
// 本地的分区数
val partitionCount = newGauge("PartitionCount", new Gauge[Int] {
        def value = allPartitions.size
    })
// 副本数量不充足的partition
val underReplicatedPartitions = newGauge("UnderReplicatedPartitions", new Gauge[Int] {
        def value = underReplicatedPartitionCount()
    })
// isr列表扩张和伸缩的速率
val isrExpandRate = newMeter("IsrExpandsPerSec",  "expands", TimeUnit.SECONDS)
val isrShrinkRate = newMeter("IsrShrinksPerSec",  "shrinks", TimeUnit.SECONDS)
// 下面还有一些函数和线程之类的
```

初始化完ReplicaManager之后执行startup方法，启动两个关于isr的定时调度线程：

```scala
def startup() {
    // 定时(默认10s)去掉同步太慢的follower
    scheduler.schedule("isr-expiration", maybeShrinkIsr, period = config.replicaLagTimeMaxMs, unit = TimeUnit.MILLISECONDS)
    scheduler.schedule("isr-change-propagation", maybePropagateIsrChanges, period = 2500L, unit = TimeUnit.MILLISECONDS)
}
```

可以看到ReplicaManager都是管理副本、hw、isr的东西，真正操作磁盘都在LogManager里，为什么叫Log？

在Kafka里，每个partition的leader或者follower都是一个replica，每个replica对应了一个Log，每个Log在磁盘上都是一个目录，目录里拆分了多个Log Segment日志段，每个Segment都有自己的.log和.index文件，.log存放数据，.index存放稀疏索引，先看看初始化LogManager里面有哪些东西：

```scala
private def createLogManager(zkClient: ZkClient, brokerState: BrokerState): LogManager = {
    // 拿到配置
    val defaultProps = KafkaServer.copyKafkaConfigToLog(config)
    val defaultLogConfig = LogConfig(defaultProps)
	// 通过zk拿到所有topic信息
    val configs = AdminUtils.fetchAllTopicConfigs(zkUtils).map { case (topic, configs) =>
        topic -> LogConfig.fromProps(defaultProps, configs)
    }
    // 清理磁盘的，比如存在时间太长的消息
    val cleanerConfig = CleanerConfig(numThreads = config.logCleanerThreads,
                                      dedupeBufferSize = config.logCleanerDedupeBufferSize,
                                      dedupeBufferLoadFactor = config.logCleanerDedupeBufferLoadFactor,
                                      ioBufferSize = config.logCleanerIoBufferSize,
                                      maxMessageSize = config.messageMaxBytes,
                                      maxIoBytesPerSecond = config.logCleanerIoMaxBytesPerSecond,
                                      backOffMs = config.logCleanerBackoffMs,
                                      enableCleaner = config.logCleanerEnable)
    // 实例化LogManager
    new LogManager(logDirs = config.logDirs.map(new File(_)).toArray,
                   topicConfigs = configs,
                   defaultConfig = defaultLogConfig,
                   cleanerConfig = cleanerConfig,
                   ioThreads = config.numRecoveryThreadsPerDataDir,
                   flushCheckMs = config.logFlushSchedulerIntervalMs,
                   flushCheckpointMs = config.logFlushOffsetCheckpointIntervalMs,
                   retentionCheckMs = config.logCleanupIntervalMs,
                   scheduler = kafkaScheduler,
                   brokerState = brokerState,
                   time = time)
}
```

实例化LogManager没啥东西，都是一些文件锁之类的，重要的代码可能就两行：

```scala
// 创建数据目录
createAndValidateLogDirs(logDirs)
// 看看目录下都有哪些topic的消息等等
loadLogs()
```

loadLogs源码：

```scala
private def loadLogs(): Unit = {
    // 定义一个线程池，每个目录都会有一个结果数组
    val threadPools = mutable.ArrayBuffer.empty[ExecutorService]
    val jobs = mutable.Map.empty[File, Seq[Futrue[_]]]
	// 这个是我们配置的log.dir，他是一个数组，遍历这个配置
    for (dir <- this.logDirs) {
        // 每个dir有一个线程池，ioThreads是之前那个io.threads
        val pool = Executors.newFixedThreadPool(ioThreads)
        threadPools.append(pool)
        val cleanShutdownFile = new File(dir, Log.CleanShutdownFile)
        ……
        val jobsForDir = for {
            // dirContent是目录下的所有文件和目录
            dirContent <- Option(dir.listFiles).toList
            // logDir就是目录下的文件夹，文件夹是由partition名+分区号组成的
            logDir <- dirContent if logDir.isDirectory
        } yield {
            CoreUtils.runnable {
				// 根据logDir就能拿到partition的名
                val topicPartition = Log.parseTopicPartitionName(logDir)
                val config = topicConfigs.getOrElse(topicPartition.topic, defaultConfig)
				// 把这个目录封装成新的Log
                val current = new Log(logDir, config, logRecoveryPoint, scheduler, time)
                // partition和对应的Log插入map中，如果map已经有这个元素了，就会返回旧的
                val previous = this.logs.put(topicPartition, current)
				// partition已经存在Log信息就抛出异常
                if (previous != null) {
                    throw new IllegalArgumentException("Duplicate log directories found: %s, %s!".format(
                            current.dir.getAbsolutePath, previous.dir.getAbsolutePath))
                }
            }
        }
		// 这个应该是启动定时清理磁盘的调度任务
        jobs(cleanShutdownFile) = jobsForDir.map(pool.submit).toSeq
    }
```

启动结束之后就可以在收到消息的时候追加到磁盘了，在apis中已经看到了在ReplicaManager的appendMessages()方法中，这个方法里有个参数是messagesPerPartition，他是一个map，对应着每个partition的batch中的消息

```scala
// 前面省略了一些ack的判断
// 这里就是追加磁盘的操作了
val localProduceResults = appendToLocalLog(internalTopicsAllowed, messagesPerPartition, requiredAcks)

val produceStatus = localProduceResults.map { case (topicPartition, result) =>
    topicPartition ->
    ProducePartitionStatus(
        result.info.lastOffset + 1, // required offset
        new PartitionResponse(result.errorCode, result.info.firstOffset, result.info.timestamp)) // response status
}
// ack是-1，需要等待所有副本都fetch成功
if (delayedRequestRequired(requiredAcks, messagesPerPartition, localProduceResults)) {
    // 封装一个DelayedProduce
    val produceMetadata = ProduceMetadata(requiredAcks, produceStatus)
    val delayedProduce = new DelayedProduce(timeout, produceMetadata, this, responseCallback)
    val producerRequestKeys = messagesPerPartition.keys.map(new TopicPartitionOperationKey(_)).toSeq
    // 放到时间轮里，等待被唤醒或者超时
    delayedProducePurgatory.tryCompleteElseWatch(delayedProduce, producerRequestKeys)
} else {
    // 如果ack不是-1，leader写入次攀枝花就可以直接调用回调返回结果了
    val produceResponseStatus = produceStatus.mapValues(status => status.responseStatus)
    responseCallback(produceResponseStatus)
}
```

这么看写磁盘的代码可能就在appendToLocalLog()方法中，appendToLocalLog去掉了一些catch和无关紧要的代码后长成这样：

```java
private def appendToLocalLog(internalTopicsAllowed: Boolean,
                               messagesPerPartition: Map[TopicPartition, MessageSet],
                               requiredAcks: Short): Map[TopicPartition, LogAppendResult] = {
    try {
        // 前面是判断topic，如果是kafka自己的topic的处理逻辑
        // 拿到partition的信息，brokerId、leader、zk、isr列表之类的
        val partitionOpt = getPartition(topicPartition.topic, topicPartition.partition)
        val info = partitionOpt match {
            // 有partition就追加数据
            case Some(partition) =>
                partition.appendMessagesToLeader(messages.asInstanceOf[ByteBufferMessageSet], requiredAcks)
            // 没有就抛出异常
            case None => throw new UnknownTopicOrPartitionException("Partition %s doesn't exist on %d"
                                                                        .format(topicPartition,
                                                                                localBrokerId))
        }
        val numAppendedMessages = if (info.firstOffset == -1L || info.lastOffset == -1L)
            	0
            else
                info.lastOffset - info.firstOffset + 1
            // 更新一些状态
     }
}
```

所以写磁盘的代码应该就在partition.appendMessagesToLeader()中，进到Partition的appendMessagesToLeader方法中：

```scala
def appendMessagesToLeader(messages: ByteBufferMessageSet, requiredAcks: Int = 0) = {
    // 添加读锁
    val (info, leaderHWIncremented) = inReadLock(leaderIsrUpdateLock) {
        // 这个是拿到本地的replica是不是leader
        val leaderReplicaOpt = leaderReplicaIfLocal()
        leaderReplicaOpt match {
            // 如果有leader就会走这个分支
            case Some(leaderReplica) =>
            	// 获取Log对象
                val log = leaderReplica.log.get
                // 这个是我们配置的isr列表中至少有几个replica才能写消息
                // 比如配置的是2，那么至少要有一个leader和一个follower才可以
                val minIsr = log.config.minInSyncReplicas
                // 这个是实际的isr列表长度
                val inSyncSize = inSyncReplicas.size
                // 如果acks是-1并且isr列表长度不够就抛出异常
                if (inSyncSize < minIsr && requiredAcks == -1) {
                    throw new NotEnoughReplicasException("Number of insync replicas for partition [%s,%d] is [%d], below required minimum [%d]".format(topic, partitionId, inSyncSize, minIsr))
                }

                val info = log.append(messages, assignOffsets = true)
                // 这个是唤醒来同步数据的follower，因为没有最新数据被放入时间轮中，当收到数据后需要唤醒，让他尝试拉取
                replicaManager.tryCompleteDelayedFetch(new TopicPartitionOperationKey(this.topic, this.partitionId))
                // 更新了一下高水位
                (info, maybeIncrementLeaderHW(leaderReplica))
			// 如果本地没有leader就抛出异常
            case None =>
            	throw new NotLeaderForPartitionException("Leader not local for partition [%s,%d] on broker %d".format(topic, partitionId, localBrokerId))
        }
    }
    // some delayed operations may be unblocked after HW changed
    if (leaderHWIncremented)
    	tryCompleteDelayedRequests()
}
```

写磁盘的入口又跑到了Log类的append方法，这个方法里细节非常多，先大概看一下

```scala
// 检查消息
var validMessages = trimInvalidBytes(messages, appendInfo)
try {
    // 每个分区的副本只能让一个线程来写入
    lock synchronized {
        if (assignOffsets) {
            // assign offsets to the message set
            val offset = new LongRef(nextOffsetMetadata.messageOffset)
            appendInfo.firstOffset = offset.value
            val now = time.milliseconds
            // 校验代码被我省略了……
            validMessages = validatedMessages
            appendInfo.lastOffset = offset.value - 1
            if (config.messageTimestampType == TimestampType.LOG_APPEND_TIME)
            appendInfo.timestamp = now
            if (messageSizesMaybeChanged) {
                for (messageAndOffset <- validMessages.shallowIterator) {
                    if (MessageSet.entrySize(messageAndOffset.message) > config.maxMessageSize) {
                        BrokerTopicStats.getBrokerTopicStats(topicAndPartition.topic).bytesRejectedRate.mark(messages.sizeInBytes)
                        BrokerTopicStats.getBrokerAllTopicsStats.bytesRejectedRate.mark(messages.sizeInBytes)
                        throw new RecordTooLargeException("Message size is %d bytes which exceeds the maximum configured message size of %d.".format(MessageSet.entrySize(messageAndOffset.message), config.maxMessageSize))
                    }
                }
            }
        } else {
            // we are taking the offsets we are given
            if (!appendInfo.offsetsMonotonic || appendInfo.firstOffset < nextOffsetMetadata.messageOffset)
            throw new IllegalArgumentException("Out of order offsets found in " + messages)
        }
        // 如果消息的大小超过了配置的segment大小（默认1G）就抛出异常，因为写不下
        if (validMessages.sizeInBytes > config.segmentSize) {
            throw new RecordBatchTooLargeException("Message set size is %d bytes which exceeds the maximum configured segment size of %d.".format(validMessages.sizeInBytes, config.segmentSize))
        }
        // 判断segment是否满了，满了就生成一个新的
        // 当消息的大小+当前segment的大小 > 配置的segment大小就是满了
        val segment = maybeRoll(validMessages.sizeInBytes)
        // 基于segment写到对应的磁盘文件里
        segment.append(appendInfo.firstOffset, validMessages)
        // 更新LEO
        updateLogEndOffset(appendInfo.lastOffset + 1)
        if (unflushedMessages >= config.flushInterval)
        	flush()
    }
}
```

终于找到了写磁盘的入口：LogSegment的append()方法，但是要先看一下生成新的segment方法，当maybeRoll()判断为true的时候会调用roll()方法：

```scala
def roll(): LogSegment = {
    val start = time.nanoseconds
    // 加了个锁
    lock synchronized {
        val newOffset = logEndOffset
        // 生成新的文件对象，文件名就是newOffset+后缀
        // 因为logEndOffset表示下一条消息的偏移量，所以logEndOffset就是文件名
        val logFile = logFilename(dir, newOffset)
        val indexFile = indexFilename(dir, newOffset)
        // 生成对象之后需要判断这个文件是不是存在了，存在就删掉
        for(file <- List(logFile, indexFile); if file.exists) {
            warn("Newly rolled segment file " + file.getName + " already exists; deleting it first")
            file.delete()
        }
        segments.lastEntry() match {
            case null =>
            case entry => {
                entry.getValue.index.trimToValidSize()
                entry.getValue.log.trim()
            }
        }
        // 定义一个新的LogSegment对象
        val segment = new LogSegment(dir,
                                     startOffset = newOffset,
                                     indexIntervalBytes = config.indexInterval,
                                     maxIndexSize = config.maxIndexSize,
                                     rollJitterMs = config.randomSegmentJitter,
                                     time = time,
                                     fileAlreadyExists = false,
                                     initFileSize = initFileSize,
                                     preallocate = config.preallocate)
        // 插入segments中，这是一个map，key就是起始偏移量
        val prev = addSegment(segment)
        // 更新LEO
        updateLogEndOffset(nextOffsetMetadata.messageOffset)
        // 刷新旧的segment
        scheduler.schedule("flush-log", () => flush(newOffset), delay = 0L)
    }
}
```

然后继续看刷新磁盘的方法，刷新磁盘不用想就是两步，写入index和写入log，但是index是一个稀疏索引，首先文件名和log文件名是相同的，写入1条消息的时候LEO就会加1，这样拿到偏移量就可以定位到下一条消息在那个文件中，然后index文件存储的是第几条消息到第几条消息在log文件中的偏移量，并不是每条消息都记录，减少写磁盘的压力，读消息的时候根据偏移量的范围去查找消息，可能从index中找到的偏移量往后查几十条就可以找到目标的消息，index文件的内容看起来可能是这样（忽略等号和前面的内容）：

offset = 23332 物理位置=220

```scala
// index中的稀疏索引的间隔是indexIntervalBytes，默认4096个字节
// 如果索引中的消息大小超过了indexIntervalBytes就写一条索引数据
if(bytesSinceLastIndexEntry > indexIntervalBytes) {
    index.append(offset, log.sizeInBytes())
    this.bytesSinceLastIndexEntry = 0
}
// 写入消息
log.append(messages)
// 稀疏索引中存储的消息大小
this.bytesSinceLastIndexEntry += messages.sizeInBytes
```

进到OffsetIndex的append方法里：

```scala
def append(offset: Long, position: Int) {
    // 加了个锁
    inLock(lock) {
        if (_entries == 0 || offset > _lastOffset) {
            // 这里有个mmap
            // 他是一个MappedByteBuffer对象，他是一个基于os cache实现的文件映射到内存的技术
            // 把底层的index文件映射到了os cache中
            // 针对这个东西的读和写都是基于os cache内存来执行的
            
            // 先插入这条数据的offset，为什么要减掉baseOffset
            // 因为这样减掉之后数字就会很小，节约磁盘空间，baseOffset就是文件名的那个offset
            // 这样减掉之后就相当于每个index中记录的offset是从0开始，真是的offset需要加上文件名的offset
            // 还有一个原因是减掉之后肯定可以转成int，读的时候就很方便，4个字节的逻辑offset，4个字节的物理offset
            mmap.putInt((offset - baseOffset).toInt)
            // 在插入物理位置
            mmap.putInt(position)
            _entries += 1
            _lastOffset = offset
        }
    }
}
```

MappedByteBuffer的实例化：

```scala
private[this] var mmap: MappedByteBuffer = {
    // 创建一个文件
    val newlyCreated = _file.createNewFile()
    // 一个读写流
    val raf = new RandomAccessFile(_file, "rw")
    try {
        // 新创建的文件处理一下
        if (newlyCreated) {
            if (maxIndexSize < 8)
            	throw new IllegalArgumentException("Invalid max index size: " + maxIndexSize)
            raf.setLength(roundToExactMultiple(maxIndexSize, 8))
        }
		// 文件大小
        val len = raf.length()
        // 映射到了mmap
        val idx = raf.getChannel.map(FileChannel.MapMode.READ_WRITE, 0, len)

        // 设置偏移量
        if (newlyCreated)
        	idx.position(0)
        else
        	idx.position(roundToExactMultiple(idx.limit, 8))
        // 返回MappedByteBuffer
        idx
    } finally {
        // 关闭打开的io流
        CoreUtils.swallow(raf.close())
    }
}
```

继续看写磁盘的方法FileMessageSet的append(),这个方法里就是调用了ByteBufferMessageSet的writeFullyTo(channel)方法，这个channel是构建FileMessageSet的时候创建的，根据mutable参数来判断，如果是true就创建log文件的RandomAccessFile的channel，如果是false就创建log文件的FileInputStream的channel，没找到什么时候会传false，writeFullyTo的代码很少：

```scala
def writeFullyTo(channel: GatheringByteChannel): Int = {
    // 设置position的标记，读完数据复位用到
    buffer.mark()
    var written = 0
    // 一直写
    while (written < sizeInBytes)
    	written += channel.write(buffer)
    buffer.reset()
    // 返回读出来的buffer的字节数
    written
}
```

看起来是通过channel刷到了磁盘，其实不是，通过channel的write也是写到了os cache中。

现在来看写入磁盘的都是二进制的数据，看不出来什么格式，往回找从头看一下，找到了一个Message，我也不知道怎么找到的，看一看message的格式：

```java
/**
 * 1. 4 byte CRC32 of the message 校验和，判断数据是否破损
 * 2. 1 byte "magic" identifier to allow format changes, value is 0 or 1 是否允许格式变更
 * 3. 1 byte "attributes" identifier to allow annotations on the message independent of the version
 *    bit 0 ~ 2 : Compression codec.
 *      0 : no compression
 *      1 : gzip
 *      2 : snappy
 *      3 : lz4
 *    bit 3 : Timestamp type
 *      0 : create time
 *      1 : log append time
 *    bit 4 ~ 7 : reserved
 * 4. (Optional) 8 byte timestamp only if "magic" identifier is greater than 0 时间戳
 * 5. 4 byte key length, containing length K key的长度
 * 6. K byte key key数据
 * 7. 4 byte payload length, containing length V 消息体长度
 * 8. V byte payload 消息体数据
 */
```

回到Log的updateLogEndOffset方法，写完消息之后会有一个更新LEO的操作，这个操作非常简单就是生成一个新的对象赋给全局的变量

```scala
nextOffsetMetadata = new LogOffsetMetadata(messageOffset, activeSegment.baseOffset, activeSegment.size.toInt)
```

所以LogOffsetMetadata就是存储LEO的类，nextOffsetMetadata就是LEO。

下面还有一行代码很关键：

```scala
// unflushedMessages是一个方法this.logEndOffset - this.recoveryPoint，根据LEO计算出来有多少数据没刷到磁盘
// 如果没刷磁盘的数据大于配置(默认10000条)的，就刷新磁盘
if (unflushedMessages >= config.flushInterval)
	flush()
```

flush的代码也很简单，就是现获出来没刷盘的消息所在的segment，然后遍历调用FileMessageSet和OffsetIndex的flush。

刷新完之后就会把之前操作的信息封装到LogAppendInfo中返回，然后一层一层返回，回到ReplicaManager中，会对LogAppendInfo处理一下，如果发生异常就会把异常的code放进去，最后返回的是分区:写入结果的map，最后把结果交给apis中定义的sendResponseCallback处理。