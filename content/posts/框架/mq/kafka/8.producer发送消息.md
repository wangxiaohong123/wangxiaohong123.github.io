---
title: 8.kafka源码-producer发送消息
date: 2021-08-06 06:27:35
tags:
  - 大数据
categories: kafka
copyright: true
---

初始化完成后就可以调用producer的send方法了，先看异步发送，传进一个ProducerRecord类型的消息体和一个回调函数，最终调用到KafkaProducer的doSend方法：

```java
// 调用doSend之前会使用拦截器在处理一下record
private Futrue<RecordMetadata> doSend(ProducerRecord<K, V> record, Callback callback) {
    TopicPartition tp = null;
    try {
        // 这里要获取topic了，同步阻塞
        ClusterAndWaitTime clusterAndWaitTime = waitOnMetadata(record.topic(), record.partition(), maxBlockTimeMs);
        long remainingWaitMs = Math.max(0, maxBlockTimeMs - clusterAndWaitTime.waitedOnMetadataMs);
        Cluster cluster = clusterAndWaitTime.cluster;
        // 把key和value都进行序列化
        byte[] serializedKey;
        try {
            serializedKey = keySerializer.serialize(record.topic(), record.headers(), record.key());
        } catch (ClassCastException cce) {
            throw new SerializationException("Can't convert key of class " + record.key().getClass().getName() +
                                             " to class " + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() +
                                             " specified in key.serializer", cce);
        }
        // value就是消息的body
        byte[] serializedValue;
        try {
            serializedValue = valueSerializer.serialize(record.topic(), record.headers(), record.value());
        } catch (ClassCastException cce) {
            throw new SerializationException("Can't convert value of class " + record.value().getClass().getName() +
                                             " to class " + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() +
                                             " specified in value.serializer", cce);
        }
        // 根据topic信息获取对应的分区
        int partition = partition(record, serializedKey, serializedValue, cluster);
        tp = new TopicPartition(record.topic(), partition);

        setReadOnly(record.headers());
        Header[] headers = record.headers().toArray();
		// 统计消息大小
        int serializedSize = AbstractRecords.estimateSizeInBytesUpperBound(apiVersions.maxUsableProduceMagic(),
                                                                           compressionType, serializedKey, serializedValue, headers);
        // 检查消息是否超过了设置的大小上限和是否超过了缓冲区的大小上限
        ensureValidRecordSize(serializedSize);
        long timestamp = record.timestamp() == null ? time.milliseconds() : record.timestamp();
        log.trace("Sending record {} with callback {} to topic {} partition {}", record, callback, record.topic(), partition);
        // producer callback will make sure to call both 'callback' and interceptor callback
        // 设置自定义回调和拦截器的回调
        Callback interceptCallback = new InterceptorCallback<>(callback, this.interceptors, tp);

        if (transactionManager != null && transactionManager.isTransactional())
            transactionManager.maybeAddPartitionToTransaction(tp);

        // 把消息放到缓冲里
        RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey,
                                                                         serializedValue, headers, interceptCallback, remainingWaitMs);
        // 这里就是判断如果一个batch满了或者创建了一个新的batch，就要唤醒sender线程，发送消息
        if (result.batchIsFull || result.newBatchCreated) {
            log.trace("Waking up the sender since topic {} partition {} is either full or getting a new batch", record.topic(), partition);
            this.sender.wakeup();
        }
        // 返回futrue对象
        return result.futrue;
        // 下面是一大堆异常处理，基本上每一个步骤都会自定义一个自己的异常，在这层捕获进行处理
        // 一般中间件的异常捕获处理之后也会抛出去
        // 如果是业务代码要在业务的最顶层进行捕获处理，根据异常返回给前端对应的提示
    } catch (ApiException e) {
        log.debug("Exception occurred during message send:", e);
        if (callback != null)
            callback.onCompletion(null, e);
        this.errors.record();
        this.interceptors.onSendError(record, tp, e);
        return new FutrueFailure(e);
    } catch (InterruptedException e) {
        this.errors.record();
        this.interceptors.onSendError(record, tp, e);
        throw new InterruptException(e);
    } catch (BufferExhaustedException e) {
        this.errors.record();
        this.metrics.sensor("buffer-exhausted-records").record();
        this.interceptors.onSendError(record, tp, e);
        throw e;
    } catch (KafkaException e) {
        this.errors.record();
        this.interceptors.onSendError(record, tp, e);
        throw e;
    } catch (Exception e) {
        // we notify interceptor about all exceptions, since onSend is called before anything else in this method
        this.interceptors.onSendError(record, tp, e);
        throw e;
    }
}
```

##### 同步阻塞获取topic

KafkaProducer的waitOnMetadata方法有三个参数，topic、partition、maxWaitMs，topic和partition是在发消息时候设置的，maxWaitMs是初始化producer的时候配置的max.block.ms，最开始是简单配置一下然后判断缓存：

```java
// 更新要拉取的topic信息
// 更新needUpdate标志位
metadata.add(topic);
// 获取现在缓存的cluster
Cluster cluster = metadata.fetch();
// 这里就把topic的partition拿出来，判断数量，没有或者分区数比传进来的小就返回当前的cluster
Integer partitionsCount = cluster.partitionCountForTopic(topic);
if (partitionsCount != null && (partition == null || partition < partitionsCount))
    return new ClusterAndWaitTime(cluster, 0);
```

然后有设置了一遍需要拉取的topic信息，在尝试拉取topic元数据：

```java
do {
    // 这里需要重新设置更新的topic信息，我猜因为拉取信息是异步的，有可能走到这，然后version被别的线程拉取之后发生改变了，或者说needUpdate这个标志位被别的线程改成了false，这样就不会拉取元数据了
    metadata.add(topic);
    int version = metadata.requestUpdate();
    // 唤醒sender，sender负责拉取元数据
    sender.wakeup();
    try {
        // 同步阻塞等待元数据更新
        metadata.awaitUpdate(version, remainingWaitMs);
    } catch (TimeoutException ex) {
        // Rethrow with original maxWaitMs to prevent logging exception with remainingWaitMs
        throw new TimeoutException("Failed to update metadata after " + maxWaitMs + " ms.");
    }
    // 判断等待是否超时
    cluster = metadata.fetch();
    elapsed = time.milliseconds() - begin;
    if (elapsed >= maxWaitMs)
        throw new TimeoutException("Failed to update metadata after " + maxWaitMs + " ms.");
    if (cluster.unauthorizedTopics().contains(topic))
        throw new TopicAuthorizationException(topic);
    remainingWaitMs = maxWaitMs - elapsed;
    partitionsCount = cluster.partitionCountForTopic(topic);
} while (partitionsCount == null);
```

再看awaitUpdate()方法，这个方法里就是不断地wait直到超时或者拉取结束

```java
public synchronized void awaitUpdate(final int lastVersion, final long maxWaitMs) throws InterruptedException {
    if (maxWaitMs < 0) {
        throw new IllegalArgumentException("Max time to wait for metadata updates should not be < 0 milliseconds");
    }
    long begin = System.currentTimeMillis();
    long remainingWaitMs = maxWaitMs;
    // 通过版本号判断是否拉取结束
    while (this.version <= lastVersion) {
        AuthenticationException ex = getAndClearAuthenticationException();
        if (ex != null)
            throw ex;
        if (remainingWaitMs != 0)
            wait(remainingWaitMs);
        // 线程被唤醒之后还要在判断一次时间，如果是在别的地方唤醒，需要把已经等待的时间减去，然后接着等待同步元数据
        long elapsed = System.currentTimeMillis() - begin;
        if (elapsed >= maxWaitMs)
            throw new TimeoutException("Failed to update metadata after " + maxWaitMs + " ms.");
        remainingWaitMs = maxWaitMs - elapsed;
    }
}
```

##### sender同步数据

在sender的run方法最后面调用了一个poll方法，这个poll调用的是KafkaClient的poll方法，这个方法的下面有个handleCompletedReceives()，就是处理收到结果后的处理，这个方法里使用MetadataUpdater.handleCompletedMetadataResponse更新元数据，MetadataUpdater是一个接口，默认的实现类就是DefaultMetadataUpdater，这个是KafkaClient的内部类，这个方法里继续调用metadata.update方法：

```java
public synchronized void update(Cluster newCluster, Set<String> unavailableTopics, long now) {
    Objects.requireNonNull(newCluster, "cluster should not be null");
	// 标志位改成false了
    this.needUpdate = false;
    // 最后更新时间
    this.lastRefreshMs = now;
    this.lastSuccessfulRefreshMs = now;
    // 版本号加1
    this.version += 1;

    // 处理过期的topic信息
    if (topicExpiryEnabled) {
        ……
    }
    // 更新元数据信息
	……
    // 唤醒线程，之前等待元数据信息的可以继续执行了
    notifyAll();
    log.debug("Updated cluster metadata version {} to {}", this.version, this.cluster);
}
```

##### 选择分区

KafkaProducer的partition方法会先判断发送的消息是否指定了分区，指定了就直接返回，没有的话会调用DefaultPartitioner的partition方法：

```java
public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {
    // 拿到这个topic的所有分区
    List<PartitionInfo> partitions = cluster.partitionsForTopic(topic);
    int numPartitions = partitions.size();
    // 发消息时没有指定key
    if (keyBytes == null) {
        int nextValue = nextValue(topic);
        List<PartitionInfo> availablePartitions = cluster.availablePartitionsForTopic(topic);
        // 可用分区数量大于0就用nextValue和可用分区取余
        if (availablePartitions.size() > 0) {
            int part = Utils.toPositive(nextValue) % availablePartitions.size();
            return availablePartitions.get(part).partition();
        } else {
            // 和所有分区取模，这时候返回的分区都是不可用的
            return Utils.toPositive(nextValue) % numPartitions;
        }
    } else {
        // 如果指定了key，会根据key的byte数组转成32位证书，和总分区数取模，这样就可以保证同一个key会落到同一个分区
        // 可以实现发送消息顺序性
        return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;
    }
}
```

##### 加入缓冲

加入缓冲的方法在RecordAccumulator.append()方法，首先获取batch所在集合，集合的变量名是batches，类型是CopyOnWriteMap，这个是kafka自己实现的mao，适合度多写少，原理和CopyOnWriteArrayList一样

```java
// 从ConcurrentMap<TopicPartition, Deque<ProducerBatch>> = new CopyOnWriteMap<>()中获取分区对应的batch
Deque<ProducerBatch> dq = getOrCreateDeque(tp);
```

然后尝试append消息：

```java
// 加锁
synchronized (dq) {
    RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callback, dq);
    // 如果dq是空直接返回空
    // 如果不是空就是写入成功了，直接返回结果
    if (appendResult != null)
        return appendResult;
}
```

如果是空就说明没有正在使用的batch，然后计算出来消息的大小：

```java
int size = Math.max(this.batchSize, AbstractRecords.estimateSizeInBytesUpperBound(maxUsableMagic, compression, key, value, headers));
```

然后从缓冲池里分配一块内存：

```java
buffer = free.allocate(size, maxTimeToBlock);
```

然后再次尝试写入，这里使用双重检查防止多线程的时候出现并发创建batch，在老版本的时候会多一个把申请的内存放到bytebuffer里，是为了防止多线程获取到多个buffer，然后把多余的还回去。

```java
synchronized (dq) {
    RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callback, dq);
    // 如果不是空就是写入成功了，直接返回结果
    if (appendResult != null)
        // 新版本去掉了，新版本应该是在finally里面实现的这个
        free.deallocate(buffer);
        return appendResult;
}
```

为什么要双重检查呢，因为高并发的时候可能会出现多个线程申请到了ByteBuffer，假设10个线程申请完了之后阻塞在synchronized (dq) 这里，一个线程进去后双重检查是可以通过的，会进行后续的创建batch操作，在放回到dq中，后面的线程再进来就会在双重检查这里尝试插入成功会直接返回，不会执行后面的创建batch操作，双重检查之后会把buffer封装到MemoryRecordsBuilder中，使用刚刚创建的MemoryRecordsBuilder封装成ProducerBatch，然后ProducerBatch.tryAppend()，最后把batch加到dq中，buffer设置成null：

```java
MemoryRecordsBuilder recordsBuilder = recordsBuilder(buffer, maxUsableMagic);
ProducerBatch batch = new ProducerBatch(tp, recordsBuilder, time.milliseconds());
FutrueRecordMetadata futrue = Utils.notNull(batch.tryAppend(timestamp, key, value, headers, callback, time.milliseconds()));

dq.addLast(batch);
// 正在进行中的batch
incomplete.add(batch);
// 这步第一是为了加快gc，减少gc的可达性分析，第二是防止申请出来的buffer被释放
// finally中判断buffer != null的时候会把申请的ByteBuffer还给bufferPool
buffer = null;
```

RecordAccumulater的tryAppend很简单就是从dq里取出一个batch，然后调用batch.tryAppend()，所以不管是双重检查通过后穿件新的batch还是双重检查的时候尝试使用已有的batch，最后都是调用batch.tryAppend：

```java
// 判断batch剩余内存是否充足
if (!recordsBuilder.hasRoomFor(timestamp, key, value, headers)) {
    return null;
} else {
    // 写入数据，实现在MemoryRecordsBuilder的appendLegacyRecord()方法
    Long checksum = this.recordsBuilder.append(timestamp, key, value, headers);
    // 记录最大的消息大小
    this.maxRecordSize = Math.max(this.maxRecordSize, AbstractRecords.estimateSizeInBytesUpperBound(magic(),
	……
}
```

点进MemoryRecordsBuilder的appendLegacyRecord()方法：

```java
private long appendLegacyRecord(long offset, long timestamp, ByteBuffer key, ByteBuffer value) throws IOException {
	……
    // 获取消息大小
    int size = LegacyRecord.recordSize(magic, key, value);
    // 这里写入了offset和size
    AbstractLegacyRecordBatch.writeHeader(appendStream, toInnerOffset(offset), size);

    if (timestampType == TimestampType.LOG_APPEND_TIME)
        timestamp = logAppendTime;
    // 这里会先计算出来crc，然后按照 crc -> magic -> attibutes -> timestamp -> key size -> key -> value size -> value的顺序写入
    // 使用appendStream往byteBuffer里写
    long crc = LegacyRecord.write(appendStream, magic, timestamp, key, value, CompressionType.NONE, timestampType);
    recordWritten(offset, timestamp, size + Records.LOG_OVERHEAD);
    return crc;
}
```

appendStream是怎么来的呢？这个appendStream是在创建MemoryRecordsBuilder的时候，先用ByteBuffer来实例化一个ByteBufferOutputStream，然后在用ByteBufferOutputStream来通过compressionType.wrapForOutput()获取到对应的压缩流，CompressionType是一个枚举类，里面有none、gzip、snappy、lz4。

### 发送消息

加入缓冲之后，sender线程就可以从缓冲中拿到消息发送了，sender中的run方法显示处理了一下transactionManager的东西，这个先不看，然后执行**sendProducerData()**，这个是核心发送的逻辑，方法里首先获取了一下集群的信息，然后通过缓冲区拿到能够发送的broker节点：

```java
Cluster cluster = metadata.fetch();

// 这里拿到的是ReadyCheckResult，里面有个列表表示要发送数据的节点
RecordAccumulator.ReadyCheckResult result = this.accumulator.ready(cluster, now);
```

ReadyCheckResult结构：

```java
public final static class ReadyCheckResult {
    // 节点信息
    public final Set<Node> readyNodes;
    // 下一次尝试发送的时间
    public final long nextReadyCheckDelayMs;
    // 没有leader的topic信息
    public final Set<String> unknownLeaderTopics;
}
```

再看缓冲池（RecordAccumulator）的ready方法：

```java
public ReadyCheckResult ready(Cluster cluster, long nowMs) {
    Set<Node> readyNodes = new HashSet<>();
    long nextReadyCheckDelayMs = Long.MAX_VALUE;
    Set<String> unknownLeaderTopics = new HashSet<>();
	// 这个表示buffer池中有没有空闲的ByteBuffer了
    boolean exhausted = this.free.queued() > 0;
    // 开始遍历缓存中的batches
    for (Map.Entry<TopicPartition, Deque<ProducerBatch>> entry : this.batches.entrySet()) {
        TopicPartition part = entry.getKey();
        Deque<ProducerBatch> deque = entry.getValue();

        Node leader = cluster.leaderFor(part);
        synchronized (deque) {
            // 没有leader就在这个变量里添加topic信息
            if (leader == null && !deque.isEmpty()) {
                unknownLeaderTopics.add(part.topic());
            }
            // 只有要发送数据的分区的leader没添加过并且muted不包含这个分区
            // 当设置同时发送的请求是1时，发送完消息不管收没收到响应都会在这个集合中插入分区信息
            else if (!readyNodes.contains(leader) && !muted.contains(part)) {
                // 取出第一个batch
                ProducerBatch batch = deque.peekFirst();
                if (batch != null) {
                    // 这个是batch创建到现在的时间
                    long waitedTimeMs = batch.waitedTimeMs(nowMs);
                    // 发送失败次数>0并且等待时间小于重试间隔
                    // batch.attempts()是获取发送失败次数
                    boolean backingOff = batch.attempts() > 0 && waitedTimeMs < retryBackoffMs;
                    // 如果是发送失败，timeToWaitMs就赋值失败等待时间，否则就是正常消息的等待时间
                    long timeToWaitMs = backingOff ? retryBackoffMs : lingerMs;
                    // 取出来的batch是否已满，如果dqueue里面的batch超过1个，那取出来的这个肯定是满的
                    // 如果batch只有一个，就判断这个batch的size是不是16k或者我们自己设置的
                    boolean full = deque.size() > 1 || batch.isFull();
                    // 判断这个batch是否过期
                    boolean expired = waitedTimeMs >= timeToWaitMs;
                    // 当batch满了||已经过期||bufferPool没有内存了||客户端正在关闭||正在flush
                    boolean sendable = full || expired || exhausted || closed || flushInProgress();
                    // 如果达到了上述的条件，并且不是重发，就添加分区的leader信息
                    if (sendable && !backingOff) {
                        readyNodes.add(leader);
                    } else {
                        // 否则就计算下次可以发送的时间
                        long timeLeftMs = Math.max(timeToWaitMs - waitedTimeMs, 0);
                        nextReadyCheckDelayMs = Math.min(timeLeftMs, nextReadyCheckDelayMs);
                    }
                }
            }
        }
    }

    return new ReadyCheckResult(readyNodes, nextReadyCheckDelayMs, unknownLeaderTopics);
}
```

回到Sender的sendProducerData方法，获取完leader信息后不确定leader的机器要重新拉取元数据信息：

```java
for (String topic : result.unknownLeaderTopics)
    this.metadata.add(topic);
this.metadata.requestUpdate();
```

然后遍历可以发送的leader节点，没有连接建立连接

```java
Iterator<Node> iter = result.readyNodes.iterator();
long notReadyTimeout = Long.MAX_VALUE;
while (iter.hasNext()) {
    Node node = iter.next();
    // 判断broker节点是否可以通信
    if (!this.client.ready(node, now)) {
        iter.remove();
        notReadyTimeout = Math.min(notReadyTimeout, this.client.connectionDelay(node, now));
    }
}
```

判断节点是否可以通信在NetworkClient的ready方法：

```java
public boolean ready(Node node, long now) {
    // 判断是否可以发送
    if (isReady(node, now))
        return true;
    这个里面如果是没连接过，就返回true，否则返回是否是不能连接并且上次连接到现在的时间小于reconnect.backoff.ms
    if (connectionStates.canConnect(node.idString(), now))
        // 初始化连接
        initiateConnect(node, now);

    return false;
}
// isReady调用到了这里
public boolean isReady(Node node, long now) {
    // 满足canSendRequest()并且没有正在拉取元数据才可以发送
    return !metadataUpdater.isUpdateDue(now) && canSendRequest(node.idString());
}
// canSendRequest在这里
private boolean canSendRequest(String node) {
    // broker连接状态的缓存是建立连接的
    // broker已经在selector上注册好了
    // 同一时间允许没有响应的请求时没有到上限
    return connectionStates.isReady(node) && selector.isChannelReady(node) && inFlightRequests.canSendMore(node);
}
```

所以如果判断出来broker不能发送，但是可以连接或者尝试重连，就会初始化连接，初始化连接核心代码是调用了selector的connect方法：

```java
// 这是一个状态模式，标记当前broker的状态，不同状态可以做不同操作
this.connectionStates.connecting(nodeConnectionId, now);
selector.connect(nodeConnectionId,
                 new InetSocketAddress(node.host(), node.port()),
                 this.socketSendBuffer,
                 this.socketReceiveBuffer);
```

这个方法里在新版本中被拆成了多个方法，整合到一起大概是这样的：

```java
// 初始化一个SocketChannel
SocketChannel socketChannel = SocketChannel.open();
// 设置为非阻塞
socketChannel.configureBlocking(false);
Socket socket = socketChannel.socket();
// 设置keepAlive，这个意思是如果2个小时内双方没有通信就发送一个探测包，根据结果选择保持连接还是重新连接或者断开连接
// 2个小时是从注释看到的
socket.setKeepAlive(true);
// 设置发送和接收的缓冲大小
if (sendBufferSize != Selectable.USE_DEFAULT_BUFFER_SIZE)
    socket.setSendBufferSize(sendBufferSize);
if (receiveBufferSize != Selectable.USE_DEFAULT_BUFFER_SIZE)
    socket.setReceiveBufferSize(receiveBufferSize);
// tcp no delay的意思是关闭Nagle，一些小包会被合并成大包一起发送，设置成true的话就会有一些延迟
socket.setTcpNoDelay(true);
// 发送连接请求，如果配置的是阻塞模式，会阻塞在这里，如果是非阻塞但是发起和接收都在同一台机器上，这里就会马上返回成功
// 否则需要监听OP_CONNECT事件
boolean connected = channel.connect(address);
// 这步可以理解成把SocketChannel和SelectionKey绑定
SelectionKey key = socketChannel.register(nioSelector, SelectionKey.OP_CONNECT);
// 这里最核心的就是把selectionKey封装到TransportLayer里
KafkaChannel channel = channelBuilder.buildChannel(id, key, maxReceiveSize, memoryPool);
this.channels.put(id, channel);
// 这个我理解的是把SelectionKey和SocketChannel绑定
// 和上面的正好相反，这样就可以通过SocketChannel找到selectionKey，也可以通过selectionKey找到SocketChannel
key.attach(channel);

if (connected) {
    // 如果刚才是连接的结果是成功，标记一下马上就成功的SelectionKey，设置关注的事件是0
    immediatelyConnectedKeys.add(key);
    key.interestOps(0);
}
```

回到Sender的sendProducerData()方法，检查完连接之后会进行batch的超时处理，以后再说。如果所有节点都没有建立好连接，就会计算出来所有没建立好连接的超时时间中的最小值，和之前在ready方法里计算出来的发送的超时最小值取小返回。返回之后会继续往下走，走到NetworkClient的pool()方法，poll方法里首先获取一下距离下次更新元数据的时间，然后用这个时间和之前获取的时间还有设置的请求超时时间取一个最小值，当成poll的执行最大时间：

```java
long metadataTimeout = metadataUpdater.maybeUpdate(now);
this.selector.poll(Utils.min(timeout, metadataTimeout, requestTimeoutMs));
long updatedNow = this.time.milliseconds();
List<ClientResponse> responses = new ArrayList<>();
// 完成的请求
handleCompletedSends(responses, updatedNow);
// 完成的响应
handleCompletedReceives(responses, updatedNow);
// 连接失败的请求
handleDisconnections(responses, updatedNow);
handleConnections();
handleInitiateApiVersionRequests(updatedNow);
// 判断超时
handleTimedOutRequests(responses, updatedNow);
// 这里是处理回调函数的逻辑
completeResponses(responses);
```

Selector的poll方法里没有真正处理请求，主要的代码如下：

```java
// 清掉上次poll标记的东西，包括selectionKey
clear();
// 这里获取有多少selectionKey是ready的
// 如果timeout是0会调用nioSelector.selectNow()，selectNow和select(0)差不多，如果超时是0，会一直阻塞
// 否则会调用nioSelector.select(timeoutMs)
int numReadyKeys = select(timeout);
// 当numReadyKeys大于0，就说明存在ready状态的selectionKey，拿到readyKeys
Set<SelectionKey> readyKeys = this.nioSelector.selectedKeys();
// 执行核心的poll方法
pollSelectionKeys(readyKeys, false, endSelect);
// 这个是kafka维护的lru列表，保证一个客户端维护的连接数量
maybeCloseOldestConnection(endSelect);
```

再看pollSelectionKeys中的代码（简化版）：

```java
// 这里很奇怪，没有使用selectionKey的迭代器，取出key就删除，居然只是遍历了
// 遍历的时候把selectionKeys顺序打乱，防止内存不够，一个大的请求卡在第一个位置发不出去
for (SelectionKey key : determineHandlingOrder(selectionKeys)) {
    // 之前使用的selectionKey attach达到channel，现在就可以根据key获取到这个channel了
    KafkaChannel channel = channel(key);
	// 更新lru列表
    if (idleExpiryManager != null)
        idleExpiryManager.update(channel.id(), currentTimeNanos);
    try {
        // 如果是可以连接状态，就要完成连接
        if (isImmediatelyConnected || key.isConnectable()) {
            // 这里通过kafkaChannel调用TransportLayer的finishConnect()完成连接的建立
            // boolean connected = socketChannel.finishConnect();
        	// if (connected)
            // 这里设置selectionKey取消关注OP_CONNECT增加对OP_READ的关注，因为刚建立连接没必要还在关注OP_CONNECT
            // 但是没有设置OP_WRITE事件，因为这个时候还没有开始写数据
            // 		key.interestOps(key.interestOps() & ~SelectionKey.OP_CONNECT | SelectionKey.OP_READ);
            if (channel.finishConnect()) {
                this.connected.add(channel.id());
                this.sensors.connectionCreated.record();
            } else
                continue;
        }
        // 如果channel没有ready，需要三次握手加上权限认证
        if (channel.isConnected() && !channel.ready()) {
            try {
                channel.prepare();
            } catch (AuthenticationException e) {
                sensors.failedAuthentication.record();
                throw e;
            }
            if (channel.ready())
                sensors.successfulAuthentication.record();
        }
        // 读数据直接把数据读取到缓冲中
        if (channel.hasBytesBuffered()) {
            keysWithBufferedRead.add(key);
        }
        // 如果是写事件调用channel.write()发送数据
        if (channel.ready() && key.isWritable()) {
            Send send = channel.write();
            if (send != null) {
                this.completedSends.add(send);
                this.sensors.recordBytesSent(channel.id(), send.size());
            }
        }
        /* cancel any defunct sockets */
        if (!key.isValid())
            close(channel, CloseMode.GRACEFUL);
    } finally {
        maybeRecordTimePerConnection(channel, channelStartTimeNanos);
    }
}
```

到这里，第一次发送数据的流程就结束了，主要是建立连接，然后接收连接建立成功，准备写数据。

当第二次进入Sender的run() -> sendProducerData()的时候，此时broker已经建立好连接了，首先要执行的就是封装发送数据：

```java
// 调用缓冲区把创建发送的数据，key是node的id，得到的是每个node上可以发送的batch集合
Map<Integer, List<ProducerBatch>> batches = this.accumulator.drain(cluster, result.readyNodes,
                                                                   this.maxRequestSize, now);
```

drain方法没有特殊的处理，只是做一些过滤和判断，核心的代码：

```java
for (Node node : nodes) {
    // do while就是把node里的partition遍历一遍
    do {
        // 拿到partition中的缓存队列中的第一个batch
        ProducerBatch first = deque.peekFirst();
        // 大小判断、过滤掉发送中的
        // 满足条件就取出第一个
        ProducerBatch batch = deque.pollFirst();
        // 设置一些状态
    } while (start != drainIndex);
}
```

然后会带Map<Integer, List<ProducerBatch>>走到下一个sendProduceRequest()：

```java
// 把同一个node上的batch按照partition分组
Map<TopicPartition, MemoryRecords> produceRecordsByPartition = new HashMap<>(batches.size());
// 整个请求封装成ProduceRequest.Builder
ProduceRequest.Builder requestBuilder = ProduceRequest.Builder.forMagic(minUsedMagic, acks, timeout,
                                                                        produceRecordsByPartition,
                                                                        transactionalId);
// 在封装成clientRequest
ClientRequest clientRequest = client.newClientRequest(nodeId, requestBuilder, now, acks != 0, callback);
client.send(clientRequest, now);
```

NetworkClient的doSend方法:

```java
// 通过builder.build()实例化一个ProduceRequest，这个和老版本不一样，找了半天
doSend(clientRequest, isInternalRequest, now, builder.build(version));
```

继续看下一个doSend方法：

```java
private void doSend(ClientRequest clientRequest, boolean isInternalRequest, long now, AbstractRequest request) {
    String nodeId = clientRequest.destination();
    // 封装一个header
    RequestHeader header = clientRequest.makeHeader(request.version());
    // 封装send
    Send send = request.toSend(nodeId, header);
    // 维护inFlightRequests
    InFlightRequest inFlightRequest = new InFlightRequest(
        header,
        clientRequest.createdTimeMs(),
        clientRequest.destination(),
        clientRequest.callback(),
        clientRequest.expectResponse(),
        isInternalRequest,
        request,
        send,
        now);
    this.inFlightRequests.add(inFlightRequest);
    // 调用kafka的selector的send方法，这里面只调用了kafkaChannel的setSend方法
    selector.send(inFlightRequest.send);
}
```

KafkaChannel的setSend核心一行代码：

```java
// 设置selectionKey关注OP_WRITE事件
// 还记得上面建立连接之后并没有关注OP_WRITE事件，在确定要发送消息的时候也就是这里才开始关注
this.transportLayer.addInterestOps(SelectionKey.OP_WRITE);
// 里面执行的代码是key.interestOps(key.interestOps() | ops);
```

因为sendProducerData(now)之后马上回执行client.poll方法，在这个方法中执行的是selector的pollSelectionKeys方法：

```java
if (channel.ready() && key.isWritable()) {
    // 发送数据
    Send send = channel.write();
    if (send != null) {
        // 维护成功发送的请求
        this.completedSends.add(send);
        this.sensors.recordBytesSent(channel.id(), send.size());
    }
}
```

kafkChannel里的write有一行代码非常关键：

```java
private boolean send(Send send) throws IOException {
    // 这里就是调用java nio的channel.write()
    send.writeTo(transportLayer);
    if (send.completed())
        // 这里会把OP_WRITE事件从selectionKey关注中移除
        // key.interestOps(key.interestOps() & ~ops);
        transportLayer.removeInterestOps(SelectionKey.OP_WRITE);

    return send.completed();
}
```

##### 对关注事件操作的总结

SelectionKey相关的时间一共有4个，OP_READ，OP_WRITE，OP_CONNECT，OP_ACCEPT，上面没有OP_ACCEPT，因为他表示接收连接，在producer中都是发起连接，这4个事件分别是int的第0、2、3、4位，第1位没有值？？？

发送消息的源码一共出现了3次关注事件：

*   socketChannel.register(nioSelector, SelectionKey.OP_CONNECT);这个就是注册的时候设置的关注事件；
*   key.interestOps(key.interestOps() & ~SelectionKey.OP_CONNECT | SelectionKey.OP_READ);连接之后开始关注OP_READ事件；
*   key.interestOps(key.interestOps() | ops);把消息发送到缓冲里后开始关注OP_WRITE事件；

可以看到nio的SelectionKey是可以同时关注多个事件的，通过& ~来取消关注，| 新增关注。

##### 如果一次write不能没有发送所有的ByteBuffer数据

在发送数据的时候Send里会初始化一个remaining，这个数据的大小，然后每次write之后remaining都会减掉发送数据的大小，kafkChannel的send中通过send.completed()来判断是否全部发送，其实就是判断remaining是不是=0，如果remaining>0，不会标记成completed，send也不会标记成null，然后还记得在run方法中调用的ready()方法吗，在这里会调用InFlightRequests的canSendMore()方法：

```java
public boolean canSendMore(String node) {
    Deque<NetworkClient.InFlightRequest> queue = requests.get(node);
    // queue.peekFirst().send.completed()就是判断一个请求是否发送完
    return queue == null || queue.isEmpty() ||
        (queue.peekFirst().send.completed() && queue.size() < this.maxInFlightRequestsPerConnection);
}
```

如果没有发送完，result会把他删掉，就不会发送心情求了，但是，poll方法还会执行，并且这个SelectionKey还在关注OP_WRITE事件，就会继续发送数据，直到数据全部发送出去。
