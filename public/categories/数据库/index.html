<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>数据库 | 王小红的笔记</title><meta name=keywords content><meta name=description content><meta name=author content><link rel=canonical href=https://wangxiaohong123.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/><link crossorigin=anonymous href=/assets/css/stylesheet.08f7d74f0ada0f975d29ae436285b61ed7a719d05f350cb888d00341642995a2.css integrity="sha256-CPfXTwraD5ddKa5DYoW2HtenGdBfNQy4iNADQWQplaI=" rel="preload stylesheet" as=style><link rel=icon href=https://wangxiaohong123.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://wangxiaohong123.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://wangxiaohong123.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://wangxiaohong123.github.io/apple-touch-icon.png><link rel=mask-icon href=https://wangxiaohong123.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://wangxiaohong123.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/index.xml><link rel=alternate hreflang=en href=https://wangxiaohong123.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://wangxiaohong123.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"><meta property="og:site_name" content="王小红的笔记"><meta property="og:title" content="数据库"><meta property="og:locale" content="zh-CN"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="数据库"><meta name=twitter:description content></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://wangxiaohong123.github.io/ accesskey=h title="王小红的笔记 (Alt + H)">王小红的笔记</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://wangxiaohong123.github.io/posts/ title=笔记><span>笔记</span></a></li><li><a href=https://wangxiaohong123.github.io/tags/ title=标签><span>标签</span></a></li><li><a href=https://wangxiaohong123.github.io/categories/ title=分类><span>分类</span></a></li></ul></nav></header><main class=main><header class=page-header><h1>数据库</h1></header><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>5.底层原理</h2></header><div class=entry-content><p>HLog 每个HRegionServer都有一个HLog，也可以配置多个，所有region共享这个HLog，每次操作数据都会产生一个log，很像MySQL的redoLog，这条log的key就是表名+region名+写入时间+sequenceid+clusterid，内容就是对那个列改了什么value。
在hdfs上有两个专门的目录存放HLog："/hbase/WALs"和"/hbase/oldWALs"，WALs里存放的是还没过期的数据，就说还在memStore里，没有刷到hdfs中的，在这个目录下面对于每个HRegionServer都有一个自己的目录，类似这样：
/hbase/WALs/hbase12.df.zszs.org,60020,1304970381600
hbase12.df.zszs.org是HRegionServer的机器域名，60020是统一的端口号，最后面试创建的时间戳，在HRegionServer文件夹里就是一堆HLog文件了。有多个HLog文件是为了方便删除，因为大部分的数据都会刷到hdfs中，已经落盘的数据对应的HLog就没有存在的必要了。
什么时候创建一个新的HWAL日志文件呢？通过配置hbase.regionserver.logroll.period，这个默认是1小时，每个小时HBase后台的一个线程就会去创建一个新的日志文件。当数据被持久化到了hdfs中后，对应的日志文件就会被放到oldWALs中，然后HBase的后台还有一个线程根据参数hbase.master.cleaner.interval(默认1分钟)来检查文件在oldWALs文件夹里待的时间超过了hbase.master.logcleaner.ttl(默认10分钟)，就会把这个日志删掉了。为什么还要再等10分钟呢？据说是主要用于调试。
memStore的写入 memStore使用的是双跳表机制来实现key的有序性，直接使用的java的ConcurrentSkipListMap，把rowkey+列族+列+timestamp当做跳表的key，当一个跳表满了的时候新进来的数据都会写入到另一个跳表中，这个跳表的数据慢慢刷到hdfs里，数据的value存储在chunk数组里。
HFile 逻辑组成 HFile逻辑上包含4部分：
Scanned block：Data Block（key-value数据），Leaf Index Block（索引树的叶子节点），Bloom Block（布隆过滤器）； Non-scanned Block：Meta Block、Intermediate Level Data Index Blocks； Load-on-open Block：这部分会在RegionServer打开HFile的时候直接加载到内存里去，包括FileInfo、布隆过滤器的MetaBlock、Root Index Block和Meta IndexBlock（在查找元素的时候都要从根节点开始）、Bloom Index Block（这个是布隆过滤器的索引，因为一个HFile可能会有很多布隆过滤器）； Trailer Block：HFile版本和其他几个部分的偏移量以及寻址信息，寻址信息就包括Load-on-open Block的地址； 当HRegionServer打开HFile的时候会先读取文件的信息，这个时候就知道了文件有多少字节，然后就可以从末尾把Trailer Block读取出来，Trailer中有Load-on-open Block的位置信息，就可以把Load-on-open Block读取到内存中。
查询 当从HFile里查找文件的时候，会现根据Load-on-open Block中的布隆过滤器索引拿到所有的过滤器，然后判断要查找的数据是否在这个文件中，如果在就根据LSM树来查找数据，也就是根据Scanned block中的Leaf Index Block最终拿到Data Block，data block中就是一个一个的key-vakue对，key由rowkey、列族、列、操作时间戳、keyType（keyType就是Put、Delete、DeleteColumn、DeleteFamily这些）组成。
LSM树和MySQL的聚簇索引差不多，当数据量少的时候只有一层，Load-on-open Block直接指向Data Block，当数据量变多后，会出现叶子节点，再多就会出现Non-scanned Block的Intermediate Level Data Index Blocks。
不管是get还是scan，底层都是scan，在查询时，首先会有3层scan（RegionScanner、StoreScanner、MemStoreScanner和StoreFileScanner），第一层先找到对应的region，第二层找到store，第三层在memStore找到具体的数据和通过布隆找对具体的HFile。
找到数据之后会在内存中进行合并筛选。
block cache 因为每次查找数据会涉及到几次磁盘IO，读取intermediate level index block发生一次IO，读取leaf index block发生一次IO，读取data block发生一次IO，最多就3次IO，但是如果是批量get并且只有每次只有3个IO，查询速度也会达到秒级，所以hbase有一个和MySQL的缓冲池类似的东西，block cache，它是以我们存储数据的block为单位存储，其实就是3个ConcurrentHashMap，map的key就是block的key，value可能就是一个地址的指向：
single-access：数据刚被读取的时候会在这个map里 multi-access：当数据被第二次使用的时候会从single-access移动到multi-access里 in-memory：比如在创建列族的时候指定了n_memory=true，这个列族的block就会存在在这个map中 这3个map都是LRU map，内存占用比是25%:50%:25%。
...</p></div><footer class=entry-footer><span title='2021-06-24 06:27:35 +0000 UTC'>June 24, 2021</span>&nbsp;·&nbsp;1 min</footer><a class=entry-link aria-label="post link to 5.底层原理" href=https://wangxiaohong123.github.io/posts/%E5%9F%BA%E7%A1%80/%E6%95%B0%E6%8D%AE%E5%BA%93/hbase/05%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>3.物理模型</h2></header><div class=entry-content><p>hbase使用多维的稀疏排序map存储数据，比如像下面这样：
rowkey_01+列族1:列1+put+t3：v1，他的key是多维的，存储了rowkey、列族名称、列名称、操作方法、操作时间戳，并且根据key进行排序，value就是操作的值，这样看的话就是一个多维排序的map，稀疏的话说的是每一列不一定有值，这样看起来就很稀疏。
他是一种列簇式存储，把同一列族下的所有列的值存到一个大文件里，所以查询同一列族下的数据会很快，到是一次查找多个列族，就会涉及到读取多个文件，所以在建表的时候需要考虑好，尽量让每次插叙都是同一列族下的列数据，相对于列存储和行存储，列簇存储更灵活，如果一个表只有一个列簇，就变成了行存储，如果每一个列簇只有一个列，就变成了列存储。
架构图</p></div><footer class=entry-footer><span title='2021-06-14 06:27:35 +0000 UTC'>June 14, 2021</span>&nbsp;·&nbsp;1 min</footer><a class=entry-link aria-label="post link to 3.物理模型" href=https://wangxiaohong123.github.io/posts/%E5%9F%BA%E7%A1%80/%E6%95%B0%E6%8D%AE%E5%BA%93/hbase/03%E7%89%A9%E7%90%86%E6%A8%A1%E5%9E%8B/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>4.put流程</h2></header><div class=entry-content><p>hbase:meta put就是找到对应的regision，然后把数据写到HLog和对应的store里去，所以第一步就是要先拿到region信息，这个信息存到了一张meta表里，所属空间是hbase，当client端没找到region信息的时候就回去这个表里查出来region的信息，然后缓存在本地，下次直接读取本地的就可以了，但是hbase:meta也是一张表，client怎么知道这个表在那个region上呢？在zk中，zk里维护了一份meta表的信息，client先去zk上拿到meta在那，然后把数据读取出来，拿到了将要put的表的region信息，就可以进行put操作了。
hbase:meta只有一个列族，叫info，他的rowkey结构是表名+起始rowkey+region创建时间戳+这3个字段的md5，表里面有4个列，分别是info:regioninfo(包括md5值，region名称，起始rowkey，结束rowkey)、info:seqnumDuringOpen(region打开时的sequenceId)、info:server(存储Region在哪个RegionServer上)、info:serverstartcode(RegionServer的启动timestamp)。
开始put 找到region信息之前还需要判断是否开启了autoFlush，如果autoFlush=false(默认是true)，这个时候是不会想region发送数据的，他会在客户端缓存数据，当数据达到2M时一起推送到region，一般没人开这个，因为开了这个数据很容易就丢失。
拿到region信息之后就要把哪一行数据加锁，然后把这次的操作封装成一个FSWALEntry对象写入WAL的队列中，这是一个基于disruptor的无锁有界队列，消费者拿到这条数据后会把WAL日志写到缓存中，修改完成之后会释放锁，并且把结果封装成SyncFutrue对象发送到disruptor中，消费者拿到这条数据后就可以把WAL日志刷到hdfs里去了，对于HLog的持久化有5种机制，在调用JavaAPI的时候可以通过**put.setDurability(Durability.SYNC_WAL)**来设置：
SKIP_WAL：不写WAL日志，把数据写到memStore里就算成功，但是memStore里的数据可能还没写到HFile中，数据丢失的概率也非常大； ASYNC_WAL：这个也是把数据写到memStore里就算成功，然后异步去写WAL； SYNC_WAL：要把WAl日志写到hdfs里，但是也只是存在在hdfs的os cache中； FSYNC_WAL：强制刷到hdfs的磁盘里才算成功； USER_DEFAULT：默认级别，就是SYNC_WAL； memStore频繁full gc问题 大量的put操作会让memStore年青代频繁gc，短时间发生多次gc就会有大量的对象躲过gc并进入老年代，这样很快老年代也会满掉，频繁的发生full gc，HBase通过chunk数组的方式，每一个memStore会创建一个MemStoreLAB对象，对象里是一个2M的chunk数组，这里的2M可以和autoFlush的2M对应起来，当一个chunk数组写满后，就会申请一个新的chunk数组，这样就算老年代发生了full gc，但是使用了数组的数据结构，内存都是连续的，减少了内存碎片，第一gc的频率会比之前低很多，第二减少了gc之后的整理内存时间。但是还是会频繁的young gc，所以他把chunk放到了一个chunk池里反复利用。
memStore生成HFile的时机 当memStore中的数据达到128M的时候，使用hbase.hregion.memstore.flush.size控制这个数值； 当region中的所有memStore总和超过了hbase.hregion.memstore.block.multiplier * hbase.hregion.memstore.flush.size时； 当所有memStore的总和超过了hbase.regionserver.global.memstore.size.lower.limit * hbase.regionserver.global.memstore.size的时候，从大到小刷，刷到不满足为止； 当HLog数量超过了hbase.regionserver.maxlogs时，选择最早的HLog来刷； 默认一小时刷一次； 通过flush命令；</p></div><footer class=entry-footer><span title='2021-06-14 06:27:35 +0000 UTC'>June 14, 2021</span>&nbsp;·&nbsp;1 min</footer><a class=entry-link aria-label="post link to 4.put流程" href=https://wangxiaohong123.github.io/posts/%E5%9F%BA%E7%A1%80/%E6%95%B0%E6%8D%AE%E5%BA%93/hbase/04put%E6%B5%81%E7%A8%8B/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>1.habse基础</h2></header><div class=entry-content><p>HDFS和HBase的关系： HDFS能做的事非常少，也就是创建文件，删除文件，大文件读取，追加数据，如果想要修改或者根据某些条件查询就不行，hbase就是做这个的。
hbase是一个nosql，他不负责存储数据，需要基于HDFS来实现，但是他不能执行复杂的条件查询，对海量数据的简单的增删改查。
架构特点 hbase基于多台regionServer来管理数据分片，regionServer是高可用的，region是存储在HDFS中，本省就有两个副本。
主从强一致。
支持MapReduce和spark这种分布式计算引擎。
支持java API。
支持协处理器，块缓存和布隆过滤器。
可以使用web端管理和运维。
使用场景 海量数据的简单增删改查，不支持索引，也不知道事务，不支持SQL语法。想要上面的功能也没必要用hbase，实现索引可以用es，实现海量数据事务可以用TiDB，实现海量数据的的实时分析可以用clickhouse或者druid。
数据模型和物理模型 针对数据模型有几个概念：
rowkey：每行都有一个rowkey，和mysql的主键差不多，也会根据这个排序，所以要把相似的数据放到一起，这样的话好查，比如这里存了订单和订单明细，rowkey可以是这样的order_1_111，order-detail_1_110，就是用户1在的111订单，和用户1的110订单明细。
列族：就是列名，两部分组成，列族+分号+列限定符（column qualifier），列族就是一系列的类的family，有点像表的意思。
列：他的列是要存储多个版本的值的，每个值都带着一个时间戳（版本）。
单元格：取到某行某列的某个时间戳对应的值就是一个单元格。
rowkey order:base order:detail order:extent
order_1_110 xxx xxx
order_1_111 x1(t1); x2(t2) xxx xxx
实际存储上他会把数据根据列来拆分成行进行存储，所以叫列式存储，像下面这样：
rowkey timestamp 列 值
order_1_110 t1 order:detail xxx
order_1_111 t2 order:base xxx
order_1_111 t3 order:extent xxx
简单的语法 进入bin下执行**./hbase shell**连接hbase。
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 # 查看命令使用 help 'list' # 创建工作空间 create_namespace 'stats' # 修改工作空间 alter_namespace 'stats', {METHOD=>'set', NAME => 'stats1'} # 修改工作空间 drop_namespace 'stats' # 创建一张test表，制定工作空间是stats，指定一个列族cf，创建表时必须要有列族，可以是多个列族create 'test','cf1','cf2' create 'stats:test','cf' # 使用list获取所有表，也可以指定表，比如list 'test' list # 或者使用exist 表名 exists 'test' # 禁用表，所有dml操作都需要先禁用表 disable 'test' # 启用表 enable 'test' # 表是否启用 is_enabled 'test' # 查看表的明细 describe 'test' # 添加列族 alter 'test', 'Extra' # 删除列族 alter 'test', {NAME=>'Extra', METHOD=>'delete'} # 删除表，这一步是基于disable之后的 drop 'test' # 插入或者更新一行数据，put 表名,rowkey,列族：列名,值…… put 'test','row1','cf1:a','1','cf2:b' # 指定返回行数 scan 'test', {LIMIT=>10} # 获取列族或者列数据 scan 'test', {COLUMNS=>['cf1:a', 'cf2:b']} # 根据rowkey前缀匹配 scan 'test', {STARTROW => 'row1'} # 获取表数据，例子是倒叙，条件可以不带 scan 'test',{REVERSED=>true,STARTROW=>'row100~',ENDROW=>'row100',LIMIT=>5} # 再来个正序 scan 'test',{STARTROW=>'row100',ENDROW=>'row100~',LIMIT=>5} # 还可以根据row key获取一条数据 get 'test' 'row1' # 删除数据，表名,rowkey,列族:列名，列族和列名都不是必须的，这样是删除最新的一个数据 # 把所有版本全部删除时deleteall delete 'test','row1','cf1:a' 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # 刷新memStore的阈值，默认是128M hbase.hregion.memstore.flush.size # 当所有memStore都超过了这么大的时候把memStore刷到hdfs中 hbase.regionserver.global.memstore.size.lower.limit # HLog数量上限 hbase.regionserver.maxlogs # 设置chunk大小，默认是2MB hbase.hregion.memstore.mslab.chunksize # 开启chunk pool，默认是0，可以设置0到1的数字 # 比如设置成0.3，就会把年轻代的大小 * 0.3分配给pool hbase.hregion.memstore.chunkpool.maxsize # pool中初始化多少个chunk hbase.hregion.memstore.chunkpool.initialsize # 每隔多久创建一个新的WAL日志文件，默认一小时 hbase.regionserver.logroll.period # 每隔多久删掉过期的old WAL文件，默认1分钟 hbase.master.cleaner.interval # old WAL文件过期时间，默认10分钟 hbase.master.logcleaner.ttl # 合并HFile的阈值 hbase.hstore.compactionThreshold # major compaction的周期（天），0是关闭 hbase.hregion.majorcompaction # 当未合并的文件数量低于这个阈值时，停止合并，默认是3 hbase.store.compaction.min # 合并文件的large线程池的线程数 hbase.regionserver.thread.compaction.large # 合并文件的small线程池的线程数 hbase.regionserver.thread.compaction.small # 合并的文件数超过这个值就会使用large线程池 hbase.regionserver.thread.compaction.throttle</p></div><footer class=entry-footer><span title='2021-06-06 06:27:35 +0000 UTC'>June 6, 2021</span>&nbsp;·&nbsp;2 min</footer><a class=entry-link aria-label="post link to 1.habse基础" href=https://wangxiaohong123.github.io/posts/%E5%9F%BA%E7%A1%80/%E6%95%B0%E6%8D%AE%E5%BA%93/hbase/01%E5%9F%BA%E7%A1%80/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>2.habse安装</h2></header><div class=entry-content><p></p></div><footer class=entry-footer><span title='2021-06-06 06:27:35 +0000 UTC'>June 6, 2021</span>&nbsp;·&nbsp;0 min</footer><a class=entry-link aria-label="post link to 2.habse安装" href=https://wangxiaohong123.github.io/posts/%E5%9F%BA%E7%A1%80/%E6%95%B0%E6%8D%AE%E5%BA%93/hbase/02%E5%AE%89%E8%A3%85/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>1.分库分表-sharding_sphere</h2></header><div class=entry-content><p>sharding-sphere仅仅是一个框架，业务引入依赖就可以使用了，通过一些配置，他就会根据指定sharding字段路由，执行SQL，[官网][https://shardingsphere.apache.org/index_zh.html]。现在sharding-sphere也有proxy server模式了，它还支持主键生成、事务(最终一致性、XA)、数据迁移、数据可视化链路追踪、扩容等，就是说基本上除了垮裤跨表的复杂查询，分库分表用到的功能他基本上都有了，最早是当当开源出来的。
mycat是proxy server模式，需要独立部署server端，然后在业务引入client依赖，所有SQL通过client发送到server</p></div><footer class=entry-footer>1 min</footer><a class=entry-link aria-label="post link to 1.分库分表-sharding_sphere" href=https://wangxiaohong123.github.io/posts/%E5%9F%BA%E7%A1%80/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8/2.sharding-sphere/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>1.分库分表-介绍</h2></header><div class=entry-content><p>1.怎么拆 分库分表之前要考虑怎么拆分，一般都是先垂直后水平，首先垂直拆分把表的列按模块尽可能分到不同的库中，这样可能会解决读写并发压力过大的问题和单条数据过大导致磁盘瓶颈问题，然后在考虑水平拆分。
然后在考虑用什么技术，一般就是sharding-sphere和mcat。
2.唯一id 使用leaf
3.多场景不同条件查询 比如电商的订单，用户可以查询自己的id获取订单记录，商家可以根据自己的id获取订单记录，这个时候是以用户id路由，保证同一个用户的订单分到了同一张表还是以商家id路由，如果不考虑存储空间就都做，这样数据量会变成原来的两倍，如果考虑存储空间就以经常访问的业务作为路有条件，其他的可以二次路由。
4.分布式事务 分库分表后由于规则不同，比如订单使用用户id作为路由条件，订单条目使用订单id作为路由条件，这样就会导导致跨库的事务，就需要使用XA事务。
5.查询条件中没有sharding字段 比如查询是一堆组合条件，可以用sharding-jdbc之类的中间件，他会去所有表中把符合条件的数据都merge到一台机器的内存中，这个很慢也很占带宽，可以把条件放到es中， 根据条件查询出sharding字段，在回表查询。分库分表之后使用es抗复杂查询是不可避免的。
6.读写分离 使用一主多从分摊大部分的查询请求，这样一主多从抗住80%的crud，es抗住20%的复杂查询。
7.物理架构的规划 一般表就是1024张，库的话需要根据写并发和数据大小做出规划，比如高峰时这个库有多少tps，未来是否会增长，然后每天增长多少数据，计算主库和从库的配置，分几台主库，es要几台服务器，几个分片，保证分库之后可以抗几年。
8.数据迁移 首先开发一个数据同步加检测的程序，同步就一直在同步，检测也是一直在检测，保证某一个时间点时之后旧库和新库的数据完全一致。 然后在开发新系统，数据的crud全部打到新数据库中。 进行双写，比如使用nginx把一个请求分发到两套系统里，停掉数据同步，只开启数据检测。 运行一段时间后如果检测没有问题修改nginx，把请求只打到新程序中，停掉旧程序完成数据迁移。 9.控制台 分库分表之前需要操作控制台实现大量的DDL操作，不能每个表拆成1024张都手动去操作吧，以后新业务需要新表新库的时候也不能手动操作吧，太没效率了，在分库分表之后还需要监控每个库表的性能、负载，还需要知道每张物理表的数据量，占用空间，1024张物理表组成的大逻辑表一共有多少数据等等。
10.扩容问题 之前创建的1024张表在扩容时可以直接移动表，不需要重新把路由数据，但是在迁移表的时候还要考虑迁移的时候数据写入问题、迁移之后数据库配置，这也是很麻烦的，可以编写一套脚本，然后在控制台控制执行，在来一个配置中心，也可以是控制台，修改数据库连接之类的。</p></div><footer class=entry-footer>1 min</footer><a class=entry-link aria-label="post link to 1.分库分表-介绍" href=https://wangxiaohong123.github.io/posts/%E5%9F%BA%E7%A1%80/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8/1.%E4%BB%8B%E7%BB%8D/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>1.概念</h2></header><div class=entry-content><p>什么是搜索 当我们在百度里搜索关键词的时候就是一个搜索的过程，但是搜索不光是百度，还有一些垂直搜索（站内搜索）比如淘宝里搜一下商品关键字，比如OA系统搜个打卡统计之类的。简单来说就是在任何场景下输入关键字会返回一些想要的信息。
倒排索引 就是把数据拆成词，每个词对应着一条数据的id之类的东西。 比如有3条数据：1 生化危机电影，2 生化危机海报， 3 生化危机新闻，这3条数据生成的倒排索引就类似下面这样：
关键词 ids 生化 1,2,3 危机 1,2,3 生化危机 1,2,3 海报 2 新闻 3 电影 1 如果不考虑搜索优化，假设我们使用es或者lucene进行所有的时候，倒排索引的结构也要比MySQL全表扫描的模糊查询要快很多，因为这里相当于对关键词进行equals比较，而模糊搜索要检查是否包含。
全文检索 全文检索包括把数据插入倒排索引，然后把搜索条件拆词，每个词都去倒排索引中查找数据。
Lucene 就是一个jar包，里面封装好了很多建立倒排索引和全文检索的算法。用lucene可以把现有的数据在磁盘上建立索引，并且Lucene会帮我们组织索引的数据结构，还可用Lucene提供的api对磁盘上的索引数据进行搜索。
elasticsearch Lucene可以实现全文检索功能，但是他有很大的弊端：比如它不支持分布式，api有点复杂。 当我们的数据量在一台机器上放不下的时候，每次查询需要我们自己去各个Lucene机器上搜索，插入数据时也要自己维护索引所在机器，当某一台机器宕机，那这台机器上的数据就全搜不出来了。
所以分布式的搜索引擎elasticsearch就诞生了，对比Lucene而言，他自动冗余数据备份，更高可用，分布式存储更多的数据，自动维护数据到多个节点的索引，还封装了更多高级的功能。
ES功能 数据分析：比如查看最近七天的牙膏的销量前十的商家，每个商品分类下有多少商品。 数据搜索：全文检索和结构化检索，结构化检索就是根据类型，比如搜索日用品类型的商品。 搜索推荐、自动补全这些。 他是一个分布式的海量数据的近实时搜索引擎。
核心概念 Near Realtime（NRT），近实时，有两个意思，第1个是从写入到可查询到是秒级的延迟，第2个是搜索和分析也是秒级的延迟。 Cluster和Node，集群和节点，集群和节点都可以设置名字的，集群的默认名字是elasticsearch，节点的默认名字随机分配，如果节点没有配置集群的名字就会默认加入到elasticsearch集群中。 Document：es里的最小数据单元，就是一条数据。 field：document中的数据字段。 Index：索引，一个index包含很多document，index中是所有类似或者相同的document。 type：index中的document的逻辑分类，比如说index中有10000个document，1到3000个document的field一样，3001到6000的filed一样，6001到10000的id一样，那么就可以定义index中有三个type，type更像是MySQL中的一样表，而index像是一类数据库，新版本中废弃了，因为在es中存储是不区分type的。 shard：一个index可以包含多个shard，这些shared会散落在多台服务器上，这样搜索可以在多个服务器上并行执行，提升吞吐量。扩容时新创建一个更多的shard的index，把数据重新导入进去就可以了，他也叫primary shard，可以接受读写请求。 replica：就是shard的副本，也叫replica shard，每个shard可以有多个replica。replica也可以提供读的请求。 默认情况下每个index有5个shard，每个shard有一个replic，es规定了shard和replica不能在同一节点上，也就是说要部署在两台机器上。使用replica的第一个好处就是分摊查询的压力，提高吞吐，第2个好处就是保证高可用，shard宕机时进行切换。
es是面向文档的数据格式。
锁 es有三个粒度的锁，全局锁（加在索引上）；document锁；读写锁（共享锁和排它锁）；</p></div><footer class=entry-footer>1 min</footer><a class=entry-link aria-label="post link to 1.概念" href=https://wangxiaohong123.github.io/posts/%E5%9F%BA%E7%A1%80/%E6%95%B0%E6%8D%AE%E5%BA%93/es/1.%E6%A6%82%E5%BF%B5/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>10.es-数据导入</h2></header><div class=entry-content><p>新建索引：
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 PUT /stars_web_search_user_conditions_1 { "settings": { "number_of_shards": 1, "number_of_replicas": 0, // 索引刷盘间隔，默认1s，导入数据要求不高，先设置120s "index.refresh_interval":"120s", // 是否同步到replica和同步提交translog // 如果是request(默认)同步复制到replica和同步刷盘 // 如果是async则每次在index.translog.sync_interval的时候同步和刷盘 "index.translog.durability":"async", // translog的刷盘间隔 "index.translog.sync_interval":"120s", // 刷盘的translog大小，这个和index.translog.sync_interval满足一个条件时就会刷盘(默认512mb) "index.translog.flush_threshold_size":"2048mb" }, "mappings": { "_source": {"enabled": false}, "properties": { "_all": {"enabled": false}, "userId": { "type": "long" }, "nickname": { "type": "text", "analyzer": "ik_max_word", "search_analyzer": "ik_smart" }, "mobile": { "type": "text", "analyzer": "whitespace" }, "gender": { "type": "keyword" }, "idealGender": { "type": "keyword" }, "registrationTime": { "type": "long" }, "lastImChangeTime": { "type": "long" }, "pornLevel": { "type": "keyword" }, "sendAndReceiveExploreMsgNum": { "type": "integer" }, "momentsNum": { "type": "integer" }, "lastLoginPlatform": { "type": "keyword" }, "vipStatus": { "type": "keyword" }, "adChannel": { "type": "keyword" }, "downloadChannel": { "type": "keyword" }, "adId": { "type": "keyword" }, "frozen": { "type": "boolean" } } } } 查看设置是否生效：
...</p></div><footer class=entry-footer>2 min</footer><a class=entry-link aria-label="post link to 10.es-数据导入" href=https://wangxiaohong123.github.io/posts/%E5%9F%BA%E7%A1%80/%E6%95%B0%E6%8D%AE%E5%BA%93/es/10.%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>2.安装es</h2></header><div class=entry-content><p>1准备工作 首先准备3台虚拟机，三台机器上进行相同的操作
1.1修改系统参数 修改资源配置文件：vim /etc/security/limits.conf：
1 2 3 4 5 6 7 # *表示任何用户 # soft表示警告值，hard是真正限制的阈值 # nofile是文件描述符数量，nproc是开启进程数 * soft nofile 65535 * hard nofile 65535 * soft nproc 4096 * hard nproc 4096 修改这个配置要退出当前用户重新登录生效。
修改sysctl配置文件：vim /etc/sysctl.conf修改每个进程可以拥有的虚拟内存区域的数量
1 vm.max_map_count=262144 执行sysctl -p刷新配置
1.2创建es用户 执行命令useradd es添加名为es的用户，执行命令passwd es 然后根据提示输入要设置密码比如qiyuan1502，再次输入确认密码完成设置。
1.3创建目录 1 2 3 mkdir -p /app/elasticsearch mkdir -p /app/elasticsearch/data mkdir -p /app/elasticsearch/log 2安装es 到官网[https://www.elastic.co/cn/downloads/past-releases#elasticsearch ]下载es安装包，上传到服务器下的/app/elasticsearch文件夹中解压：
...</p></div><footer class=entry-footer>6 min</footer><a class=entry-link aria-label="post link to 2.安装es" href=https://wangxiaohong123.github.io/posts/%E5%9F%BA%E7%A1%80/%E6%95%B0%E6%8D%AE%E5%BA%93/es/2.%E5%AE%89%E8%A3%85/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://wangxiaohong123.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/page/2/>Next&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2025 <a href=https://wangxiaohong123.github.io/>王小红的笔记</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>