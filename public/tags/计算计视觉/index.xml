<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>计算计视觉 on 王小红的笔记</title>
    <link>http://localhost:1313/tags/%E8%AE%A1%E7%AE%97%E8%AE%A1%E8%A7%86%E8%A7%89/</link>
    <description>Recent content in 计算计视觉 on 王小红的笔记</description>
    <generator>Hugo -- 0.150.0</generator>
    <language>zh-CN</language>
    <atom:link href="http://localhost:1313/tags/%E8%AE%A1%E7%AE%97%E8%AE%A1%E8%A7%86%E8%A7%89/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>6.计算机视觉(CV)介绍</title>
      <link>http://localhost:1313/posts/ai/6.%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%BB%8B%E7%BB%8D/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/ai/6.%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%BB%8B%E7%BB%8D/</guid>
      <description>&lt;p&gt;图像是由长度、宽度、颜色通道组成的矩阵，一般是3个通道RGB，每个通道有多个像素点，像素点的值是0到255，这个数值表示的是亮度，比如0就是最暗-黑色，255是最亮-白色。&lt;/p&gt;
&lt;p&gt;计算机视觉的挑战：部分遮掩(比如模型识别不出来被遮住头的猫)，背景混入(无法识别和背景颜色相似的目标)。&lt;/p&gt;
&lt;h3 id=&#34;一主要流程&#34;&gt;一、主要流程&lt;/h3&gt;
&lt;h4 id=&#34;1-数据采集与预处理data-acquisition--preprocessing&#34;&gt;1. 数据采集与预处理（Data Acquisition &amp;amp; Preprocessing）&lt;/h4&gt;
&lt;p&gt;获取高质量的视觉数据，并进行规范化处理。比如图像/视频采集、数据增强（如翻转、旋转、缩放）、数据标准化（如均值归一化、缩放到 [0,1]）、去噪、去模糊。常用工具OpenCV、Pillow（PIL）、Albumentations。&lt;/p&gt;
&lt;p&gt;常用的开源基准数据集有2种，VOC和COCO，里面包含了一些人、动物、物品等带b标注图片。&lt;/p&gt;
&lt;h4 id=&#34;2-特征提取featrue-extraction--backbone&#34;&gt;2. 特征提取（Featrue Extraction / Backbone）&lt;/h4&gt;
&lt;p&gt;从原始图像中提取有代表性的特征，经典网络VGG、ResNet、MobileNet、Darknet。输出的是高维的特征图。&lt;/p&gt;
&lt;h4 id=&#34;3-特征表示与学习featrue-representation--learning&#34;&gt;3. 特征表示与学习（Featrue Representation &amp;amp; Learning）&lt;/h4&gt;
&lt;p&gt;对特征进行降维或压缩、将特征输入到分类器或检测器中、学习特征与任务目标之间的映射关系。常见模型：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;分类任务：Softmax 分类器&lt;/li&gt;
&lt;li&gt;检测任务：RPN（Region Proposal Network）&lt;/li&gt;
&lt;li&gt;分割任务：上采样模块（如 U-Net 中的 Decoder）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;4-任务特定头task-specific-heads&#34;&gt;4. 任务特定头（Task-Specific Heads）&lt;/h4&gt;
&lt;p&gt;根据不同任务设计不同的网络头（Head）。常见任务头：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;图像分类：全连接层 + Softmax&lt;/li&gt;
&lt;li&gt;目标检测：边界框回归 + 分类头&lt;/li&gt;
&lt;li&gt;语义分割：像素级分类头&lt;/li&gt;
&lt;li&gt;实例分割：目标检测头 + 分割头（如 Mask R-CNN）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;5-推理与后处理inference--post-processing&#34;&gt;5. 推理与后处理（Inference &amp;amp; Post-processing）&lt;/h4&gt;
&lt;p&gt;将模型的输出转换为可解释的结果，常见后处理方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NMS（非极大值抑制）：去除冗余边界框&lt;/li&gt;
&lt;li&gt;Softmax：转换为类别概率&lt;/li&gt;
&lt;li&gt;阈值化：过滤低置信度的检测结果&lt;/li&gt;
&lt;li&gt;CRF（条件随机场）：优化分割边界&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;51-nms&#34;&gt;5.1 NMS&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;非极大值抑制（Non-Maximum Suppression, NMS）&lt;/strong&gt; 是一种常用于目标检测任务中的后处理技术，主要目的是从多个重叠的候选框（Bounding Boxes）中选择出最佳的框，减少冗余框，确保每个目标只保留一个检测框。流程：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;对所有候选框按&lt;strong&gt;置信度分数&lt;/strong&gt;进行降序排序。&lt;/li&gt;
&lt;li&gt;选择&lt;strong&gt;当前置信度最高的框&lt;/strong&gt;，将其加入保留框列表 &lt;code&gt;Keep&lt;/code&gt; 中。&lt;/li&gt;
&lt;li&gt;计算当前选择的框与&lt;strong&gt;所有剩余框&lt;/strong&gt;之间的IoU。&lt;/li&gt;
&lt;li&gt;对于与当前框 &lt;strong&gt;IoU 大于设定阈值&lt;/strong&gt;（例如 0.5）的其他框，将它们从候选框列表中&lt;strong&gt;移除&lt;/strong&gt;。因为这些高重叠的框很可能指向同一个目标，保留多个会导致冗余。&lt;/li&gt;
&lt;li&gt;从剩余的候选框中，&lt;strong&gt;选择下一个置信度最高的框&lt;/strong&gt;，重复步骤 &lt;strong&gt;2 → 3 → 4&lt;/strong&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;如果目标靠的太近可能造成误删&lt;/strong&gt;，改进版有Soft-NMS(调整置信度而不是直接丢弃框)和Multi-Class NMS(针对不同类别分别进行 NMS)。&lt;/p&gt;</description>
    </item>
    <item>
      <title>7.目标检测</title>
      <link>http://localhost:1313/posts/ai/7.%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/ai/7.%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/</guid>
      <description>&lt;p&gt;目标检测首先需要有一个类型的集合，只能检测集合中的类型，并且他会检测出图片中所有属于集合中的&lt;strong&gt;类型和位置&lt;/strong&gt;。坐标可以使用极坐标或者中心点坐标表示。&lt;/p&gt;
&lt;p&gt;模型的评价指标有2种：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;IoU(交并比)，检测结果和测试数据真实结果的交集/检测结果和真实结果的并集。&lt;/li&gt;
&lt;li&gt;mAP：IoU没有衡量检测类型准确性的指标，他是用精确率和召回率组成的PR图的面积表示。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;不管是单阶段提交算法还是两阶段提交算法都会产生多个候选框。&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&#34;2-两阶段提交算法&#34;&gt;2. 两阶段提交算法&lt;/h4&gt;
&lt;p&gt;相比于单阶段多了一个预选操作，速度慢，效果好，适合离线检测。&lt;/p&gt;
&lt;h5 id=&#34;21-overheat&#34;&gt;2.1 overheat&lt;/h5&gt;
&lt;p&gt;使用固定宽高的矩形区域滑动窗口，将窗口扫描的结果送到神经网络分类和回归。这种类似于穷举，会消耗大量算力，并且他的窗口大小固定，结果不是很准确。&lt;/p&gt;
&lt;h5 id=&#34;22-r-cnn&#34;&gt;2.2 R-CNN&lt;/h5&gt;
&lt;p&gt;引入&lt;strong&gt;区域建议&lt;/strong&gt;概念，将目标检测分为&lt;strong&gt;候选区域提取 → 特征提取 → 分类&lt;/strong&gt;三步：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;候选区域提取：使用Selective Search(SS)算法在图像上生成约 2000 个候选区域（Region Proposals）。&lt;/li&gt;
&lt;li&gt;特征提取：将每个候选区域送入 CNN（如 AlexNet）中提取特征。&lt;/li&gt;
&lt;li&gt;分类：将提取的特征送入**SVM（支持向量机）**进行分类。&lt;/li&gt;
&lt;li&gt;边界框回归：使用&lt;strong&gt;回归器&lt;/strong&gt;优化边界框的位置。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;他的分类是传统机器学习而不是神经网络。缺点：每个候选区域都要单独经过CNN处理，速度慢；很多候选区域重叠，特征提取存在重复计算。&lt;/p&gt;
&lt;h5 id=&#34;23-fast-r-cnn&#34;&gt;2.3 fast R-CNN&lt;/h5&gt;
&lt;p&gt;改进点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;使用全图卷积避免对每个候选框单独卷积，将候选区域映射到特征图上；&lt;/li&gt;
&lt;li&gt;通过 &lt;strong&gt;ROI Pooling&lt;/strong&gt; 将不同大小的候选区域转换为固定大小；&lt;/li&gt;
&lt;li&gt;使用&lt;strong&gt;全连接层&lt;/strong&gt;同时进行&lt;strong&gt;分类&lt;/strong&gt;和&lt;strong&gt;边界框回归&lt;/strong&gt;；&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;24-faster-r-cnn&#34;&gt;2.4 faster R-CNN&lt;/h5&gt;
&lt;p&gt;改进点：使用RPN代替SS生成候选框。他是一个端到端的寻量模型，我们只需要&lt;strong&gt;提供训练数据和对应的标签，调用预定义好的训练脚本即可，不关心模型的细节（如学习率、优化器、损失函数、中间特征层的设计等）。&lt;/strong&gt;&lt;/p&gt;
&lt;h5 id=&#34;25-mask-rcnn&#34;&gt;2.5 Mask-Rcnn&lt;/h5&gt;
&lt;h4 id=&#34;3-单阶段提交算法&#34;&gt;3. 单阶段提交算法&lt;/h4&gt;
&lt;p&gt;单阶段检测的速度快，精度低，适合实时检测，比如自动驾驶、无人机目标检测等。YOLO和SSD都是端到端的模型。&lt;/p&gt;
&lt;h5 id=&#34;31-yolo&#34;&gt;3.1 YOLO&lt;/h5&gt;
&lt;p&gt;将输入的图片分成n个小格子，每个格子作为中心点，并且给2个尺寸的候选框，然后计算2个候选框的IOU和置信度，首先抛弃置信度过低的候选框，如果候选框没有被抛弃就选择IOU大的候选框进行微调，微调就是调整长宽，让他能够圈住检测的物体。微调的过程就是回归，所以YOLO的核心思想就是计算IOU+回归任务。如果最后的结果中1个点有很多预选框重叠了就采用NMS保留置信度最高的。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;V1版本：有很多缺点，因为全连接层要求输入的特征树是固定的，所有V1版本的YOLO要求输入的图像大小是固定的；并且多个物体重叠或者物体过小时很难检测；&lt;/li&gt;
&lt;li&gt;V2版本：舍弃Dropout，每层卷积后增加了Batch Normalization。因为V1使用224 * 224的分辨率进行训练，比较小，V2版本在训练完之后使用448 * 448的图片进行10次微调。V2中先验框的大小使用聚类算法确定，而不是写死的，数量也从2个变成5个。&lt;/li&gt;
&lt;li&gt;V3版本：改进了网络结构，更适合小目标检测，去掉了池化层，通过残差网络实现3种scale，分别对应大、中、小3种目标。&lt;/li&gt;
&lt;li&gt;V4版本：训练数据上增加了一些随机处理，比如翻转，遮盖，马赛克，DropBlock等；使用标签平滑方式(类别不设置成1，比如虽然是狗但是也要设置成0.95的狗+0.05的猫)防止过拟合；使用CIOU代替IOU；增加注意力机制、特征金字塔等；&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;32-ssd&#34;&gt;3.2 SSD&lt;/h5&gt;
&lt;p&gt;使用多尺度特征图进行预测，从不同尺度的特征图中检测不同大小的物体。在多个卷积层上进行预测，每层生成一组边界框和类别置信度。精度比YOLO高。&lt;/p&gt;
&lt;p&gt;YOLO适合实时检测小物体，更适合视频分析、图像分类。&lt;/p&gt;
&lt;h5 id=&#34;33-detr&#34;&gt;3.3 DETR&lt;/h5&gt;
&lt;p&gt;基于transformer的算法，没有NMS，没有预选框。但是训练很慢，而且对输入的特征有限制，不能很大。&lt;/p&gt;
&lt;h5 id=&#34;34-deformabledert&#34;&gt;3.4 DeformableDert&lt;/h5&gt;
&lt;p&gt;解决了DETR的输入特征限制和训练速度问题。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
