<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>5.神经网络 | 王小红的笔记</title>
<meta name="keywords" content="深度学习">
<meta name="description" content="所有的神经网络都是为了提取特征。
神经网络就是把多个线性回归和逻辑回归组合到一起。由一个输入层、一个输出层和n个隐藏层组成，第n层的每个神经元和第n-1层的所有神经元相连，第n-1层的输出就是第n层的输入。每个隐藏层有n个神经元，每个神经元都有一个非线性激活函数。
神经网络的精度高，效果好；但是训练时间长，小数据集上表现不好。他的本质是摆脱人为干预的特征提取(黑盒)，我们只负责输入，查看输出，这就是端对端模型(end-to-end)，深度学习中只有极小值没有极大值。

正向传播：从输入层--&gt;隐藏层--&gt;输出层的过程，是根据数据损失函数和激活函数得到模型的输出结果和损失函数的值。
反向传播：从损失函数开始，逐层向后传播误差(链式法则)，并计算每个参数的梯度的过程，然后用优化器更新权重和偏置，让损失主键减小。
计算图：描述张量之间运算关系的有向无环图，节点是变量（输入、权重、偏置、激活函数、输出），边是数学运算（加法、乘法、激活函数等）。为了高效计算梯度的。

一、损失函数
也叫代价函数、目标函数、误差函数。默认作用在监督学习的输出层，但是在多任务训练、对抗训练等，损失函数也可以作用在隐藏层或中间层。
损失函数 = 数据损失 &#43; 正则化惩罚，数据损失就是预测值和真实值的差异。损失函数中一般会带着偏执(常量)，偏置跟输出挂钩，一般几分类就会有几个偏置。
log函数可以表示预测值为1的时候损失为0，同事预测值越不准损失越大，所以一般数据损失函数都用log表示。
GPT-4的损失函数中有1.8万亿个权重参数，涉及到生活的很多方面。
1. 数据损失函数
1.1 分类任务损失函数
1.1.1 交叉熵损失函数
多分类的交叉熵：多分类的输出层使用softmax函数，损失公式：$ L = -\frac{1}{N} \sum_{i=1}^{N} \sum_{k=1}^{K} y_{i,k} \cdot \log(\hat{y}{i,k})$，N是样本数量；K是类别数量；$y{i,k}$ 是第 i 个样本在第 k 个类别上的真实标签（1 表示属于该类别，0 表示不属于）； $\hat{y}_{i,k}$ 是第 i 个样本在第 k 个类别上的预测概率，就是模型输出的预测结果。
二分类的交叉熵：二分类的输出层使用sigmoid函数，损失公式：$L = -\frac{1}{N} \sum_{i=1}^{N} \Big(y_i \cdot \log(\hat{y}_i) &#43; (1 - y_i) \cdot \log(1 - \hat{y}_i)\Big)$，参数和多酚类一样。sigmoid函数的趋势是近似平行x轴的，会导致梯度消失。
1.2 回归任务损失函数
1.2.1 MAE损失
也叫L1 Loss，以绝对误差作为距离。具有稀疏性，也可以作为正则化添加到其他loss中作为约束。他的梯度在0点不平滑，容易跳过最小值。公式：$L = \frac{1}{N} \sum_{i=1}^{N} \left| y_i - \hat{y}_i \right|$">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/posts/ai/5.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.08f7d74f0ada0f975d29ae436285b61ed7a719d05f350cb888d00341642995a2.css" integrity="sha256-CPfXTwraD5ddKa5DYoW2HtenGdBfNQy4iNADQWQplaI=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/ai/5.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="/katex/katex.min.css">
<script defer src="/katex/katex.min.js"></script>
<script defer src="/katex/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: "$$", right: "$$", display: true},
        {left: "$", right: "$", display: false},
        {left: "\\(", right: "\\)", display: false},
        {left: "\\[", right: "\\]", display: true}
      ]
    });
  });
</script>

</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="王小红的笔记 (Alt + H)">王小红的笔记</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/posts/" title="笔记">
                    <span>笔记</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/categories/" title="分类">
                    <span>分类</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="标签">
                    <span>标签</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      5.神经网络
    </h1>
    <div class="post-meta">3 min

</div>
  </header> 
  <div class="post-content"><p>所有的神经网络都是为了提取特征。</p>
<p>神经网络就是把多个线性回归和逻辑回归组合到一起。由一个输入层、一个输出层和n个隐藏层组成，第n层的每个神经元和第n-1层的所有神经元相连，第n-1层的输出就是第n层的输入。每个隐藏层有n个神经元，每个神经元都有一个非线性激活函数。</p>
<p>神经网络的精度高，效果好；但是训练时间长，小数据集上表现不好。他的本质是摆脱人为干预的特征提取(<strong>黑盒</strong>)，我们只负责输入，查看输出，这就是端对端模型(end-to-end)，深度学习中只有极小值没有极大值。</p>
<ul>
<li>正向传播：从输入层--&gt;隐藏层--&gt;输出层的过程，是根据数据损失函数和激活函数<strong>得到模型的输出结果和损失函数的值</strong>。</li>
<li>反向传播：从损失函数开始，逐层向后传播误差(链式法则)，并计算每个参数的梯度的过程，然后用优化器更新权重和偏置，让损失主键减小。</li>
<li>计算图：描述张量之间运算关系的有向无环图，节点是变量（输入、权重、偏置、激活函数、输出），边是数学运算（加法、乘法、激活函数等）。为了高效计算梯度的。</li>
</ul>
<h3 id="一损失函数">一、损失函数<a hidden class="anchor" aria-hidden="true" href="#一损失函数">#</a></h3>
<p>也叫代价函数、目标函数、误差函数。默认作用在监督学习的输出层，但是在多任务训练、对抗训练等，损失函数也可以作用在隐藏层或中间层。</p>
<p>损失函数 = 数据损失 + 正则化惩罚，数据损失就是预测值和真实值的差异。损失函数中一般会带着偏执(常量)，偏置跟输出挂钩，一般几分类就会有几个偏置。</p>
<p>log函数可以表示预测值为1的时候损失为0，同事预测值越不准损失越大，所以一般数据损失函数都用log表示。</p>
<p>GPT-4的损失函数中有1.8万亿个权重参数，涉及到生活的很多方面。</p>
<h4 id="1-数据损失函数">1. 数据损失函数<a hidden class="anchor" aria-hidden="true" href="#1-数据损失函数">#</a></h4>
<h5 id="11-分类任务损失函数">1.1 分类任务损失函数<a hidden class="anchor" aria-hidden="true" href="#11-分类任务损失函数">#</a></h5>
<h6 id="111-交叉熵损失函数">1.1.1 交叉熵损失函数<a hidden class="anchor" aria-hidden="true" href="#111-交叉熵损失函数">#</a></h6>
<p>多分类的交叉熵：多分类的输出层使用softmax函数，损失公式：$ L = -\frac{1}{N} \sum_{i=1}^{N} \sum_{k=1}^{K} y_{i,k} \cdot \log(\hat{y}<em>{i,k})$，N是样本数量；K是类别数量；$y</em>{i,k}$ 是第 i 个样本在第 k 个类别上的真实标签（1 表示属于该类别，0 表示不属于）； $\hat{y}_{i,k}$ 是第 i 个样本在第 k 个类别上的预测概率，就是模型输出的预测结果。</p>
<p>二分类的交叉熵：二分类的输出层使用sigmoid函数，损失公式：$L = -\frac{1}{N} \sum_{i=1}^{N} \Big(y_i \cdot \log(\hat{y}_i) + (1 - y_i) \cdot \log(1 - \hat{y}_i)\Big)$，参数和多酚类一样。<strong>sigmoid函数的趋势是近似平行x轴的，会导致梯度消失</strong>。</p>
<h5 id="12-回归任务损失函数">1.2 回归任务损失函数<a hidden class="anchor" aria-hidden="true" href="#12-回归任务损失函数">#</a></h5>
<h6 id="121-mae损失">1.2.1 MAE损失<a hidden class="anchor" aria-hidden="true" href="#121-mae损失">#</a></h6>
<p>也叫L1 Loss，以绝对误差作为距离。具有稀疏性，也可以作为正则化添加到其他loss中作为约束。他的梯度在0点不平滑，容易跳过最小值。公式：$L = \frac{1}{N} \sum_{i=1}^{N} \left| y_i - \hat{y}_i \right|$</p>
<h6 id="122-mse损失">1.2.2 MSE损失<a hidden class="anchor" aria-hidden="true" href="#122-mse损失">#</a></h6>
<p>求误差的平方和，就是欧式距离，也叫L2 Loss，也可以作为正则项。公式：$L = \frac{1}{N} \sum_{i=1}^{N} \left( y_i - \hat{y}_i \right)^2$。容易造成梯度爆炸。</p>
<h6 id="123-smooth-l1">1.2.3 smooth L1<a hidden class="anchor" aria-hidden="true" href="#123-smooth-l1">#</a></h6>
<p>平滑L1损失函数，他是一个分段函数，为了解决L1的不平滑和L2的梯度爆炸问题。公式：
$$
L = \frac{1}{N} \sum_{i=1}^{N} \text{Smooth}_{L1}(y_i - \hat{y}<em>i)\
\
其中：\
\
\text{Smooth}</em>{L1}(x) =
\begin{cases}
0.5 x^2, &amp; \text{if } |x| &lt; 1 \
|x| - 0.5, &amp; \text{otherwise}
\end{cases}
$$</p>
<h4 id="2-正则化">2 正则化<a hidden class="anchor" aria-hidden="true" href="#2-正则化">#</a></h4>
<h5 id="1-l1-l2正则化">1. L1 L2正则化<a hidden class="anchor" aria-hidden="true" href="#1-l1-l2正则化">#</a></h5>
<p>通过在损失函数中增加正则项减少过拟合，L1让某些系数变成0，L2让某些系数趋于0.</p>
<h5 id="2-dropout">2. Dropout<a hidden class="anchor" aria-hidden="true" href="#2-dropout">#</a></h5>
<p>每次迭代的时候随机选择神经元失活。这么做是为了解决特征过多导致的过拟合问题。属于七伤拳。</p>
<h5 id="3-提前停止">3. 提前停止<a hidden class="anchor" aria-hidden="true" href="#3-提前停止">#</a></h5>
<p>将一部分训练集作为验证集，当训练的时候发现验证集的性能越来越差或者不在提升的时候停止训练。因为这个时候继续训练有可能过拟合。</p>
<h3 id="二优化方法">二、优化方法<a hidden class="anchor" aria-hidden="true" href="#二优化方法">#</a></h3>
<h4 id="1-梯度下降">1. 梯度下降<a hidden class="anchor" aria-hidden="true" href="#1-梯度下降">#</a></h4>
<p>梯度下降是为了寻找损失函数最小化的系数的算法。分为3类：</p>
<table>
  <thead>
      <tr>
          <th>算法</th>
          <th>描述</th>
          <th>优点</th>
          <th>缺点</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>BGD(批量梯度下降)</td>
          <td>每次迭代计算每个样本上的梯度并求和</td>
          <td>全剧最优</td>
          <td>计算量大，速度慢</td>
      </tr>
      <tr>
          <td>SGD(随机梯度下降)</td>
          <td>每次迭代只采集一个样本</td>
          <td>速度快，可以在线学习</td>
          <td>准确度下降，存在噪音</td>
      </tr>
      <tr>
          <td>MBGD(小批量梯度下降)</td>
          <td>每次随机选一小部分样本的梯度并更新参数</td>
          <td>同SGD</td>
          <td>同SGD</td>
      </tr>
  </tbody>
</table>
<p>梯度下降可能出现的问题：</p>
<ol>
<li>平坦区下降速度慢；</li>
<li>鞍点：存在导数为0，但不是极值点；</li>
<li>局部极小值；</li>
</ol>
<p>动量梯度下降：计算梯度的指数加权平均数，利用这个值更新参数，可以缓解鞍点问题。</p>
<p>AdaGrad：使用一个小批量随机梯度g_t按元素平方累加st对学习率修正。</p>
<p>RmsProp：AdaGrad后期学习率较小，很难找到最优解，RmsProp是对AdaGrad的改进。</p>
<p>Adam：动量梯度下降和RmsProp的结合。</p>
<h3 id="三rnn模型">三、RNN模型<a hidden class="anchor" aria-hidden="true" href="#三rnn模型">#</a></h3>
<p>循环神经网络是最早应用到NLP的神经网络，训练时首先将文本进行分词，每个单词会分别输入到模型中，同时本次单词的输出会作为这条文本之后的单词的训练的特征。</p>
<h4 id="1-传统rnn">1 传统RNN<a hidden class="anchor" aria-hidden="true" href="#1-传统rnn">#</a></h4>
<p>结构简单，<strong>短句子的训练性能和效果不错</strong>，句子过长时会出现特征消失。他是串联结构，不能叠太多层，性能很差，而且他单向训练只能记住历史，不能推理。</p>
<p><img alt="RNN" loading="lazy" src="https://raw.githubusercontent.com/wangxiaohong123/p-bed/main/uPic/RNN.png"></p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">import</span> torch.nn <span style="color:#ff79c6">as</span> nn
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 实例化rnn对象，输入张量中特征维度是5，隐藏层张量中特征维度是6(隐藏层中神经元的个数)，隐藏层个数是1</span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 默认的激活函数时tanh</span>
</span></span><span style="display:flex;"><span>rnn <span style="color:#ff79c6">=</span> nn<span style="color:#ff79c6">.</span>RNN(<span style="color:#bd93f9">5</span>, <span style="color:#bd93f9">6</span>, <span style="color:#bd93f9">1</span>)
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 设定输入的张量，输入序列的长度是1，批次的样本数是3，输入张量的维度是5</span>
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">input</span> <span style="color:#ff79c6">=</span> torch<span style="color:#ff79c6">.</span>randn(<span style="color:#bd93f9">1</span>, <span style="color:#bd93f9">3</span>, <span style="color:#bd93f9">5</span>)
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 1是隐藏层个数 * 网络方向(单向是1，双向是2),批次的样本数是3，隐藏层中神经元个数是6</span>
</span></span><span style="display:flex;"><span>h0 <span style="color:#ff79c6">=</span> torch<span style="color:#ff79c6">.</span>randn(<span style="color:#bd93f9">1</span>, <span style="color:#bd93f9">3</span>, <span style="color:#bd93f9">6</span>)
</span></span><span style="display:flex;"><span>output, hidden <span style="color:#ff79c6">=</span> rnn(<span style="color:#8be9fd;font-style:italic">input</span>, h0)
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="2-lstm">2 LSTM<a hidden class="anchor" aria-hidden="true" href="#2-lstm">#</a></h4>
<p>长短时记忆网络，由4部分组成，训练效率比RNN低，可以缓解长句子的梯度消失和梯度爆炸问题。</p>
<ol>
<li>遗忘门：决定当前时刻要丢弃多少上一时刻的记忆：$ f_t = σ(W_f⋅[h_{t−1},x_t]+b_f) $。</li>
<li>输入门：决定当前时刻有多少新的信息需要存储到记忆单元中：$ i_t=σ(W_i⋅[h{t−1},x_t]+b_i) $。</li>
<li>候选记忆单元：生成新的记忆内容：$ \tilde {C_t}=tanh(W_c⋅[h_{t−1},x_t]+b_c) $。</li>
<li>输出门：决定当前时刻要输出多少记忆单元的内容：$ o_t=σ(W_o⋅[h{t−1},x_t]+b_o) $。</li>
<li>记忆单元状态更新：结合遗忘门和输入门，对记忆单元进行更新：$ C_t=f_t⋅C_{t−1}+i_t⋅\tilde C_t~ $。</li>
<li>隐藏状态：使用输出门控制，决定输出哪些信息:$ h_t=o_t⋅tanh(C_t) $。</li>
</ol>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">import</span> torch.nn <span style="color:#ff79c6">as</span> nn
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 输入张量维度：5，隐藏层神经元数量：6，隐藏层数：2</span>
</span></span><span style="display:flex;"><span>lstm <span style="color:#ff79c6">=</span> nn<span style="color:#ff79c6">.</span>LSTM(<span style="color:#bd93f9">5</span>, <span style="color:#bd93f9">6</span>, <span style="color:#bd93f9">2</span>)
</span></span><span style="display:flex;"><span>input1 <span style="color:#ff79c6">=</span> torch<span style="color:#ff79c6">.</span>randn(<span style="color:#bd93f9">1</span>, <span style="color:#bd93f9">3</span>, <span style="color:#bd93f9">5</span>)
</span></span><span style="display:flex;"><span>h0 <span style="color:#ff79c6">=</span> torch<span style="color:#ff79c6">.</span>randn(<span style="color:#bd93f9">2</span>, <span style="color:#bd93f9">3</span>, <span style="color:#bd93f9">6</span>)
</span></span><span style="display:flex;"><span>c0 <span style="color:#ff79c6">=</span> torch<span style="color:#ff79c6">.</span>randn(<span style="color:#bd93f9">2</span>, <span style="color:#bd93f9">3</span>, <span style="color:#bd93f9">6</span>)
</span></span><span style="display:flex;"><span>output, (hn, cn) <span style="color:#ff79c6">=</span> lstm(input1, (h0, c0))
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">print</span>(output<span style="color:#ff79c6">.</span>shape)
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="3-gru">3 GRU<a hidden class="anchor" aria-hidden="true" href="#3-gru">#</a></h4>
<p>门控循环单元，也可以缓解长句子的梯度消失和梯度爆炸问题，同时他的结构和计算要比LSTM简单。主要由两部分组成：</p>
<ol>
<li>更新门：</li>
<li>重置门：</li>
</ol>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">9
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">import</span> torch.nn <span style="color:#ff79c6">as</span> nn
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 输入张量维度：5，隐藏层神经元数量：6，隐藏层数：2</span>
</span></span><span style="display:flex;"><span>gru <span style="color:#ff79c6">=</span> nn<span style="color:#ff79c6">.</span>GRU(<span style="color:#bd93f9">5</span>, <span style="color:#bd93f9">6</span>, <span style="color:#bd93f9">2</span>)
</span></span><span style="display:flex;"><span>input1 <span style="color:#ff79c6">=</span> torch<span style="color:#ff79c6">.</span>randn(<span style="color:#bd93f9">1</span>, <span style="color:#bd93f9">3</span>, <span style="color:#bd93f9">5</span>)
</span></span><span style="display:flex;"><span>h0 <span style="color:#ff79c6">=</span> torch<span style="color:#ff79c6">.</span>randn(<span style="color:#bd93f9">2</span>, <span style="color:#bd93f9">3</span>, <span style="color:#bd93f9">6</span>)
</span></span><span style="display:flex;"><span>output, hn <span style="color:#ff79c6">=</span> gru(input1, h0)
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">print</span>(output<span style="color:#ff79c6">.</span>shape)
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="四卷积神经网络cnn">四、卷积神经网络(CNN)<a hidden class="anchor" aria-hidden="true" href="#四卷积神经网络cnn">#</a></h3>
<p>CNN网络就是全连接神经网络前增加了2层：卷积层(提取图像特征)、池化层(降维，防止过拟合)。</p>
<p>跟图片、视频相关的都可以使用卷积网络实现，比如图片分类、图片分割、姿态估计、追踪任务、BEV等。</p>
<p>传统神经网络不适合处理图像，因为一个300 * 300 *3的图像就有27w的像素了，在扔到128个神经元的隐藏层中就有27w*128的矩阵的输入参数，这还是1个隐藏层，更多的隐藏层，更多的神经元就会导致巨量的计算。</p>
<h4 id="1-卷积层">1 卷积层<a hidden class="anchor" aria-hidden="true" href="#1-卷积层">#</a></h4>
<p>卷积运算是在<strong>滤波器和输入数据的局部区域做内积</strong>，内积是对应位置相乘再求和，卷积之后得到的是内积组成的特征图，内积越大就表示特征越大，越相关，内积为0表示不相关。英伟达对3*3的卷积核支持最好，1个通道的卷积运算示例：</p>
<p><img alt="卷积运算" loading="lazy" src="https://raw.githubusercontent.com/wangxiaohong123/p-bed/main/uPic/%E5%8D%B7%E7%A7%AF%E8%BF%90%E7%AE%97.png"></p>
<p>左上角点位计算方位为(<strong>内积</strong>)：1 * 1 + 1 * 0 + 1 * 1 + 0 * 0 + 1 * 1 + 1 * 0 + 0 * 1 + 0 * 0 + 1 * 1 + 偏置 = 4 + 偏置。</p>
<p>卷积运算对外圈是不公平的，因为外圈参与运算的次数最少，解决这个问题需要对原图进行padding，就是在原图外侧扩充，扩充出来的数据都是0。</p>
<p>上图中计算特征的步长是1，每次卷积核只移动1个格子，步长越大得到的特征越小，可以起到降低维度的作用，下面是步长为2的卷积云运算：</p>
<p><img alt="卷积运算-步长为2" loading="lazy" src="https://raw.githubusercontent.com/wangxiaohong123/p-bed/main/uPic/%E5%8D%B7%E7%A7%AF%E8%BF%90%E7%AE%97-%E6%AD%A5%E9%95%BF%E4%B8%BA2.png"></p>
<p>$ 卷积之后的宽 = \frac{原宽 - 卷积核宽 + 2padding} {步长} + 1 $，$ 卷积之后的长 = \frac{原长 - 卷积核长 + 2padding} {步长} + 1 $​。</p>
<p>可以看到卷积之后图像的像素变小，第一可以减少之后的全连接计算次数，第二卷积的过程中权重参数固定，都是卷积核的矩阵。所以卷积网络相比传统神经网络计算更少。</p>
<p>为什么需要做多次卷积？随着层数增加卷积核是越来越大的，层次越深观察的特征的范围越大。</p>
<h4 id="2-池化层下采样">2 池化层(下采样)<a hidden class="anchor" aria-hidden="true" href="#2-池化层下采样">#</a></h4>
<p>池化层为了筛选出一个区域里最重要的特征，可以进一步减少特征数。比如特征图的尺寸是4 * 4，池化之后的结果要求是2 * 2，那么就要把特征图分成2 * 2份，每份里面有很多小格：</p>
<p><img alt="特征图" loading="lazy" src="https://raw.githubusercontent.com/wangxiaohong123/p-bed/main/uPic/%E7%89%B9%E5%BE%81%E5%9B%BE.png"></p>
<p>池化层有2种，最大池化和平均池化。最大池化就是取最大值作为池化后的特征，平均池化就是计算平均值：
<img alt="特征图池化" loading="lazy" src="https://raw.githubusercontent.com/wangxiaohong123/p-bed/main/uPic/%E7%89%B9%E5%BE%81%E5%9B%BE%E6%B1%A0%E5%8C%96.png"></p>
<p>然后将特征图变成1维数据就可以发送到全连接的神经网络中了。</p>
<h4 id="3-demo">3 demo<a hidden class="anchor" aria-hidden="true" href="#3-demo">#</a></h4>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">36
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">37
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">38
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">39
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">40
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">import</span> tensorflow <span style="color:#ff79c6">as</span> tf
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 加载数据</span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># mnist是基准数据集，包含了28x28像素的灰度图像，代表从0到9的手写数字</span>
</span></span><span style="display:flex;"><span>(train_images, train_labels), (test_images, test_labels) <span style="color:#ff79c6">=</span> tf<span style="color:#ff79c6">.</span>keras<span style="color:#ff79c6">.</span>datasets<span style="color:#ff79c6">.</span>mnist<span style="color:#ff79c6">.</span>load_data()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 数据基础处理-将训练集变成：图像个数、宽、高、通道数</span>
</span></span><span style="display:flex;"><span>train_images <span style="color:#ff79c6">=</span> tf<span style="color:#ff79c6">.</span>reshape(train_images, [train_images<span style="color:#ff79c6">.</span>shape[<span style="color:#bd93f9">0</span>], train_images<span style="color:#ff79c6">.</span>shape[<span style="color:#bd93f9">1</span>], train_images<span style="color:#ff79c6">.</span>shape[<span style="color:#bd93f9">2</span>], <span style="color:#bd93f9">1</span>])
</span></span><span style="display:flex;"><span>test_images <span style="color:#ff79c6">=</span> tf<span style="color:#ff79c6">.</span>reshape(test_images, [test_images<span style="color:#ff79c6">.</span>shape[<span style="color:#bd93f9">0</span>], test_images<span style="color:#ff79c6">.</span>shape[<span style="color:#bd93f9">1</span>], test_images<span style="color:#ff79c6">.</span>shape[<span style="color:#bd93f9">2</span>], <span style="color:#bd93f9">1</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 模型构建</span>
</span></span><span style="display:flex;"><span>net <span style="color:#ff79c6">=</span> tf<span style="color:#ff79c6">.</span>keras<span style="color:#ff79c6">.</span>models<span style="color:#ff79c6">.</span>Sequential([
</span></span><span style="display:flex;"><span>    <span style="color:#6272a4"># 卷积层： 6个5 * 5的卷积核，激活函数是sigmoid</span>
</span></span><span style="display:flex;"><span>    tf<span style="color:#ff79c6">.</span>keras<span style="color:#ff79c6">.</span>layers<span style="color:#ff79c6">.</span>Conv2D(filters<span style="color:#ff79c6">=</span><span style="color:#bd93f9">6</span>, kernel_size<span style="color:#ff79c6">=</span><span style="color:#bd93f9">5</span>, activation<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#39;sigmoid&#39;</span>, input_shape<span style="color:#ff79c6">=</span>(<span style="color:#bd93f9">28</span>, <span style="color:#bd93f9">28</span>, <span style="color:#bd93f9">1</span>)),
</span></span><span style="display:flex;"><span>    <span style="color:#6272a4"># 最大池化</span>
</span></span><span style="display:flex;"><span>    tf<span style="color:#ff79c6">.</span>keras<span style="color:#ff79c6">.</span>layers<span style="color:#ff79c6">.</span>MaxPooling2D(pool_size<span style="color:#ff79c6">=</span>(<span style="color:#bd93f9">2</span>, <span style="color:#bd93f9">2</span>)),
</span></span><span style="display:flex;"><span>    <span style="color:#6272a4"># 卷积层： 16个5 * 5的卷积核，激活函数是sigmoid</span>
</span></span><span style="display:flex;"><span>    tf<span style="color:#ff79c6">.</span>keras<span style="color:#ff79c6">.</span>layers<span style="color:#ff79c6">.</span>Conv2D(filters<span style="color:#ff79c6">=</span><span style="color:#bd93f9">16</span>, kernel_size<span style="color:#ff79c6">=</span><span style="color:#bd93f9">5</span>, activation<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#39;sigmoid&#39;</span>),
</span></span><span style="display:flex;"><span>    <span style="color:#6272a4"># 最大池化</span>
</span></span><span style="display:flex;"><span>    tf<span style="color:#ff79c6">.</span>keras<span style="color:#ff79c6">.</span>layers<span style="color:#ff79c6">.</span>MaxPooling2D(pool_size<span style="color:#ff79c6">=</span>(<span style="color:#bd93f9">2</span>, <span style="color:#bd93f9">2</span>)),
</span></span><span style="display:flex;"><span>    <span style="color:#6272a4"># 调整成1维数据</span>
</span></span><span style="display:flex;"><span>    tf<span style="color:#ff79c6">.</span>keras<span style="color:#ff79c6">.</span>layers<span style="color:#ff79c6">.</span>Flatten(),
</span></span><span style="display:flex;"><span>    <span style="color:#6272a4"># 全连接神经网络-隐藏层1</span>
</span></span><span style="display:flex;"><span>    tf<span style="color:#ff79c6">.</span>keras<span style="color:#ff79c6">.</span>layers<span style="color:#ff79c6">.</span>Dense(<span style="color:#bd93f9">120</span>, activation<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;sigmoid&#34;</span>),
</span></span><span style="display:flex;"><span>    <span style="color:#6272a4"># 全连接神经网络-隐藏层2</span>
</span></span><span style="display:flex;"><span>    tf<span style="color:#ff79c6">.</span>keras<span style="color:#ff79c6">.</span>layers<span style="color:#ff79c6">.</span>Dense(<span style="color:#bd93f9">84</span>, activation<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;sigmoid&#34;</span>),
</span></span><span style="display:flex;"><span>    <span style="color:#6272a4"># 全连接神经网络-输出层</span>
</span></span><span style="display:flex;"><span>    tf<span style="color:#ff79c6">.</span>keras<span style="color:#ff79c6">.</span>layers<span style="color:#ff79c6">.</span>Dense(<span style="color:#bd93f9">10</span>, activation<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;softmax&#34;</span>)
</span></span><span style="display:flex;"><span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 模型编译：定义优化器、损失函数、评价指标</span>
</span></span><span style="display:flex;"><span>net<span style="color:#ff79c6">.</span>compile(tf<span style="color:#ff79c6">.</span>keras<span style="color:#ff79c6">.</span>optimizers<span style="color:#ff79c6">.</span>SGD(learning_rate<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0.9</span>), loss<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#39;sparse_categorical_crossentropy&#39;</span>, metrics<span style="color:#ff79c6">=</span>[<span style="color:#f1fa8c">&#39;accuracy&#39;</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 模型训练</span>
</span></span><span style="display:flex;"><span>net<span style="color:#ff79c6">.</span>fit(train_images, train_labels, epochs<span style="color:#ff79c6">=</span><span style="color:#bd93f9">5</span>, validation_split<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0.1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 模型评估</span>
</span></span><span style="display:flex;"><span>score <span style="color:#ff79c6">=</span> net<span style="color:#ff79c6">.</span>evaluate(test_images, test_labels, verbose<span style="color:#ff79c6">=</span><span style="color:#bd93f9">1</span>)
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">&#34;评估分数:&#34;</span>, score[<span style="color:#bd93f9">1</span>])
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="四transformer">四、Transformer<a hidden class="anchor" aria-hidden="true" href="#四transformer">#</a></h3>
<p>2017年出现了一篇论文叫attention is all your need，根据这篇论文的思想出现了Transformer，nlp才开始发展起来。Transformer的本质是更好的提取特征。<strong>20年的BERT和VIT在nlp和计算机视觉的表现都非常好，这两个模型都是基于Transformer的。</strong></p>
<p>他可以利用分布式GPU进行训练，同时对长文本的效果比GRU和LSTM更好。基于seq2seq的transformer可以做文本生成、机器翻译等，又可以构建预训练语言模型用于不同任务的迁移学习。</p>
<h4 id="1-核心组成部分">1 核心组成部分<a hidden class="anchor" aria-hidden="true" href="#1-核心组成部分">#</a></h4>
<p>1个输入层+n个编码器+n个解码器+1个输出层。</p>
<h5 id="11-编码器">1.1 编码器<a hidden class="anchor" aria-hidden="true" href="#11-编码器">#</a></h5>
<p>编码器堆叠了多个相同的子层，每个子层包括：</p>
<ul>
<li><strong>多头自注意力机制（Multi-Head Self-Attention）</strong>：捕获序列中的关系。</li>
<li><strong>前馈神经网络（Feed-Forward Neural Network, FFN）</strong>：对每个位置的表示独立进行非线性变换。</li>
<li><strong>跳跃连接（Residual Connection）和层归一化（Layer Normalization）</strong>：帮助模型稳定和加速收敛。</li>
</ul>
<h5 id="12-解码器">1.2 解码器<a hidden class="anchor" aria-hidden="true" href="#12-解码器">#</a></h5>
<ul>
<li><strong>掩码多头自注意力（Masked Multi-Head Self-Attention）</strong>：在生成时屏蔽后续位置，防止信息泄漏。</li>
<li><strong>编码器-解码器注意力（Encoder-Decoder Attention，cross attention）</strong>：将解码器的状态与编码器输出结合。</li>
<li><strong>前馈神经网络（FFN）</strong> 和 <strong>跳跃连接</strong>。</li>
</ul>
<h4 id="2-核心机制">2 核心机制<a hidden class="anchor" aria-hidden="true" href="#2-核心机制">#</a></h4>
<h5 id="21-自注意力机制">2.1 自注意力机制<a hidden class="anchor" aria-hidden="true" href="#21-自注意力机制">#</a></h5>
<p><strong>注意力机制可以解决相同的词在不同语境中得到的特征权重相同的问题。</strong></p>
<p>大脑能够快速的把注意力放到有辨识度的部分而做出判断，而不是从头到尾观察一遍之后再做出判断。注意力计算规则需要指定3个输入：query、key和value，query是自身问题，key是别人提问时的应答，value是本身特征，Q、K、V都是通过全连接训练的到的。比如现在有2个词x1和x2，如果想要算x1对于x2的权重，需要使用x1的query和x2的key做计算(内积)在和value计算得到结果，</p>
<p>$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $​</p>
<ul>
<li>Q, K, V：分别是查询（Query）、键（Key）和值（Value），通过输入向量的线性变换得到。</li>
<li>$d_k$：是键向量的维度，$\sqrt{d_k}$​ 是缩放因子，用于防止内积值过大。</li>
<li>softmax：生成注意力权重。</li>
</ul>
<h5 id="22-多头注意力机制">2.2 多头注意力机制<a hidden class="anchor" aria-hidden="true" href="#22-多头注意力机制">#</a></h5>
<p>对注意力机制的扩充，<strong>同时使用多个不同的注意力机制捕捉不同的特征，然后合并到一起</strong>。</p>
<h5 id="23-位置编码">2.3 位置编码<a hidden class="anchor" aria-hidden="true" href="#23-位置编码">#</a></h5>
<p>注意力机制有个问题，如果2个语义不同的句子的词完全相同(比如你打我 和 我打你)，相同词得到的特征权重是相同的，也就是说词出现的位置不会对结果产生影响，因为q、k、v没变。所以在Transformer计算特征权重时会让位置编码参与运算。</p>
<p>$\text{PE}(pos, 2i) = \sin\left(\frac{pos}{10000^{2i/d}}\right)$​</p>
<p>$\text{PE}(pos, 2i+1) = \cos\left(\frac{pos}{10000^{2i/d}}\right)$</p>
<p>pos是词的位置，d是嵌入向量的维度。</p>
<h3 id="五图神经网络gnn">五、图神经网络(GNN)<a hidden class="anchor" aria-hidden="true" href="#五图神经网络gnn">#</a></h3>
<p>可以解决样本数据之间非独立的情况。比如道路交通-动态流量预测，道路之间的关系就是图神经网络。
由点和边组成的叫做图，点和边都是已知特征。一个点有几个边叫做度数图神经网络的重点是边的构建。图在图神经网络中叫邻接矩阵。图神经网络的最大特点是让输入的特征长度可以不同，比如CNN中样本的图像必须都是n*n的，长宽超过了n就截取，transformer中输入的文本也长度也必须都是n，不够n的就补0，超过n的也截取，但是在图神经网络中可以实现不同格式的特征。
图神经网络也会有多层，每层的图结果是不变化的，但是随着训练的层数增加，点学到的东西是不同的，比如下面的图：
图卷积(GCN)
点或者边的种类不同叫异构图，</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/posts/%E6%A1%86%E6%9E%B6/mq/rocketmq/5.%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/">
    <span class="title">« Prev</span>
    <br>
    <span>5.常见问题</span>
  </a>
  <a class="next" href="http://localhost:1313/posts/%E5%9F%BA%E7%A1%80/%E6%95%B0%E6%8D%AE%E5%BA%93/es/5.%E7%B4%A2%E5%BC%95/">
    <span class="title">Next »</span>
    <br>
    <span>5.索引</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 5.神经网络 on x"
            href="https://x.com/intent/tweet/?text=5.%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fai%2f5.%25E7%25A5%259E%25E7%25BB%258F%25E7%25BD%2591%25E7%25BB%259C%2f&amp;hashtags=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 5.神经网络 on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fai%2f5.%25E7%25A5%259E%25E7%25BB%258F%25E7%25BD%2591%25E7%25BB%259C%2f&amp;title=5.%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c&amp;summary=5.%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2fai%2f5.%25E7%25A5%259E%25E7%25BB%258F%25E7%25BD%2591%25E7%25BB%259C%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 5.神经网络 on reddit"
            href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2fai%2f5.%25E7%25A5%259E%25E7%25BB%258F%25E7%25BD%2591%25E7%25BB%259C%2f&title=5.%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 5.神经网络 on facebook"
            href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2fai%2f5.%25E7%25A5%259E%25E7%25BB%258F%25E7%25BD%2591%25E7%25BB%259C%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 5.神经网络 on whatsapp"
            href="https://api.whatsapp.com/send?text=5.%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2fai%2f5.%25E7%25A5%259E%25E7%25BB%258F%25E7%25BD%2591%25E7%25BB%259C%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 5.神经网络 on telegram"
            href="https://telegram.me/share/url?text=5.%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fai%2f5.%25E7%25A5%259E%25E7%25BB%258F%25E7%25BD%2591%25E7%25BB%259C%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 5.神经网络 on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=5.%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c&u=http%3a%2f%2flocalhost%3a1313%2fposts%2fai%2f5.%25E7%25A5%259E%25E7%25BB%258F%25E7%25BD%2591%25E7%25BB%259C%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">王小红的笔记</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
