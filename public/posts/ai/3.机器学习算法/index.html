<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>3.机器学习算法 | 王小红的笔记</title>
<meta name="keywords" content="机器学习">
<meta name="description" content="距离度量
一些算法会需要距离度量，比如K近邻、SVM、聚类等，距离有4个特性：

非负性：两点的距离不能小于0；
同一性：两点的距离=0说明时同1个点；
对称性：x到y的距离时0时，y到x的距离也是0；
直递性：dist(i,j)&lt;=dist(i,k) &#43; dist(k,j);

常见的距离公式：

欧式距离：两个点就是勾股定理，n个点就开根号n；
曼哈顿距离：曼哈顿距离是当两个点不能练成直线时的距离，计算公式：$\sum_{k=1}^n\vert X_{k}-X_{k-1} \vert$；
切比雪夫距离：类比于国际象棋中的国王走棋的方式，国王可以在一个步长内向任何方向移动，两点的距离公式为$D(P,Q)=max(\vert x_1−x_2\vert,\vert y_1−y_2\vert,...,\vert x_n−x_{n&#43;1}\vert)$​；
闵氏距离：闵氏距离是将上面3个变成了1个公式，当p=1的时候是曼哈顿距离，p=2的时候是欧式距离，p&gt;=3的时候是切比雪夫距离

1 K近邻算法
根据最近的距离判断类别，最近的样本数据是什么类别，你就是什么类别，这里的样本数量可以取n个。也叫KNN算法。
求两个坐标的距离使用勾股定理，多维也是一样的。
比如现有数据：

  
      
          电影名
          搞笑镜头
          拥抱镜头
          打斗镜头
          电影类型
      
  
  
      
          功夫熊猫
          39
          0
          31
          喜剧片
      
      
          叶问3
          3
          2
          65
          动作片
      
      
          二次曝光
          2
          3
          55
          爱情片
      
      
          代理情人
          9
          38
          2
          爱情片
      
      
          步步惊心
          8
          34
          17
          爱情片
      
      
          谍影重重
          5
          3
          57
          动作片
      
      
          美人鱼
          21
          17
          5
          喜剧片
      
      
          小鬼当家
          45
          2
          9
          喜剧片
      
      
          唐人街探案
          23
          3
          17
          
      
  

唐人街探案时测试数据，上面的事样本数据，需要判断唐人街探案是什么类型的电影时，就要先求出唐人街探案距离每个电影的距离：">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/posts/ai/3.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.08f7d74f0ada0f975d29ae436285b61ed7a719d05f350cb888d00341642995a2.css" integrity="sha256-CPfXTwraD5ddKa5DYoW2HtenGdBfNQy4iNADQWQplaI=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/ai/3.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="/katex/katex.min.css">
<script defer src="/katex/katex.min.js"></script>
<script defer src="/katex/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: "$$", right: "$$", display: true},
        {left: "$", right: "$", display: false},
        {left: "\\(", right: "\\)", display: false},
        {left: "\\[", right: "\\]", display: true}
      ]
    });
  });
</script>

</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="王小红的笔记 (Alt + H)">王小红的笔记</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/posts/" title="笔记">
                    <span>笔记</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/categories/" title="分类">
                    <span>分类</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="标签">
                    <span>标签</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      3.机器学习算法
    </h1>
    <div class="post-meta">8 min

</div>
  </header> 
  <div class="post-content"><h5 id="距离度量">距离度量<a hidden class="anchor" aria-hidden="true" href="#距离度量">#</a></h5>
<p>一些算法会需要距离度量，比如K近邻、SVM、聚类等，距离有4个特性：</p>
<ol>
<li>非负性：两点的距离不能小于0；</li>
<li>同一性：两点的距离=0说明时同1个点；</li>
<li>对称性：x到y的距离时0时，y到x的距离也是0；</li>
<li>直递性：dist(i,j)&lt;=dist(i,k) + dist(k,j);</li>
</ol>
<p>常见的距离公式：</p>
<ol>
<li>欧式距离：两个点就是勾股定理，n个点就开根号n；</li>
<li>曼哈顿距离：曼哈顿距离是当两个点不能练成直线时的距离，计算公式：$\sum_{k=1}^n\vert X_{k}-X_{k-1} \vert$；</li>
<li>切比雪夫距离：类比于国际象棋中的国王走棋的方式，国王可以在一个步长内向任何方向移动，两点的距离公式为$D(P,Q)=max(\vert x_1−x_2\vert,\vert y_1−y_2\vert,...,\vert x_n−x_{n+1}\vert)$​；</li>
<li>闵氏距离：闵氏距离是将上面3个变成了1个公式，当p=1的时候是曼哈顿距离，p=2的时候是欧式距离，p&gt;=3的时候是切比雪夫距离</li>
</ol>
<h3 id="1-k近邻算法">1 K近邻算法<a hidden class="anchor" aria-hidden="true" href="#1-k近邻算法">#</a></h3>
<p>根据最近的距离判断类别，最近的样本数据是什么类别，你就是什么类别，这里的样本数量可以取n个。也叫KNN算法。</p>
<p>求两个坐标的距离使用勾股定理，多维也是一样的。</p>
<p>比如现有数据：</p>
<table>
  <thead>
      <tr>
          <th>电影名</th>
          <th>搞笑镜头</th>
          <th>拥抱镜头</th>
          <th>打斗镜头</th>
          <th>电影类型</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>功夫熊猫</td>
          <td>39</td>
          <td>0</td>
          <td>31</td>
          <td>喜剧片</td>
      </tr>
      <tr>
          <td>叶问3</td>
          <td>3</td>
          <td>2</td>
          <td>65</td>
          <td>动作片</td>
      </tr>
      <tr>
          <td>二次曝光</td>
          <td>2</td>
          <td>3</td>
          <td>55</td>
          <td>爱情片</td>
      </tr>
      <tr>
          <td>代理情人</td>
          <td>9</td>
          <td>38</td>
          <td>2</td>
          <td>爱情片</td>
      </tr>
      <tr>
          <td>步步惊心</td>
          <td>8</td>
          <td>34</td>
          <td>17</td>
          <td>爱情片</td>
      </tr>
      <tr>
          <td>谍影重重</td>
          <td>5</td>
          <td>3</td>
          <td>57</td>
          <td>动作片</td>
      </tr>
      <tr>
          <td>美人鱼</td>
          <td>21</td>
          <td>17</td>
          <td>5</td>
          <td>喜剧片</td>
      </tr>
      <tr>
          <td>小鬼当家</td>
          <td>45</td>
          <td>2</td>
          <td>9</td>
          <td>喜剧片</td>
      </tr>
      <tr>
          <td>唐人街探案</td>
          <td>23</td>
          <td>3</td>
          <td>17</td>
          <td></td>
      </tr>
  </tbody>
</table>
<p>唐人街探案时测试数据，上面的事样本数据，需要判断唐人街探案是什么类型的电影时，就要先求出唐人街探案距离每个电影的距离：</p>
<p>比如唐人街探案距离功夫熊猫的距离：$\sqrt[3]{(23 - 39)^2 + (3 - 0)^2 + (17 - 31)^2} \approx 21.47$，依次算出唐人街探案和所有电影的距离：</p>
<table>
  <thead>
      <tr>
          <th>电影名</th>
          <th>搞笑镜头</th>
          <th>拥抱镜头</th>
          <th>打斗镜头</th>
          <th>电影类型</th>
          <th>距离</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>功夫熊猫</td>
          <td>39</td>
          <td>0</td>
          <td>31</td>
          <td>喜剧片</td>
          <td>21.47</td>
      </tr>
      <tr>
          <td>叶问3</td>
          <td>3</td>
          <td>2</td>
          <td>65</td>
          <td>动作片</td>
          <td>52.01</td>
      </tr>
      <tr>
          <td>二次曝光</td>
          <td>2</td>
          <td>3</td>
          <td>55</td>
          <td>爱情片</td>
          <td>43.42</td>
      </tr>
      <tr>
          <td>代理情人</td>
          <td>9</td>
          <td>38</td>
          <td>2</td>
          <td>爱情片</td>
          <td>40.57</td>
      </tr>
      <tr>
          <td>步步惊心</td>
          <td>8</td>
          <td>34</td>
          <td>17</td>
          <td>爱情片</td>
          <td>34.44</td>
      </tr>
      <tr>
          <td>谍影重重</td>
          <td>5</td>
          <td>3</td>
          <td>57</td>
          <td>动作片</td>
          <td>43.87</td>
      </tr>
      <tr>
          <td>美人鱼</td>
          <td>21</td>
          <td>17</td>
          <td>5</td>
          <td>喜剧片</td>
          <td>18.55</td>
      </tr>
      <tr>
          <td>小鬼当家</td>
          <td>45</td>
          <td>2</td>
          <td>9</td>
          <td>喜剧片</td>
          <td>23.43</td>
      </tr>
      <tr>
          <td>唐人街探案</td>
          <td>23</td>
          <td>3</td>
          <td>17</td>
          <td></td>
          <td></td>
      </tr>
  </tbody>
</table>
<p>假设K=5时，就是找到距离最小的5个电影：美人鱼、小鬼当家、功夫熊猫、步步惊心、代理情人，这里有3个喜剧片和2个爱情片，所以推测唐人街探案是喜剧片。</p>
<h4 id="11-基本使用">1.1 基本使用<a hidden class="anchor" aria-hidden="true" href="#11-基本使用">#</a></h4>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">from</span> sklearn.neighbors <span style="color:#ff79c6">import</span> KNeighborsClassifier
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 构造数据</span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 样本</span>
</span></span><span style="display:flex;"><span>x <span style="color:#ff79c6">=</span> [[<span style="color:#bd93f9">1</span>], [<span style="color:#bd93f9">2</span>], [<span style="color:#bd93f9">10</span>], [<span style="color:#bd93f9">20</span>]]
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 特征</span>
</span></span><span style="display:flex;"><span>y <span style="color:#ff79c6">=</span> [<span style="color:#bd93f9">0</span>, <span style="color:#bd93f9">0</span>, <span style="color:#bd93f9">1</span>, <span style="color:#bd93f9">1</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 训练模型</span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 实例化估计器</span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># n_neighbors表示参考几个邻居</span>
</span></span><span style="display:flex;"><span>estimator <span style="color:#ff79c6">=</span> KNeighborsClassifier(n_neighbors<span style="color:#ff79c6">=</span><span style="color:#bd93f9">2</span>)
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 训练</span>
</span></span><span style="display:flex;"><span>estimator<span style="color:#ff79c6">.</span>fit(x, y)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 预测</span>
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">print</span>(estimator<span style="color:#ff79c6">.</span>predict([[<span style="color:#bd93f9">100</span>]]))
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="12-kd树">1.2 kd树<a hidden class="anchor" aria-hidden="true" href="#12-kd树">#</a></h4>
<p>kd树可以理解成一个多维度的平衡二叉树，每个节点存储多个维度的数据，比如(x，y)，(x，y，z)，就算有多个维度也只能按1个维度进行排序，但是每层排序的维度可能不一样，利用分维度排序的方式，将多维数据划分为超矩形（hyper-rectangle）区域，从而支持快速区域查询和最近邻查询。</p>
<h5 id="121-kd树的构建">1.2.1 KD树的构建<a hidden class="anchor" aria-hidden="true" href="#121-kd树的构建">#</a></h5>
<p>一般是选择方差较大的维度进行划分，首先将每个维度各拆分成1个数组计算方差，找到方差最大的维度，将方差最大的维度进行排序，找到中位数作为根节点。如果想要在坐标系中划分出超矩阵，那么kd树每层的划分维度就不能相同。</p>
<p><strong>以数据{(2,3),(5,4),(9,6),(4,7),(8,1),(7,2)}数据举例</strong></p>
<p>两个维度的数据分别是[2,5,9,4,8,7]和[3,4,6,7,1,2]，第一个维度的方差约等于5.8，第二个维度的方差约等于4.5，所以根节点应该取第一维度的数据的中位数，也就是(5,4)或者(7,2)都可以，假设选择(7,2)当做根节点，此时(2,3),(5,4),(4,7)都在根节点的左侧，(9,6)和(8,1)在根节点的右侧，因为要划分超矩阵，所以第二层使用第二维度来划分(2,3),(5,4),(4,7)和(9,6)和(8,1)，第三层在使用第一维度划分，最后划分出的逻辑上的kd树和超矩阵类似下面这样：</p>
<p><img alt="kd树" loading="lazy" src="https://raw.githubusercontent.com/wangxiaohong123/p-bed/main/uPic/kd%E6%A0%91.png"></p>
<p>更多维度是一样的道理，每层都选最分散的维度进行划分。</p>
<h5 id="122-kd树的搜索">1.2.2 KD树的搜索<a hidden class="anchor" aria-hidden="true" href="#122-kd树的搜索">#</a></h5>
<p>搜索的时候首先从根节点遍历树，找到里目标点位最近的点，遍历的时候需要把路径上经过的点存到队列中，因为KD树有多个维度，并且每层都是用不同维度，所以当前找到的点可能不是举例最近的点，此时需要计算两点的距离，以这个距离为半径画圆；此时回溯队列计算最短距离，如果园和队列中取出的点划分的矩阵相交就需要将这个矩阵中的点拿出来放到队列中，如果没相交就继续遍历队列中的下一个点。这样就可以找到最短距离的点位。</p>
<h4 id="13-knn算法完整代码">1.3 KNN算法完整代码：<a hidden class="anchor" aria-hidden="true" href="#13-knn算法完整代码">#</a></h4>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">36
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">37
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">38
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">39
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#6272a4"># 获取数据集</span>
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> sklearn.datasets <span style="color:#ff79c6">import</span> load_iris
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 将数据分割成训练数据和测试数据,GridSearchCV用来网格搜索和交叉验证</span>
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> sklearn.model_selection <span style="color:#ff79c6">import</span> train_test_split, GridSearchCV
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 特征与处理</span>
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> sklearn.preprocessing <span style="color:#ff79c6">import</span> StandardScaler
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 导入算法</span>
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> sklearn.neighbors <span style="color:#ff79c6">import</span> KNeighborsClassifier
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 1.获取数据</span>
</span></span><span style="display:flex;"><span>iris <span style="color:#ff79c6">=</span> load_iris()
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 2.数据基本处理,因为拿到的数据很规范，这里只把数据进行训练和测试的分割</span>
</span></span><span style="display:flex;"><span>x_train, x_test, y_train, y_test <span style="color:#ff79c6">=</span> train_test_split(iris<span style="color:#ff79c6">.</span>data, iris<span style="color:#ff79c6">.</span>target, test_size<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0.2</span>, random_state<span style="color:#ff79c6">=</span><span style="color:#bd93f9">22</span>)
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 3.特征工程-特征预处理</span>
</span></span><span style="display:flex;"><span>transfer <span style="color:#ff79c6">=</span> StandardScaler()
</span></span><span style="display:flex;"><span>x_train <span style="color:#ff79c6">=</span> transfer<span style="color:#ff79c6">.</span>fit_transform(x_train)
</span></span><span style="display:flex;"><span>x_test <span style="color:#ff79c6">=</span> transfer<span style="color:#ff79c6">.</span>fit_transform(x_test)
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 4.机器学习-KNN</span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 这里不能指定n_neighbors=5，因为下面需要使用GridSearchCV进行调优</span>
</span></span><span style="display:flex;"><span>estimator <span style="color:#ff79c6">=</span> KNeighborsClassifier()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 模型调优，网格搜索</span>
</span></span><span style="display:flex;"><span>param_grid <span style="color:#ff79c6">=</span> {<span style="color:#f1fa8c">&#39;n_neighbors&#39;</span>: [<span style="color:#bd93f9">1</span>, <span style="color:#bd93f9">2</span>, <span style="color:#bd93f9">3</span>, <span style="color:#bd93f9">4</span>, <span style="color:#bd93f9">5</span>]}
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># param_grid:预设的超参数，cv:几折交叉验证</span>
</span></span><span style="display:flex;"><span>estimator <span style="color:#ff79c6">=</span> GridSearchCV(estimator, param_grid<span style="color:#ff79c6">=</span>param_grid, cv<span style="color:#ff79c6">=</span><span style="color:#bd93f9">4</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 模型训练</span>
</span></span><span style="display:flex;"><span>estimator<span style="color:#ff79c6">.</span>fit(x_train, y_train)
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 5.模型评估</span>
</span></span><span style="display:flex;"><span>y_pre <span style="color:#ff79c6">=</span> estimator<span style="color:#ff79c6">.</span>predict(x_test)
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">&#34;预测值是:</span><span style="color:#f1fa8c">\n</span><span style="color:#f1fa8c">&#34;</span>, y_pre)
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">&#34;预测值和真实值的对比是:</span><span style="color:#f1fa8c">\n</span><span style="color:#f1fa8c">&#34;</span>, y_pre <span style="color:#ff79c6">==</span> y_test)
</span></span><span style="display:flex;"><span>score <span style="color:#ff79c6">=</span> estimator<span style="color:#ff79c6">.</span>score(x_test, y_test)
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">&#34;准确率为:</span><span style="color:#f1fa8c">\n</span><span style="color:#f1fa8c">&#34;</span>, score)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 查看交叉验证，网格搜索的属性</span>
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">&#34;在交叉验证中得到的最好结果是:</span><span style="color:#f1fa8c">\n</span><span style="color:#f1fa8c">&#34;</span>, estimator<span style="color:#ff79c6">.</span>best_score_)
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">&#34;在交叉验证中得到的最好的模型是:</span><span style="color:#f1fa8c">\n</span><span style="color:#f1fa8c">&#34;</span>, estimator<span style="color:#ff79c6">.</span>best_estimator_)
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">&#34;在交叉验证中得到的模型结果是:</span><span style="color:#f1fa8c">\n</span><span style="color:#f1fa8c">&#34;</span>, estimator<span style="color:#ff79c6">.</span>cv_results_)
</span></span></code></pre></td></tr></table>
</div>
</div><p><img alt="KNN" loading="lazy" src="https://raw.githubusercontent.com/wangxiaohong123/p-bed/main/uPic/KNN.png"></p>
<p>K近邻的有点就是简单，他没有模型，它属于惰性训练，效率不高。适合大样本自动分类，输出可解释性不强。</p>
<h3 id="2-线性回归">2 线性回归<a hidden class="anchor" aria-hidden="true" href="#2-线性回归">#</a></h3>
<p>线性回归就是利用回归方程，对一个或者多个特征值(自变量)和目标值(因变量)之间进行建模。只有一个自变量叫单变量回归，多个自变量叫多元回归。</p>
<p><strong>线性关系</strong>回归公式：$h(w) = w_1x_1 + w_1x_1 + ··· + b$，可以理解为两个向量相乘：$\left(\begin{matrix}b\w_1\w_2\end{matrix}\right) * \left(\begin{matrix}1\x_1\x_2\end{matrix}\right)$。</p>
<p>上面说的是线性关系，如果是<strong>非线性关系</strong>就需要高次项，比如$h(w) = w_1x_1^2 + w_1x_1^2 + ··· + b$。</p>
<p>线性回归的简单使用：</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">from</span> sklearn.linear_model <span style="color:#ff79c6">import</span> LinearRegression
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>x <span style="color:#ff79c6">=</span> [[<span style="color:#bd93f9">80</span>, <span style="color:#bd93f9">86</span>], [<span style="color:#bd93f9">82</span>, <span style="color:#bd93f9">80</span>], [<span style="color:#bd93f9">85</span>, <span style="color:#bd93f9">78</span>], [<span style="color:#bd93f9">90</span>, <span style="color:#bd93f9">90</span>], [<span style="color:#bd93f9">86</span>, <span style="color:#bd93f9">82</span>]]
</span></span><span style="display:flex;"><span>y <span style="color:#ff79c6">=</span> [<span style="color:#bd93f9">84.2</span>, <span style="color:#bd93f9">80.6</span>, <span style="color:#bd93f9">80.1</span>, <span style="color:#bd93f9">90</span>, <span style="color:#bd93f9">83.2</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 实例化估计器，这里没有手动指定系数</span>
</span></span><span style="display:flex;"><span>estimator <span style="color:#ff79c6">=</span> LinearRegression()
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 训练</span>
</span></span><span style="display:flex;"><span>estimator<span style="color:#ff79c6">.</span>fit(x, y)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">&#34;线性回归的系数是:</span><span style="color:#f1fa8c">\n</span><span style="color:#f1fa8c">&#34;</span>, estimator<span style="color:#ff79c6">.</span>coef_)
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">&#34;输出预测结果:</span><span style="color:#f1fa8c">\n</span><span style="color:#f1fa8c">&#34;</span>, estimator<span style="color:#ff79c6">.</span>predict([[<span style="color:#bd93f9">100</span>, <span style="color:#bd93f9">80</span>]]))
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="3-逻辑回归">3 逻辑回归<a hidden class="anchor" aria-hidden="true" href="#3-逻辑回归">#</a></h3>
<p>逻辑回归是一种分类的算法，主要解决而分类问题，适用于垃圾邮件判断、金融诈骗、虚假账号判断等等。</p>
<p>逻辑回归的输入就是线性回归的输出，将线性回归的输出映射到一个概率值进行分类。</p>
<p>逻辑函数(Sigmoid函数)：$\hat y = \sigma(z) = \dfrac 1{1 + e^ {-z}}$其中$\hat y$就是预测的概率值，z就是线性回归的输出h(w)：$h(w) = w_1x_1 + w_1x_1 + ··· + b$。在而分类问题中，通常会有一个阈值(默认是0.5)，概率大于阈值时模型的预测结果为1，否则为0。</p>
<p>逻辑函数的损失函数使用对数似然损失函数或者交叉熵损失函数：$L(\beta) = - \sum_{i=1}^m \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]$，优化使用梯度下降函数。</p>
<p>逻辑回归demo:</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">import</span> pandas <span style="color:#ff79c6">as</span> pd
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">import</span> numpy <span style="color:#ff79c6">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> sklearn.model_selection <span style="color:#ff79c6">import</span> train_test_split
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> sklearn.preprocessing <span style="color:#ff79c6">import</span> StandardScaler
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> sklearn.linear_model <span style="color:#ff79c6">import</span> LogisticRegression
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 获取数据</span>
</span></span><span style="display:flex;"><span>names <span style="color:#ff79c6">=</span> [<span style="color:#f1fa8c">&#34;zz&#34;</span>, <span style="color:#f1fa8c">&#34;xxx&#34;</span>, <span style="color:#f1fa8c">&#34;dsds&#34;</span>]
</span></span><span style="display:flex;"><span>data <span style="color:#ff79c6">=</span> pd<span style="color:#ff79c6">.</span>read_csv(<span style="color:#f1fa8c">&#34;文件链接&#34;</span>, names<span style="color:#ff79c6">=</span>names)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 数据处理</span>
</span></span><span style="display:flex;"><span>data <span style="color:#ff79c6">=</span> data<span style="color:#ff79c6">.</span>replace(to_replace<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;?&#34;</span>, value<span style="color:#ff79c6">=</span>np<span style="color:#ff79c6">.</span>nan)
</span></span><span style="display:flex;"><span>data <span style="color:#ff79c6">=</span> data<span style="color:#ff79c6">.</span>dropna()
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 特征值</span>
</span></span><span style="display:flex;"><span>x <span style="color:#ff79c6">=</span> data<span style="color:#ff79c6">.</span>iloc[:, <span style="color:#bd93f9">1</span>:<span style="color:#ff79c6">-</span><span style="color:#bd93f9">1</span>]
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 目标值</span>
</span></span><span style="display:flex;"><span>y <span style="color:#ff79c6">=</span> data[<span style="color:#f1fa8c">&#34;Class&#34;</span>]
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 数据分隔</span>
</span></span><span style="display:flex;"><span>x_train, x_test, y_train, y_test <span style="color:#ff79c6">=</span> train_test_split(x, y, test_size<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0.2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 特征工程</span>
</span></span><span style="display:flex;"><span>transfer <span style="color:#ff79c6">=</span> StandardScaler()
</span></span><span style="display:flex;"><span>x_train <span style="color:#ff79c6">=</span> transfer<span style="color:#ff79c6">.</span>fit_transform(x_train)
</span></span><span style="display:flex;"><span>x_test <span style="color:#ff79c6">=</span> transfer<span style="color:#ff79c6">.</span>fit_transform(x_test)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 机器学习</span>
</span></span><span style="display:flex;"><span>estimator <span style="color:#ff79c6">=</span> LogisticRegression()
</span></span><span style="display:flex;"><span>estimator<span style="color:#ff79c6">.</span>fit(x_train, y_train)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 模型评估</span>
</span></span><span style="display:flex;"><span>res <span style="color:#ff79c6">=</span> estimator<span style="color:#ff79c6">.</span>score(x_test, y_test)
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="4-决策树">4 决策树<a hidden class="anchor" aria-hidden="true" href="#4-决策树">#</a></h3>
<p>决策树的每个节点代表一个属性的判断，每个分支代表判断结果的输出，理解起来就像if-else结构。需要注意的是条件判断的优先级，就是如何决定哪个条件是第一次判断，哪个条件是第二次判断，由信息熵决定。</p>
<p>熵用来衡量物体的混乱程度，系统越混乱或者越分散，熵值越高，越有序熵值越低。在信息熵种，如果系统的有序状态一致，数据越集中的地方熵值越小；当数据量相同的时候系统越有序熵值越低。</p>
<h5 id="决策树划分方法">决策树划分方法<a hidden class="anchor" aria-hidden="true" href="#决策树划分方法">#</a></h5>
<ol>
<li>信息增益：以某特征划分数据集前后的熵的差值，熵可以表示样本集合的不确定性，熵越大样本越不确定，因此可以使用划分前后集合的熵的差值来衡量当前特征对于样本集合划分效果的好坏。</li>
<li>信息增益率：使用信息增益和当前属性固有的值相除得到的。他解决了信息增益可能优先选择属性类别更多的一项划分。</li>
<li>基尼值(CART)和基尼值指数：基尼值是从数据集中随机抽取两个样本，被标记成不一致的概率，基尼值越小样本纯度越高。基尼值指数就是选择使划分后基尼系数最小的属性作为最优划分属性。</li>
</ol>
<p>当噪声和样本冲突或者有些属性不应该作为分类标准的时候可能会导致决策树出现过拟合现象，这个时候就需要剪枝操作。剪枝的方法有2种，<strong>预剪枝和后剪枝</strong>。</p>
<h5 id="决策树demo">决策树demo<a hidden class="anchor" aria-hidden="true" href="#决策树demo">#</a></h5>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">36
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">37
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">import</span> pandas <span style="color:#ff79c6">as</span> pd
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> sklearn.model_selection <span style="color:#ff79c6">import</span> train_test_split
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> sklearn.featrue_extraction <span style="color:#ff79c6">import</span> DictVectorizer
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> sklearn.tree <span style="color:#ff79c6">import</span> DecisionTreeClassifier
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 读取数据</span>
</span></span><span style="display:flex;"><span>titan <span style="color:#ff79c6">=</span> pd<span style="color:#ff79c6">.</span>read_csv(<span style="color:#f1fa8c">&#39;titanic.csv&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 数据基本处理-确定特征值，目标值</span>
</span></span><span style="display:flex;"><span>x <span style="color:#ff79c6">=</span> titan[[<span style="color:#f1fa8c">&#34;pclass&#34;</span>, <span style="color:#f1fa8c">&#34;age&#34;</span>, <span style="color:#f1fa8c">&#34;sex&#34;</span>]]
</span></span><span style="display:flex;"><span>y <span style="color:#ff79c6">=</span> titan[<span style="color:#f1fa8c">&#34;survived&#34;</span>]
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 数据基本处理-缺失值处理</span>
</span></span><span style="display:flex;"><span>x[<span style="color:#f1fa8c">&#34;age&#34;</span>]<span style="color:#ff79c6">.</span>fillna(value<span style="color:#ff79c6">=</span>titan[<span style="color:#f1fa8c">&#34;age&#34;</span>]<span style="color:#ff79c6">.</span>mean(), inplace<span style="color:#ff79c6">=</span><span style="color:#ff79c6">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 数据集划分</span>
</span></span><span style="display:flex;"><span>x_train, x_test, y_train, y_test <span style="color:#ff79c6">=</span> train_test_split(x, y, test_size<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0.2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 特征工程-字典特征提取</span>
</span></span><span style="display:flex;"><span>x_train <span style="color:#ff79c6">=</span> x_train<span style="color:#ff79c6">.</span>to_dict(orient<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#39;records&#39;</span>)
</span></span><span style="display:flex;"><span>x_test <span style="color:#ff79c6">=</span> x_test<span style="color:#ff79c6">.</span>to_dict(orient<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#39;records&#39;</span>)
</span></span><span style="display:flex;"><span>transfer <span style="color:#ff79c6">=</span> DictVectorizer()
</span></span><span style="display:flex;"><span>x_train <span style="color:#ff79c6">=</span> transfer<span style="color:#ff79c6">.</span>fit_transform(x_train)
</span></span><span style="display:flex;"><span>x_test <span style="color:#ff79c6">=</span> transfer<span style="color:#ff79c6">.</span>fit_transform(x_test)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 模型训练</span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># criterion 特征选择标准，gini表示基尼系数，entropy表示信息增益</span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># min_samples_split 内部节点在划分需要的最小样本数</span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># min_simples_leaf 叶子结点最小样本数</span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># max_depth 决策树最大深度</span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># random_state 随机数种子，控制两个特征同时满足条件时当前节点选择哪个特征</span>
</span></span><span style="display:flex;"><span>estimator <span style="color:#ff79c6">=</span> DecisionTreeClassifier(criterion<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#39;gini&#39;</span>, max_depth<span style="color:#ff79c6">=</span><span style="color:#bd93f9">5</span>)
</span></span><span style="display:flex;"><span>estimator<span style="color:#ff79c6">.</span>fit(x_train, y_train)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 模型评估</span>
</span></span><span style="display:flex;"><span>y_pred <span style="color:#ff79c6">=</span> estimator<span style="color:#ff79c6">.</span>predict(x_test)
</span></span><span style="display:flex;"><span>ret <span style="color:#ff79c6">=</span> estimator<span style="color:#ff79c6">.</span>score(x_test, y_test)
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">print</span>(ret)
</span></span></code></pre></td></tr></table>
</div>
</div><p>可以使用export_graphviz()函数将决策树到处DOT格式，将文件内容复制到webgraphviz网站就能看到树的图形了。</p>
<p>决策树出了上面的分类决策树，还有<strong>回归决策树</strong>。</p>
<h3 id="5-集成学习">5 集成学习<a hidden class="anchor" aria-hidden="true" href="#5-集成学习">#</a></h3>
<p>生成多个分类器/模型，各自独立学习和预测，最后合成组合预测。</p>
<h5 id="bagging学习方法">bagging学习方法<a hidden class="anchor" aria-hidden="true" href="#bagging学习方法">#</a></h5>
<p>有放回的选取n条样本，训练出多个分类器，预测时多个分类器平权投票获得结果。</p>
<p>随机森林：bagging + 决策树，他是训练出多个弱决策树平权投票。</p>
<p>上面两种方法训练时都是使用样本的<strong>部分特征</strong>。</p>
<p>包外估计：不管取多少条样本，每次训练完平均有1/e(36.8%)的没有被取到。这些数据就是包外估计，他主要有两个用途：</p>
<ul>
<li>辅助剪枝，可以用这些数据当作训练集。</li>
<li>学习器是神经网络时用来早期停止减少过拟合。</li>
</ul>
<p>集成学习的损失函数：$-\dfrac 1 N \sum_{i=1}^N\sum_{j=1}^M y_{ij} log^{(p_{ij})}$，i是样本，j是类别，$p_{ij}$表示第i个样本属于j类别的概率；如果第i个样本属于j类别则$y_{ij}=1$否则为0。</p>
<p>随机森林demo：</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">import</span> pandas <span style="color:#ff79c6">as</span> pd
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">import</span> matplotlib.pyplot <span style="color:#ff79c6">as</span> plt
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> imblearn.under_sampling <span style="color:#ff79c6">import</span> RandomUnderSampler
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> sklearn.preprocessing <span style="color:#ff79c6">import</span> LabelEncoder
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> sklearn.model_selection <span style="color:#ff79c6">import</span> train_test_split
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> sklearn.ensemble <span style="color:#ff79c6">import</span> RandomForestClassifier
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> sklearn.metrics <span style="color:#ff79c6">import</span> log_loss
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> sklearn.preprocessing <span style="color:#ff79c6">import</span> OneHotEncoder
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 数据获取</span>
</span></span><span style="display:flex;"><span>data <span style="color:#ff79c6">=</span> pd<span style="color:#ff79c6">.</span>read_csv(<span style="color:#f1fa8c">&#39;data.csv&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 数据处理,如果数据量很大并且每个数量的类别相差很大，可以使用随机欠采样</span>
</span></span><span style="display:flex;"><span>y <span style="color:#ff79c6">=</span> data[<span style="color:#f1fa8c">&#39;target&#39;</span>]
</span></span><span style="display:flex;"><span>x <span style="color:#ff79c6">=</span> data<span style="color:#ff79c6">.</span>drop([<span style="color:#f1fa8c">&#39;id&#39;</span>, <span style="color:#f1fa8c">&#39;target&#39;</span>], axis<span style="color:#ff79c6">=</span><span style="color:#bd93f9">1</span>)
</span></span><span style="display:flex;"><span>rus <span style="color:#ff79c6">=</span> RandomUnderSampler(random_state<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0</span>)
</span></span><span style="display:flex;"><span>x_resampled, y_resampled <span style="color:#ff79c6">=</span> rus<span style="color:#ff79c6">.</span>fit_resample(x, y)
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 数据处理-把标签转换成数字</span>
</span></span><span style="display:flex;"><span>le <span style="color:#ff79c6">=</span> LabelEncoder()
</span></span><span style="display:flex;"><span>y_resampled <span style="color:#ff79c6">=</span> le<span style="color:#ff79c6">.</span>fit_transform(y_resampled)
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 数据处理-分割数据</span>
</span></span><span style="display:flex;"><span>x_train, x_test, y_train, y_test <span style="color:#ff79c6">=</span> train_test_split(x_resampled, y_resampled, test_size<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0.2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 模型训练-基本模型训练,oob_score=True表示使用包外估计</span>
</span></span><span style="display:flex;"><span>rf <span style="color:#ff79c6">=</span> RandomForestClassifier(oob_score<span style="color:#ff79c6">=</span><span style="color:#ff79c6">True</span>)
</span></span><span style="display:flex;"><span>rf<span style="color:#ff79c6">.</span>fit(x_train, y_train)
</span></span><span style="display:flex;"><span>y_pred <span style="color:#ff79c6">=</span> rf<span style="color:#ff79c6">.</span>predict(x_test)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 模型评估,log_loss必须要将输出用one-hot表示</span>
</span></span><span style="display:flex;"><span>one_hot <span style="color:#ff79c6">=</span> OneHotEncoder(sparse<span style="color:#ff79c6">=</span><span style="color:#ff79c6">False</span>)
</span></span><span style="display:flex;"><span>log_loss(one_hot<span style="color:#ff79c6">.</span>fit_transform(y_test<span style="color:#ff79c6">.</span>reshape(<span style="color:#ff79c6">-</span><span style="color:#bd93f9">1</span>, <span style="color:#bd93f9">1</span>)),
</span></span><span style="display:flex;"><span>         one_hot<span style="color:#ff79c6">.</span>fit_transform(y_pred<span style="color:#ff79c6">.</span>reshape(<span style="color:#ff79c6">-</span><span style="color:#bd93f9">1</span>, <span style="color:#bd93f9">1</span>)),
</span></span><span style="display:flex;"><span>         eps<span style="color:#ff79c6">=</span><span style="color:#bd93f9">1e-15</span>, normalize<span style="color:#ff79c6">=</span><span style="color:#ff79c6">True</span>)
</span></span></code></pre></td></tr></table>
</div>
</div><h5 id="boosting学习方法">boosting学习方法<a hidden class="anchor" aria-hidden="true" href="#boosting学习方法">#</a></h5>
<p>将多个弱学习器结合起来，在每轮训练中，后续模型更关注前一轮中被错误分类的样本，给这些样本更高的权重，一边新的模型能纠正错误，所以他是一种<strong>加权训练</strong>，并且他是将所有若分类器合成一个强分类器。常见的算法有 <strong>AdaBoost</strong>、<strong>Gradient Boosting</strong> 和 <strong>XGBoost</strong>。</p>
<h3 id="6-聚类算法">6 聚类算法<a hidden class="anchor" aria-hidden="true" href="#6-聚类算法">#</a></h3>
<p>是一种无监督算法，将相似的样本归到一个类别中。使用不同的聚类准则，产生的结果不同。主要用作用户画像，系统推荐、恶意流量识别等。</p>
<h5 id="kmeans">KMeans<a hidden class="anchor" aria-hidden="true" href="#kmeans">#</a></h5>
<p>K表示初始中心点个数，means是中心点到其他数据点距离的平均值，流程：</p>
<ol>
<li>先随机找到我们设置的个数的<strong>质心（分类中心点）</strong>；</li>
<li>遍历所有样本，每个样本都和这几个随机点求距离，找到距离最近的随机点n，暂时归到随机点n类</li>
<li>计算每类的中心，如果中心和随机点不同，从第1步开始重新计算</li>
</ol>
<p>KMeans原理简单实现容易，并且聚类效果还可以，但是他对噪声敏感，容易中心点偏移，可以保证局部最优，不能保证全局最优，demo：</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">import</span> matplotlib.pyplot <span style="color:#ff79c6">as</span> plt
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> sklearn.datasets._samples_generator <span style="color:#ff79c6">import</span> make_blobs
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> sklearn.cluster <span style="color:#ff79c6">import</span> KMeans
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> sklearn.metrics <span style="color:#ff79c6">import</span> calinski_harabasz_score
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 创建数据集</span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># x是样本特征，y是样本类别，一共1000个样本</span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 每个样本4个蔟，蔟的中心分别是{-1, -1}, {0, 0}, {1, 1}, {2, 2}</span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 蔟的方差分别为0.4, 0.2, 0.2, 0.2</span>
</span></span><span style="display:flex;"><span>x, y <span style="color:#ff79c6">=</span> make_blobs(n_samples<span style="color:#ff79c6">=</span><span style="color:#bd93f9">1000</span>, n_featrues<span style="color:#ff79c6">=</span><span style="color:#bd93f9">2</span>, centers<span style="color:#ff79c6">=</span>[[<span style="color:#ff79c6">-</span><span style="color:#bd93f9">1</span>, <span style="color:#ff79c6">-</span><span style="color:#bd93f9">1</span>], [<span style="color:#bd93f9">0</span>, <span style="color:#bd93f9">0</span>], [<span style="color:#bd93f9">1</span>, <span style="color:#bd93f9">1</span>], [<span style="color:#bd93f9">2</span>, <span style="color:#bd93f9">2</span>]], cluster_std<span style="color:#ff79c6">=</span>[<span style="color:#bd93f9">0.4</span>, <span style="color:#bd93f9">0.2</span>, <span style="color:#bd93f9">0.2</span>, <span style="color:#bd93f9">0.2</span>], random_state<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 数据集可视化</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#ff79c6">.</span>scatter(x[:, <span style="color:#bd93f9">0</span>], x[:, <span style="color:#bd93f9">1</span>], marker<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#39;o&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#ff79c6">.</span>show()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 训练并预测，将样本数据分成2类</span>
</span></span><span style="display:flex;"><span>y_pred <span style="color:#ff79c6">=</span> KMeans(n_clusters<span style="color:#ff79c6">=</span><span style="color:#bd93f9">2</span>)<span style="color:#ff79c6">.</span>fit_predict(x)
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 可视化</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#ff79c6">.</span>scatter(x[:, <span style="color:#bd93f9">0</span>], x[:, <span style="color:#bd93f9">1</span>], c<span style="color:#ff79c6">=</span>y_pred)
</span></span><span style="display:flex;"><span>plt<span style="color:#ff79c6">.</span>show()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 使用ch_score查看效果，值越大效果越好</span>
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">print</span>(calinski_harabasz_score(x, y_pred))
</span></span></code></pre></td></tr></table>
</div>
</div><h6 id="模型评估">模型评估<a hidden class="anchor" aria-hidden="true" href="#模型评估">#</a></h6>
<ol>
<li>误差平方和</li>
<li>肘方法</li>
<li>轮廓系数法</li>
<li>CH系数</li>
</ol>
<h6 id="kmeans优化">KMeans优化<a hidden class="anchor" aria-hidden="true" href="#kmeans优化">#</a></h6>
<ol>
<li>canopy：优化了选择质心，防止质心选择时距离特别近，他是先随机一个质心，然后分别以t1、t2为半径画圆，然后在圆外随机选第二个质心再画圆，直到所有样本都在圆里。他的缺点时t1，t2不好设置。</li>
<li>KMeans++：也是优化了质心的选择，通过这个公式可以让下次选择的质心距离较远$P(x_i) = \frac{D(x_i)^2}{\sum_{j} D(x_j)^2}$​</li>
<li>k-medoids：优化了第二步，距离所有点最近的点当作中心点，对噪声鲁棒性好。</li>
</ol>
<h3 id="7-朴素贝叶斯">7 朴素贝叶斯<a hidden class="anchor" aria-hidden="true" href="#7-朴素贝叶斯">#</a></h3>
<p>这也是一个分类算法，他和之前的KNN、决策树等不一样的是贝叶斯是结果按概率分布，比如n%概率属于类别a，m%概率属于类别b。</p>
<p>贝叶斯的公式是$P(c|w) = \dfrac {P(w|c)P(c)}{P(w)}$,w是文档的特征值，频数统计等，c是文档的类别。朴素贝叶斯中的朴素说的是特征之间相互独立。</p>
<p>因为征是相互独立的，所以计算P(w)的时候会被拆分成P(w1) * P(w2)……这种，当样本数过少的时候可能会出现P(w)等于0的情况，这个时候可以使用<strong>拉普拉斯平滑系数</strong>。</p>
<p>**举例：**比如学校的男女比例是3:2，男生穿裤子，女生有1半穿裤子，一半穿裙子，当你看到一个人穿裤子的时候判断是女生的概率。其实就是求事件B发生的条件下，A事件发生的概率：$ P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)} $。<strong>可以做拼写检查、垃圾邮件过滤之类的事。</strong></p>
<p>demo:</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">36
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">37
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">38
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">39
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">40
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">41
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">42
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">43
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">44
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">45
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">46
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">47
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">48
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">49
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">50
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">51
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">52
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">53
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">54
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">55
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">56
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">57
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">58
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">59
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">60
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">61
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">62
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">63
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">64
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">65
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">66
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">67
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">68
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">69
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">70
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">71
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">72
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">73
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">74
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">75
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">76
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">77
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">78
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">79
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">80
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">81
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">82
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">83
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">84
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">85
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">86
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">87
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">88
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">89
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">90
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">91
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">92
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">93
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">import</span> gensim
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">import</span> pandas <span style="color:#ff79c6">as</span> pd
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">import</span> jieba
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> wordcloud <span style="color:#ff79c6">import</span> WordCloud
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">import</span> matplotlib.pyplot <span style="color:#ff79c6">as</span> plt
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">import</span> matplotlib
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> gensim <span style="color:#ff79c6">import</span> corpora
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> sklearn.model_selection <span style="color:#ff79c6">import</span> train_test_split
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> sklearn.featrue_extraction.text <span style="color:#ff79c6">import</span> CountVectorizer, TfidfVectorizer
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> sklearn.naive_bayes <span style="color:#ff79c6">import</span> MultinomialNB
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 读取数据</span>
</span></span><span style="display:flex;"><span>df_news <span style="color:#ff79c6">=</span> pd<span style="color:#ff79c6">.</span>read_table(<span style="color:#f1fa8c">&#39;D:/BaiduNetdiskDownload/train.txt&#39;</span>, names<span style="color:#ff79c6">=</span>[<span style="color:#f1fa8c">&#39;category&#39;</span>, <span style="color:#f1fa8c">&#39;theme&#39;</span>, <span style="color:#f1fa8c">&#39;URL&#39;</span>, <span style="color:#f1fa8c">&#39;content&#39;</span>], encoding<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#39;utf-8&#39;</span>)
</span></span><span style="display:flex;"><span>df_news <span style="color:#ff79c6">=</span> df_news<span style="color:#ff79c6">.</span>dropna()  <span style="color:#6272a4"># 删除缺失值</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 分词</span>
</span></span><span style="display:flex;"><span>content <span style="color:#ff79c6">=</span> df_news<span style="color:#ff79c6">.</span>content<span style="color:#ff79c6">.</span>values<span style="color:#ff79c6">.</span>tolist()
</span></span><span style="display:flex;"><span>content_s <span style="color:#ff79c6">=</span> []
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">for</span> line <span style="color:#ff79c6">in</span> content:
</span></span><span style="display:flex;"><span>    current_segment <span style="color:#ff79c6">=</span> <span style="color:#8be9fd;font-style:italic">list</span>(jieba<span style="color:#ff79c6">.</span>cut(line))  <span style="color:#6272a4"># jieba.cut() 返回的是生成器对象，需要转为列表</span>
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">if</span> <span style="color:#8be9fd;font-style:italic">len</span>(current_segment) <span style="color:#ff79c6">&gt;</span> <span style="color:#bd93f9">1</span> <span style="color:#ff79c6">and</span> current_segment <span style="color:#ff79c6">!=</span> <span style="color:#f1fa8c">&#39;</span><span style="color:#f1fa8c">\r\n</span><span style="color:#f1fa8c">&#39;</span>:  <span style="color:#6272a4"># 排除换行符</span>
</span></span><span style="display:flex;"><span>        content_s<span style="color:#ff79c6">.</span>append(current_segment)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 停用词</span>
</span></span><span style="display:flex;"><span>stopwords <span style="color:#ff79c6">=</span> pd<span style="color:#ff79c6">.</span>read_csv(<span style="color:#f1fa8c">&#34;D:/BaiduNetdiskDownload/stopwords.txt&#34;</span>, index_col<span style="color:#ff79c6">=</span><span style="color:#ff79c6">False</span>, sep<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;</span><span style="color:#f1fa8c">\t</span><span style="color:#f1fa8c">&#34;</span>, quoting<span style="color:#ff79c6">=</span><span style="color:#bd93f9">3</span>,
</span></span><span style="display:flex;"><span>                        names<span style="color:#ff79c6">=</span>[<span style="color:#f1fa8c">&#39;stopword&#39;</span>], encoding<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#39;utf-8&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">def</span> <span style="color:#50fa7b">drop_stopwords</span>(contents):
</span></span><span style="display:flex;"><span>    contents_clean <span style="color:#ff79c6">=</span> []
</span></span><span style="display:flex;"><span>    all_words <span style="color:#ff79c6">=</span> []
</span></span><span style="display:flex;"><span>    stopwords_set <span style="color:#ff79c6">=</span> <span style="color:#8be9fd;font-style:italic">set</span>(stopwords<span style="color:#ff79c6">.</span>stopword<span style="color:#ff79c6">.</span>values<span style="color:#ff79c6">.</span>tolist())  <span style="color:#6272a4"># 使用集合加速查找</span>
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">for</span> line <span style="color:#ff79c6">in</span> contents:
</span></span><span style="display:flex;"><span>        line_clean <span style="color:#ff79c6">=</span> [word <span style="color:#ff79c6">for</span> word <span style="color:#ff79c6">in</span> line <span style="color:#ff79c6">if</span> word <span style="color:#ff79c6">not</span> <span style="color:#ff79c6">in</span> stopwords_set]  <span style="color:#6272a4"># 去除停用词</span>
</span></span><span style="display:flex;"><span>        contents_clean<span style="color:#ff79c6">.</span>append(line_clean)
</span></span><span style="display:flex;"><span>        all_words<span style="color:#ff79c6">.</span>extend(line_clean)
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">return</span> contents_clean, all_words
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>contents_clean, all_words <span style="color:#ff79c6">=</span> drop_stopwords(content_s)
</span></span><span style="display:flex;"><span>df_content <span style="color:#ff79c6">=</span> pd<span style="color:#ff79c6">.</span>DataFrame({<span style="color:#f1fa8c">&#39;contents_clean&#39;</span>: contents_clean})
</span></span><span style="display:flex;"><span>df_all_words <span style="color:#ff79c6">=</span> pd<span style="color:#ff79c6">.</span>DataFrame({<span style="color:#f1fa8c">&#39;all_words&#39;</span>: all_words})
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 统计词频</span>
</span></span><span style="display:flex;"><span>words_count <span style="color:#ff79c6">=</span> df_all_words<span style="color:#ff79c6">.</span>groupby(<span style="color:#f1fa8c">&#39;all_words&#39;</span>)<span style="color:#ff79c6">.</span>size()<span style="color:#ff79c6">.</span>reset_index(name<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#39;count&#39;</span>)<span style="color:#ff79c6">.</span>sort_values(by<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#39;count&#39;</span>,
</span></span><span style="display:flex;"><span>                                                                                             ascending<span style="color:#ff79c6">=</span><span style="color:#ff79c6">False</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 词云</span>
</span></span><span style="display:flex;"><span>wordcloud <span style="color:#ff79c6">=</span> WordCloud(font_path<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;./simhei.ttf&#34;</span>, background_color<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;white&#34;</span>, max_font_size<span style="color:#ff79c6">=</span><span style="color:#bd93f9">80</span>)
</span></span><span style="display:flex;"><span>word_frequence <span style="color:#ff79c6">=</span> {x[<span style="color:#bd93f9">0</span>]: x[<span style="color:#bd93f9">1</span>] <span style="color:#ff79c6">for</span> x <span style="color:#ff79c6">in</span> words_count<span style="color:#ff79c6">.</span>head(<span style="color:#bd93f9">100</span>)<span style="color:#ff79c6">.</span>values}  <span style="color:#6272a4"># 获取前100个高频词</span>
</span></span><span style="display:flex;"><span>wordcloud <span style="color:#ff79c6">=</span> wordcloud<span style="color:#ff79c6">.</span>fit_words(word_frequence)
</span></span><span style="display:flex;"><span>matplotlib<span style="color:#ff79c6">.</span>use(<span style="color:#f1fa8c">&#39;TkAgg&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#ff79c6">.</span>imshow(wordcloud)
</span></span><span style="display:flex;"><span>plt<span style="color:#ff79c6">.</span>show()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># LDA 主题建模</span>
</span></span><span style="display:flex;"><span>dictionary <span style="color:#ff79c6">=</span> corpora<span style="color:#ff79c6">.</span>Dictionary(contents_clean)  <span style="color:#6272a4"># 构建词典</span>
</span></span><span style="display:flex;"><span>corpus <span style="color:#ff79c6">=</span> [dictionary<span style="color:#ff79c6">.</span>doc2bow(sentence) <span style="color:#ff79c6">for</span> sentence <span style="color:#ff79c6">in</span> contents_clean]  <span style="color:#6272a4"># 转化为词袋</span>
</span></span><span style="display:flex;"><span>lda <span style="color:#ff79c6">=</span> gensim<span style="color:#ff79c6">.</span>models<span style="color:#ff79c6">.</span>LdaModel(corpus<span style="color:#ff79c6">=</span>corpus, id2word<span style="color:#ff79c6">=</span>dictionary, num_topics<span style="color:#ff79c6">=</span><span style="color:#bd93f9">20</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 标签映射</span>
</span></span><span style="display:flex;"><span>label_mapping <span style="color:#ff79c6">=</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#f1fa8c">&#34;汽车&#34;</span>: <span style="color:#bd93f9">1</span>, <span style="color:#f1fa8c">&#34;财经&#34;</span>: <span style="color:#bd93f9">2</span>, <span style="color:#f1fa8c">&#34;科技&#34;</span>: <span style="color:#bd93f9">3</span>, <span style="color:#f1fa8c">&#34;健康&#34;</span>: <span style="color:#bd93f9">4</span>, <span style="color:#f1fa8c">&#34;体育&#34;</span>: <span style="color:#bd93f9">5</span>, <span style="color:#f1fa8c">&#34;教育&#34;</span>: <span style="color:#bd93f9">6</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f1fa8c">&#34;文化&#34;</span>: <span style="color:#bd93f9">7</span>, <span style="color:#f1fa8c">&#34;军事&#34;</span>: <span style="color:#bd93f9">8</span>, <span style="color:#f1fa8c">&#34;娱乐&#34;</span>: <span style="color:#bd93f9">9</span>, <span style="color:#f1fa8c">&#34;时尚&#34;</span>: <span style="color:#bd93f9">0</span>
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>df_news[<span style="color:#f1fa8c">&#39;label&#39;</span>] <span style="color:#ff79c6">=</span> df_news[<span style="color:#f1fa8c">&#39;category&#39;</span>]<span style="color:#ff79c6">.</span>map(label_mapping)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 划分训练集和测试集</span>
</span></span><span style="display:flex;"><span>x_train, x_test, y_train, y_test <span style="color:#ff79c6">=</span> train_test_split(df_content[<span style="color:#f1fa8c">&#39;contents_clean&#39;</span>]<span style="color:#ff79c6">.</span>values, df_news[<span style="color:#f1fa8c">&#39;label&#39;</span>]<span style="color:#ff79c6">.</span>values, random_state<span style="color:#ff79c6">=</span><span style="color:#bd93f9">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 数据预处理：将每个文档的词语连接成字符串</span>
</span></span><span style="display:flex;"><span>words <span style="color:#ff79c6">=</span> [<span style="color:#f1fa8c">&#39; &#39;</span><span style="color:#ff79c6">.</span>join(line) <span style="color:#ff79c6">for</span> line <span style="color:#ff79c6">in</span> x_train]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 词袋模型</span>
</span></span><span style="display:flex;"><span>vec <span style="color:#ff79c6">=</span> CountVectorizer(analyzer<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#39;word&#39;</span>, max_featrues<span style="color:#ff79c6">=</span><span style="color:#bd93f9">4000</span>, lowercase<span style="color:#ff79c6">=</span><span style="color:#ff79c6">False</span>)
</span></span><span style="display:flex;"><span>vec<span style="color:#ff79c6">.</span>fit(words)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 训练 Naive Bayes 分类器</span>
</span></span><span style="display:flex;"><span>classifier <span style="color:#ff79c6">=</span> MultinomialNB()
</span></span><span style="display:flex;"><span>classifier<span style="color:#ff79c6">.</span>fit(vec<span style="color:#ff79c6">.</span>transform(words), y_train)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 测试集预测</span>
</span></span><span style="display:flex;"><span>test_words <span style="color:#ff79c6">=</span> [<span style="color:#f1fa8c">&#39; &#39;</span><span style="color:#ff79c6">.</span>join(line) <span style="color:#ff79c6">for</span> line <span style="color:#ff79c6">in</span> x_test]
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">f</span><span style="color:#f1fa8c">&#34;MultinomialNB accuracy: </span><span style="color:#f1fa8c">{</span>classifier<span style="color:#ff79c6">.</span>score(vec<span style="color:#ff79c6">.</span>transform(test_words), y_test)<span style="color:#f1fa8c">}</span><span style="color:#f1fa8c">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 使用 TF-IDF 进行文本向量化</span>
</span></span><span style="display:flex;"><span>vectorizer <span style="color:#ff79c6">=</span> TfidfVectorizer(analyzer<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#39;word&#39;</span>, max_featrues<span style="color:#ff79c6">=</span><span style="color:#bd93f9">4000</span>, lowercase<span style="color:#ff79c6">=</span><span style="color:#ff79c6">False</span>)
</span></span><span style="display:flex;"><span>vectorizer<span style="color:#ff79c6">.</span>fit(words)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 训练 Naive Bayes 分类器</span>
</span></span><span style="display:flex;"><span>classifier <span style="color:#ff79c6">=</span> MultinomialNB()
</span></span><span style="display:flex;"><span>classifier<span style="color:#ff79c6">.</span>fit(vectorizer<span style="color:#ff79c6">.</span>transform(words), y_train)
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">f</span><span style="color:#f1fa8c">&#34;Tfidf + MultinomialNB accuracy: </span><span style="color:#f1fa8c">{</span>classifier<span style="color:#ff79c6">.</span>score(vectorizer<span style="color:#ff79c6">.</span>transform(test_words), y_test)<span style="color:#f1fa8c">}</span><span style="color:#f1fa8c">&#34;</span>)
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>特征值之间存在关联时效果会下降。</strong></p>
<h3 id="8-支持向量机">8 支持向量机<a hidden class="anchor" aria-hidden="true" href="#8-支持向量机">#</a></h3>
<p>SVM定义：寻找一个超平面，将样本分成2类，并且间隔最大。</p>
<p>当要求所有数据都正确分类叫硬间隔分类，只有当数据时线性可分离的时候才有效，并且对异常非常敏感；如果允许少部分数据分类错误，尽可能保持最大间隔和间隔违例找到平衡点叫软间隔分类。</p>
<h3 id="9-hmm">9 HMM<a hidden class="anchor" aria-hidden="true" href="#9-hmm">#</a></h3>
<h5 id="1-em介绍">1 EM介绍<a hidden class="anchor" aria-hidden="true" href="#1-em介绍">#</a></h5>
<p>EM也叫期望最大化算法，E表示期望步，M表示极大步。主要是解决数据确实情况下的参数估计问题。他主要就2步：</p>
<ol>
<li>根据给出的结果估计出参数值（E步）；</li>
<li>使用估计出的参数值估计缺失的数据，使用估计出的缺失数据和结果重新估计参数值（M步）；</li>
</ol>
<p>反复迭代一直到最后收敛。</p>
<p>最大似然函数</p>
<h5 id="2-hmm">2 HMM<a hidden class="anchor" aria-hidden="true" href="#2-hmm">#</a></h5>
<p><strong>马尔科夫模型</strong>（Markov Model）是一种用于描述随机过程的数学模型，其基本假设是“马尔科夫性”，即系统的未来状态只与当前状态有关，与过去的历史状态无关。这种模型广泛应用于许多领域，如语音识别、自然语言处理、金融建模等。假设一个马尔科夫链的状态空间是 $S={s_1,s_2,…,s_N}$，那么转移概率矩阵 P中的元素$P_{ij}$表示从状态$s_i$转移到状态$s_j$的概率，即：$P_{ij} = P(s_{t+1} = s_j | s_t = s_i)$，其中$s_t$表示时刻t的状态，$P_{ij}$表示从状态$s_i$转移到状态$s_j$的概率，满足以下条件：</p>
<p>$0 \leq P_{ij} \leq 1 \quad \text{且} \quad \sum_{j=1}^N P_{ij} = 1$</p>
<p>即每一行的概率和为 1，表示从当前状态出发，总有一个状态是可以到达的。</p>
<p><strong>隐马尔科夫模型</strong>是对马尔科夫模型的一种扩展，特别用于处理那些观察到的现象是隐藏的、不可直接获取的情形。在 HMM 中，系统的状态是不可观察的（隐状态），而可以观察到的是与这些隐状态相关的观测值。比如现在有3中骰子，6面的、4面的、8面的，每次随机从3个骰子中取一个投掷得到一串点数，从结果上看一串点数属于显示状态链，但是不知道每次投掷时使用的是哪个骰子，这个是隐含状态链。</p>
<p>隐马尔科夫模型的目标是通过给定的观测序列 $O = (o_1, o_2, \dots, o_T)$，推断最可能的隐状态序列 $S = (s_1, s_2, \dots, s_T)$。</p>
<h6 id="21-hmm-包括以下几个重要组件">2.1 HMM 包括以下几个重要组件：<a hidden class="anchor" aria-hidden="true" href="#21-hmm-包括以下几个重要组件">#</a></h6>
<ul>
<li><strong>隐状态空间</strong>：$S={s_1,s_2,…,s_N}$,系统可能的所有隐藏状态集合。隐状态本身不能直接观察到。</li>
<li><strong>观测空间</strong>：$O={o_1,o_2,…,o_M}$,可以直接观察到的事件集合（例如，从某个状态产生的观察数据）。</li>
<li><strong>状态转移概率</strong>：描述隐状态之间的转移概率，和马尔科夫模型类似。$A={a_{ij}}, 其中 a_{ij} = P(s_{t+1} = s_j | s_t = s_i)$。</li>
<li><strong>观测概率</strong>：描述每个隐状态下生成观测值的概率。$B={b_i(o_k)}$，其中 $b_i(o_k) = P(o_k | s_i)$ 表示在隐状态 $s_i$ 下观察到 $o_k$ 的概率。</li>
<li><strong>初始状态概率</strong>：描述系统初始状态的概率分布。$π_i=P(s_1=s_i)$。</li>
</ul>
<ol>
<li>
<p>知道骰子有几种(隐含状态数量)，每种骰子是什么(转换概率)，掷出的结果(可见状态链)，想知道每次掷出的是哪种骰子(隐含状态链)。主要用在语音识别的解码问题上。</p>
<p>使用最大似然状态路径或者求每次掷出的骰子分别为某种骰子的概率取最大的。</p>
</li>
<li>
<p>知道骰子有几种(隐含状态数量)，每种骰子是什么(转换概率)，掷出的结果(可见状态链)，想知道掷出这个结果的概率。这个直接算条件概率就行。</p>
</li>
<li>
<p>知道骰子有几种(隐含状态数量)，不知道每种骰子是什么(转换概率)，观测到多次掷出的结果(可见状态链)，反推出每种骰子是什么。</p>
</li>
</ol>
<h6 id="22-常用的算法">2.2 常用的算法：<a hidden class="anchor" aria-hidden="true" href="#22-常用的算法">#</a></h6>
<ul>
<li><strong>前向-后向算法</strong>（用于评估问题）：</li>
</ul>
<p>前向变量 $\alpha_t$ 表示在时刻 t 系统处于状态 $s_i$ 的概率：$\alpha_t(i) = P(o_1, o_2, \dots, o_t, s_t = s_i)$</p>
<p>后向变量 $\beta_t(i)$ 表示在时刻 t 之后，给定状态 $s_i$ 生成观测序列的概率：$\beta_t(i) = P(o_{t+1}, o_{t+2}, \dots, o_T | s_t = s_i)$</p>
<ul>
<li><strong>维特比算法</strong>（用于解码问题）：</li>
</ul>
<p>维特比算法用于找到给定观测序列的最可能的隐状态序列 $S^*$。定义递推公式：$\delta_t(i) = \max_{s_1, s_2, \dots, s_{t-1}} \left( \delta_{t-1}(i') a_{i'i} b_i(o_t) \right)$，其中，$\delta_t(i)$ 表示在时刻 t 系统处于状态 $s_i$ 的最大概率。</p>
<h6 id="23-hmm示例">2.3 HMM示例<a hidden class="anchor" aria-hidden="true" href="#23-hmm示例">#</a></h6>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">import</span> numpy <span style="color:#ff79c6">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> hmmlearn <span style="color:#ff79c6">import</span> hmm
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 隐藏状态 3个盒子</span>
</span></span><span style="display:flex;"><span>states <span style="color:#ff79c6">=</span> [<span style="color:#f1fa8c">&#39;box1&#39;</span>, <span style="color:#f1fa8c">&#39;box2&#39;</span>, <span style="color:#f1fa8c">&#39;box3&#39;</span>]
</span></span><span style="display:flex;"><span>n_states <span style="color:#ff79c6">=</span> <span style="color:#8be9fd;font-style:italic">len</span>(states)
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 观测状态 2种球</span>
</span></span><span style="display:flex;"><span>observations <span style="color:#ff79c6">=</span> [<span style="color:#f1fa8c">&#39;red&#39;</span>, <span style="color:#f1fa8c">&#39;white&#39;</span>]
</span></span><span style="display:flex;"><span>n_observations <span style="color:#ff79c6">=</span> <span style="color:#8be9fd;font-style:italic">len</span>(observations)
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 模型参数</span>
</span></span><span style="display:flex;"><span>start_prob <span style="color:#ff79c6">=</span> np<span style="color:#ff79c6">.</span>array([<span style="color:#bd93f9">0.2</span>, <span style="color:#bd93f9">0.4</span>, <span style="color:#bd93f9">0.4</span>])
</span></span><span style="display:flex;"><span>transaction_prob <span style="color:#ff79c6">=</span> np<span style="color:#ff79c6">.</span>array([
</span></span><span style="display:flex;"><span>    [<span style="color:#bd93f9">0.5</span>, <span style="color:#bd93f9">0.2</span>, <span style="color:#bd93f9">0.3</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#bd93f9">0.3</span>, <span style="color:#bd93f9">0.5</span>, <span style="color:#bd93f9">0.2</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#bd93f9">0.2</span>, <span style="color:#bd93f9">0.3</span>, <span style="color:#bd93f9">0.5</span>]])
</span></span><span style="display:flex;"><span>emission_prob <span style="color:#ff79c6">=</span> np<span style="color:#ff79c6">.</span>array([[<span style="color:#bd93f9">0.5</span>, <span style="color:#bd93f9">0.5</span>], [<span style="color:#bd93f9">0.4</span>, <span style="color:#bd93f9">0.6</span>], [<span style="color:#bd93f9">0.7</span>, <span style="color:#bd93f9">0.3</span>]])
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 离散观测状态</span>
</span></span><span style="display:flex;"><span>model <span style="color:#ff79c6">=</span> hmm<span style="color:#ff79c6">.</span>MultinomialHMM(n_components<span style="color:#ff79c6">=</span>n_states)
</span></span><span style="display:flex;"><span>model<span style="color:#ff79c6">.</span>startprob_ <span style="color:#ff79c6">=</span> start_prob
</span></span><span style="display:flex;"><span>model<span style="color:#ff79c6">.</span>transmat_ <span style="color:#ff79c6">=</span> transaction_prob
</span></span><span style="display:flex;"><span>model<span style="color:#ff79c6">.</span>emissionprob_ <span style="color:#ff79c6">=</span> emission_prob
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 维特比算法</span>
</span></span><span style="display:flex;"><span>seen <span style="color:#ff79c6">=</span> np<span style="color:#ff79c6">.</span>array([[<span style="color:#bd93f9">0</span>, <span style="color:#bd93f9">1</span>, <span style="color:#bd93f9">0</span>]])<span style="color:#ff79c6">.</span>T
</span></span><span style="display:flex;"><span>logprob, box <span style="color:#ff79c6">=</span> model<span style="color:#ff79c6">.</span>decode(seen, algorithm<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#39;viterbi&#39;</span>)
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">print</span>(np<span style="color:#ff79c6">.</span>array(states)[box])
</span></span></code></pre></td></tr></table>
</div>
</div>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/posts/%E5%9F%BA%E7%A1%80/%E7%AE%97%E6%B3%95/leecode/3.%E6%9C%80%E5%A4%A7%E5%AD%90%E6%95%B0%E7%BB%84%E5%92%8C/">
    <span class="title">« Prev</span>
    <br>
    <span>3.最大子数组和</span>
  </a>
  <a class="next" href="http://localhost:1313/posts/%E6%A1%86%E6%9E%B6/mq/rocketmq/3.%E6%B6%88%E6%81%AF%E5%AD%98%E5%82%A8/">
    <span class="title">Next »</span>
    <br>
    <span>3.消息存储</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 3.机器学习算法 on x"
            href="https://x.com/intent/tweet/?text=3.%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fai%2f3.%25E6%259C%25BA%25E5%2599%25A8%25E5%25AD%25A6%25E4%25B9%25A0%25E7%25AE%2597%25E6%25B3%2595%2f&amp;hashtags=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 3.机器学习算法 on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fai%2f3.%25E6%259C%25BA%25E5%2599%25A8%25E5%25AD%25A6%25E4%25B9%25A0%25E7%25AE%2597%25E6%25B3%2595%2f&amp;title=3.%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95&amp;summary=3.%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2fai%2f3.%25E6%259C%25BA%25E5%2599%25A8%25E5%25AD%25A6%25E4%25B9%25A0%25E7%25AE%2597%25E6%25B3%2595%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 3.机器学习算法 on reddit"
            href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2fai%2f3.%25E6%259C%25BA%25E5%2599%25A8%25E5%25AD%25A6%25E4%25B9%25A0%25E7%25AE%2597%25E6%25B3%2595%2f&title=3.%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 3.机器学习算法 on facebook"
            href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2fai%2f3.%25E6%259C%25BA%25E5%2599%25A8%25E5%25AD%25A6%25E4%25B9%25A0%25E7%25AE%2597%25E6%25B3%2595%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 3.机器学习算法 on whatsapp"
            href="https://api.whatsapp.com/send?text=3.%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2fai%2f3.%25E6%259C%25BA%25E5%2599%25A8%25E5%25AD%25A6%25E4%25B9%25A0%25E7%25AE%2597%25E6%25B3%2595%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 3.机器学习算法 on telegram"
            href="https://telegram.me/share/url?text=3.%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fai%2f3.%25E6%259C%25BA%25E5%2599%25A8%25E5%25AD%25A6%25E4%25B9%25A0%25E7%25AE%2597%25E6%25B3%2595%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 3.机器学习算法 on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=3.%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95&u=http%3a%2f%2flocalhost%3a1313%2fposts%2fai%2f3.%25E6%259C%25BA%25E5%2599%25A8%25E5%25AD%25A6%25E4%25B9%25A0%25E7%25AE%2597%25E6%25B3%2595%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">王小红的笔记</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
