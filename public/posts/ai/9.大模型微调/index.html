<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>9.大模型微调 | 王小红的笔记</title>
<meta name="keywords" content="大模型应用开发">
<meta name="description" content="大语言中的大型主要体现在参数规模和训练数据量，一般参数规模达到1B(10亿)量级才叫大模型，只有达到这个量级才会有机遇Scaling law的涌现现象，涌现现象是大模型的魅力，有点像初中物理学的液态变固态。
模型的参数数量跟显存占比的计算，以OPT-6.7B举例：OPT-6.7B就是6.7Billion个参数，假设参数的类型是Float16，即每个参数占用16位（2字节）的显存。总显存占用=参数总量×每个参数的显存占用。总内存 = 67亿 * 2 = 134亿字节。转换成GB就是134亿 / 1024 / 1024 / 1024 = 12.5GB的显存。
使用大显存的GPU加载整个模型可以加快训练速度，部署时也可以提高响应速度，但是可以只使用CPU&#43;内存的方式训练或者部署，只不过这种方式的训练很慢，因为训练时需要大量的矩阵相乘操作。但是使用部署后的模型只是一个前向传播操作，CPU&#43;内存的方式不会比GPU慢很多，除非是有并发量的批量推理，GPU的优势会很明显。
现在是一个信息过载的时代，搜什么都会出现一堆，大模型工具的使用可以帮助我们筛选出有用的信息
模型分类：自回归(CAUSAL_LM，文本生成任务，比如GPT)、序列分类(SEQ_CLS，情感分析、文本分类)、token级分类(TOKEN_CLS，命名实体识别NER)、问答任务(QUESTION_ANS)
大模型应用4个阶段
1提示词工程
面向的是终端用户，大模型时代的沟通手段，通过提示词从大模型挖掘知识。就是如何通过对话框跟大模型更好交流。大模型都是概率模型，很多能力他都没有，比如数学运算，但是我们可做到通过描述让他理解。这也是为什么基于注意力机制的模型很容易回答错误一个描述很复杂的小学数学应用题。
2AI智能体（AI Agent）
基于ReAct范式，就是大模型自主判断应该使用哪些工具，比如chatGPT&#43;联网搜索。分为3类：

行动代理（Action agents）：自主决定使用工具，比如OpenAI的Function Call；
模拟代理（Simulation agents）：通常设计用于模拟角色扮演，在模拟的环境中运行，比如生成式智能体，CAMEL。以后可能会应用在游戏领域，类似于美剧西部世界；
自主智能体（Autonomous agents）：独立执行实现长期目标，比如Auto-GPT，manus（国产的，好像是Claude套壳）。

基于大模型开发应用的开发人员，比如自动客服，虚拟助手。
3大模型微调（Fine-tuning）
在预训练模型的基础上，使用较小的数据集进一步训练来调整模型参数。当有和目标相关的较小的数据集，并且希望模型在这个任务上表现更好的时候使用。
未来是面向基础模型编程。
4预训练技术（Pre-training）
使用大量的未标记数据（比如维基百科内容）来训练一个初步的模型，为后续的微调提供基础模型。适合有资源的大厂，有大量的数据集，数据清洗做的好，然后大力出奇迹。
能自己预训练模型的都是顶级大厂，因为需要的资源实在是太大了，比如LLaMA-65B就需要780G显存。
RAG
把我们从外部拿到的数据通过处理之后变成向量数据库中的知识。
微调技术路线
20年之前大家都不知道怎么去做微调，OpenAI发表了一篇论文提出调整prompt，让模型能更好的理解输入也能有很好的效果，再加上几年之后的文生图让prompt被大家熟知。
但是prompt有个缺点就是相同的prompt换一个模型或者换一个语言描述效果就会差很多，不管是在LangChain里或者在应用的对话框中都有这个问题。

全量微调（FFT）：所有系数都进行调整，原来的VC相关的模型用的多一点，训练成本高，容易造成灾难性遗忘。
高效微调（PEFT）：分为有监督微调（SFT）、基于人类反馈的强化学习（RLHF）、基于AI反馈的强化学习（RLAIF）。

PEFT
高效微调技术：Adapter Tuning(2019 Google) -&gt; Prefix Tuning(2021 Stanford) -&gt; Prompt Tuning(2021 Google) -&gt; P-Tuning V1(2021 TsingHua, MIT) -&gt; P-Tuning V2(2022 TsingHua, BAAI )
传统的模型微调是很容易的，比如分类的卷积网络中可以直接选择冻结卷积层，训练softmax层直接增加类别，2018年google的bert出来之后模型就已经不是CNN那种神经网络了，都是在叠加transformer的层数，整个模型看起来又宽又高，包括到今天的大语言模型中，哪部分参数干了哪些事也是未知的。">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/posts/ai/9.%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.08f7d74f0ada0f975d29ae436285b61ed7a719d05f350cb888d00341642995a2.css" integrity="sha256-CPfXTwraD5ddKa5DYoW2HtenGdBfNQy4iNADQWQplaI=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/ai/9.%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="/katex/katex.min.css">
<script defer src="/katex/katex.min.js"></script>
<script defer src="/katex/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: "$$", right: "$$", display: true},
        {left: "$", right: "$", display: false},
        {left: "\\(", right: "\\)", display: false},
        {left: "\\[", right: "\\]", display: true}
      ]
    });
  });
</script>

</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="王小红的笔记 (Alt + H)">王小红的笔记</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/posts/" title="笔记">
                    <span>笔记</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/categories/" title="分类">
                    <span>分类</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="标签">
                    <span>标签</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      9.大模型微调
    </h1>
    <div class="post-meta">5 min

</div>
  </header> 
  <div class="post-content"><p>大语言中的大型主要体现在参数规模和训练数据量，一般参数规模达到1B(10亿)量级才叫大模型，只有达到这个量级才会有机遇Scaling law的涌现现象，涌现现象是大模型的魅力，有点像初中物理学的液态变固态。</p>
<p>模型的参数数量跟显存占比的计算，以OPT-6.7B举例：OPT-6.7B就是6.7Billion个参数，假设参数的类型是Float16，即每个参数占用16位（2字节）的显存。总显存占用=参数总量×每个参数的显存占用。总内存 = 67亿 * 2 = 134亿字节。转换成GB就是134亿 / 1024 / 1024 / 1024 = 12.5GB的显存。</p>
<p>使用大显存的GPU加载整个模型可以加快训练速度，部署时也可以提高响应速度，但是可以只使用CPU+内存的方式训练或者部署，只不过这种方式的训练很慢，因为训练时需要大量的矩阵相乘操作。但是使用部署后的模型只是一个前向传播操作，CPU+内存的方式不会比GPU慢很多，除非是有并发量的批量推理，GPU的优势会很明显。</p>
<p><strong>现在是一个信息过载的时代，搜什么都会出现一堆，大模型工具的使用可以帮助我们筛选出有用的信息</strong></p>
<p>模型分类：自回归(<code>CAUSAL_LM</code>，文本生成任务，比如GPT)、序列分类(<code>SEQ_CLS</code>，情感分析、文本分类)、token级分类(<code>TOKEN_CLS</code>，命名实体识别NER)、问答任务(<code>QUESTION_ANS</code>)</p>
<h4 id="大模型应用4个阶段">大模型应用4个阶段<a hidden class="anchor" aria-hidden="true" href="#大模型应用4个阶段">#</a></h4>
<h5 id="1提示词工程">1提示词工程<a hidden class="anchor" aria-hidden="true" href="#1提示词工程">#</a></h5>
<p>面向的是终端用户，大模型时代的沟通手段，通过提示词从大模型挖掘知识。就是如何通过对话框跟大模型更好交流。大模型都是概率模型，很多能力他都没有，比如数学运算，但是我们可做到通过描述让他理解。这也是为什么基于注意力机制的模型很容易回答错误一个描述很复杂的小学数学应用题。</p>
<h5 id="2ai智能体ai-agent">2AI智能体（AI Agent）<a hidden class="anchor" aria-hidden="true" href="#2ai智能体ai-agent">#</a></h5>
<p>基于ReAct范式，就是大模型自主判断应该使用哪些工具，比如chatGPT+联网搜索。分为3类：</p>
<ol>
<li>行动代理（Action agents）：自主决定使用工具，比如OpenAI的Function Call；</li>
<li>模拟代理（Simulation agents）：通常设计用于模拟角色扮演，在模拟的环境中运行，比如生成式智能体，CAMEL。以后可能会应用在游戏领域，类似于美剧西部世界；</li>
<li>自主智能体（Autonomous agents）：独立执行实现长期目标，比如Auto-GPT，manus（国产的，好像是Claude套壳）。</li>
</ol>
<p>基于大模型开发应用的开发人员，比如自动客服，虚拟助手。</p>
<h5 id="3大模型微调fine-tuning">3大模型微调（Fine-tuning）<a hidden class="anchor" aria-hidden="true" href="#3大模型微调fine-tuning">#</a></h5>
<p>在预训练模型的基础上，使用较小的数据集进一步训练来调整模型参数。当有和目标相关的较小的数据集，并且希望模型在这个任务上表现更好的时候使用。</p>
<p>未来是面向基础模型编程。</p>
<h5 id="4预训练技术pre-training">4预训练技术（Pre-training）<a hidden class="anchor" aria-hidden="true" href="#4预训练技术pre-training">#</a></h5>
<p>使用大量的未标记数据（比如维基百科内容）来训练一个初步的模型，为后续的微调提供基础模型。适合有资源的大厂，有大量的数据集，数据清洗做的好，然后大力出奇迹。</p>
<p>能自己预训练模型的都是顶级大厂，因为需要的资源实在是太大了，比如LLaMA-65B就需要780G显存。</p>
<h4 id="rag">RAG<a hidden class="anchor" aria-hidden="true" href="#rag">#</a></h4>
<p>把我们从外部拿到的数据通过处理之后变成向量数据库中的知识。</p>
<h3 id="微调技术路线">微调技术路线<a hidden class="anchor" aria-hidden="true" href="#微调技术路线">#</a></h3>
<p>20年之前大家都不知道怎么去做微调，OpenAI发表了一篇论文提出调整prompt，让模型能更好的理解输入也能有很好的效果，再加上几年之后的文生图让prompt被大家熟知。</p>
<p>但是prompt有个缺点就是相同的prompt换一个模型或者换一个语言描述效果就会差很多，不管是在LangChain里或者在应用的对话框中都有这个问题。</p>
<ol>
<li>全量微调（FFT）：所有系数都进行调整，原来的VC相关的模型用的多一点，训练成本高，容易造成灾难性遗忘。</li>
<li>高效微调（PEFT）：分为有监督微调（SFT）、基于人类反馈的强化学习（RLHF）、基于AI反馈的强化学习（RLAIF）。</li>
</ol>
<h4 id="peft">PEFT<a hidden class="anchor" aria-hidden="true" href="#peft">#</a></h4>
<p>高效微调技术：Adapter Tuning(2019 Google) -&gt; Prefix Tuning(2021 Stanford) -&gt; Prompt Tuning(2021 Google) -&gt; P-Tuning V1(2021 TsingHua, MIT) -&gt; P-Tuning V2(2022 TsingHua, BAAI )</p>
<p>传统的模型微调是很容易的，比如分类的卷积网络中可以直接选择冻结卷积层，训练softmax层直接增加类别，2018年google的bert出来之后模型就已经不是CNN那种神经网络了，都是在叠加transformer的层数，整个模型看起来又宽又高，包括到今天的大语言模型中，哪部分参数干了哪些事也是未知的。</p>
<h5 id="adapter-tuning">Adapter Tuning<a hidden class="anchor" aria-hidden="true" href="#adapter-tuning">#</a></h5>
<p>bert是先做了一个语义理解能力很强的模型，然后在下游的具体任务上逐个的去微调模型，这中做法的成本相当高，做微调时需要把整个bert都加载到显存中。19年的时候google提出在transformer的Feed-forward layer层后增加一个adapter层，adapter层负责将高维向量转成低维，计算后在转回高维，只训练降维后的特征提升效率。</p>
<h5 id="prefix-tuning">Prefix Tuning<a hidden class="anchor" aria-hidden="true" href="#prefix-tuning">#</a></h5>
<p>21年的时候斯坦福大学发表了一篇论文，Prefix Tuning，这个是在整个transformer前增加一个Prefix模块，仅训练Prefix，冻结Transformer全部参数，这样可以不将整个模型都加载到显存中，可以降低算力和训练时间。</p>
<p>上面的微调都是有几类任务就需要微调几次，很麻烦。</p>
<h5 id="prompt-tuning">Prompt Tuning<a hidden class="anchor" aria-hidden="true" href="#prompt-tuning">#</a></h5>
<p>相当于是Prefix Tuning的简化版，只在输入层增加一些token，让模型能够更好的理解用户的输入。解决了Prefix Tuning实现困难的问题，更容易训练，而且效果很接近Prefix Tuning。相当于在模型之前增加一个新的模型，并且不需要区分任务种类，只需要增加一个通用的就可以。</p>
<h5 id="p-tuning-v1">P-Tuning V1<a hidden class="anchor" aria-hidden="true" href="#p-tuning-v1">#</a></h5>
<p>清华唐杰团队、杨志林和MIT一起发表的论文，优化了冻结模型的数据可能会导致模型过拟合的问题，他和Prefix Tuning的区别是P-Tuning更灵活，只在Input embedding层增加一些参数，且使用长短记忆网络加MLP进行初始化。</p>
<h5 id="p-tuning-v2">P-Tuning V2<a hidden class="anchor" aria-hidden="true" href="#p-tuning-v2">#</a></h5>
<p>Prompt Tuning和P-Tuning V1在模型参数较小的情况下表显不好，V2改善了一下。</p>
<p>上面的方法都没法兼顾高效和高质量，比如Adapter会增加模型深度，训练成本高，其他的会生成额外的token，会减少我们输入的token长度。</p>
<h4 id="lora">LoRA<a hidden class="anchor" aria-hidden="true" href="#lora">#</a></h4>
<p>低秩适配技术：LoRA(2021 微软) -&gt; QLoRA(2023 华盛顿大学) -&gt; AdaLoRA(2023 微软)</p>
<p>通过低秩分解将权重更新表示为两个较小的矩阵（更新矩阵），这些新矩阵可以在适应新数据的同时保持整体变化数量较少进行训练，原始矩阵保持冻结状态，并且不接受任何调整。最终结果通过原始权重和适应后的权重组合得到。</p>
<p>实际上是在原始的预训练模型旁增加一个附加的网络通路，可以看成外挂了矩阵A和矩阵B相乘来模拟征秩。</p>
<img src="https://raw.githubusercontent.com/wangxiaohong123/p-bed/main/uPic/LoRA原理.png" alt="LoRA原理" style="zoom:25%;" />
<p>假设模型某一层的矩阵大小是d_in × d_out，那么lora外挂的2个矩阵就是d_in × r和r × d_out，这个r就是秩。</p>
<p>适配时会将模型的原始权重+lora的适配矩阵，$ W_{final}=W_{pretrained}+\frac{α}{r}⋅(A×B) $，α是一个缩放因子，通过α和r控制低秩矩阵对原始权重的影响，α越大影响越大(微调越激进)。</p>
<p>相比PEFT的优势</p>
<ol>
<li>更少的可训练参数：LoRA通常只需要训练非常少的额外参数，相较于全参数微调或者某些PEFT方法，可以极大地减少需要更新的参数数量。</li>
<li>更高的训练效率：由于减少了需要更新的参数，LoRA能够更快地收敛，并且对计算资源的需求更低。</li>
<li>无推理延迟：LoRA不会增加推理时的延迟，因为低秩矩阵可以在运行时动态地与原模型权重相加或相乘，无需改变模型结构或重新存储整个模型。</li>
<li>灵活的模块化适应：LoRA允许创建轻量级、特定任务的适配器，这些适配器可以在不修改基础模型架构的情况下进行互换，便于多任务学习和任务切换。</li>
<li>稳健的知识保留：通过冻结预训练权重，LoRA有助于减轻灾难性遗忘的问题，即在学习新任务时丢失之前学到的知识。</li>
</ol>
<h5 id="adalora">AdaLoRA<a hidden class="anchor" aria-hidden="true" href="#adalora">#</a></h5>
<p>使用SVD提升矩阵低秩分解性能，动态调整不同权重矩阵的本征秩r。</p>
<p>SVD（奇异值分解）是一种矩阵分解技术，可以将任意一个矩阵分解为：左奇异向量矩阵、奇异值矩阵和右奇异向量矩阵。可以实现降维、数据压缩、噪声过滤、提取文本数据的潜在语义结构等。</p>
<h5 id="qlora">QLoRA<a hidden class="anchor" aria-hidden="true" href="#qlora">#</a></h5>
<p>在LoRA的基础上增加了量化，引入了 <strong>NormalFloat（NF4）</strong>，这是一种基于统计分布设计的非对称、4-bit 数据类型。NF4 能更好地适应权重的分布特性，在精度损失最小的前提下实现更高的压缩率。</p>
<h3 id="hugging-face-transformers使用">Hugging Face Transformers使用<a hidden class="anchor" aria-hidden="true" href="#hugging-face-transformers使用">#</a></h3>
<p>开发环境：miniconda，python3.11</p>
<p>Transformers库提供了几千个预训练模型，并且对Jax、PyTorch、TensorFlow等支持很友好，活跃度非常高。</p>
<h4 id="pipelines库">pipelines库<a hidden class="anchor" aria-hidden="true" href="#pipelines库">#</a></h4>
<p>通过pipelines可以让我们很方便的调用现成的模型。他是先将输入变成模型能理解的向量，向量可以解决不同的语言的词表达相同意思的问题。转成向量之后交给模型，在经过一个后处理输出结果，不同的模型后处理不同。</p>
<p><img alt="transformers-pipeline" loading="lazy" src="https://raw.githubusercontent.com/wangxiaohong123/p-bed/main/uPic/transformers-pipeline.jpg"></p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">from</span> transformers <span style="color:#ff79c6">import</span> AutoTokenizer, AutoModel
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> transformers <span style="color:#ff79c6">import</span> pipeline
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 手动拉取tokenizer和模型</span>
</span></span><span style="display:flex;"><span>tokenizer <span style="color:#ff79c6">=</span> AutoTokenizer<span style="color:#ff79c6">.</span>from_pretrained(<span style="color:#f1fa8c">&#34;bert-base-chinese&#34;</span>)
</span></span><span style="display:flex;"><span>model <span style="color:#ff79c6">=</span> AutoModel<span style="color:#ff79c6">.</span>from_pretrained(<span style="color:#f1fa8c">&#34;bert-base-chinese&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 保存修改的tokenizer和模型</span>
</span></span><span style="display:flex;"><span>tokenizer<span style="color:#ff79c6">.</span>save_pretrained(<span style="color:#f1fa8c">&#34;路径&#34;</span>)
</span></span><span style="display:flex;"><span>model<span style="color:#ff79c6">.</span>save_pretrained(<span style="color:#f1fa8c">&#34;路径&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 使用pipeline调用gpt2模型</span>
</span></span><span style="display:flex;"><span>prompt <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;吧啦吧啦一段话&#34;</span>
</span></span><span style="display:flex;"><span>generator <span style="color:#ff79c6">=</span> pipeline(task<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;text-generator&#34;</span>, model<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;gpt2&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 生成2句回应，每个句子最大长度为16</span>
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">print</span>(generator(prompt, num_return_sequences<span style="color:#ff79c6">=</span><span style="color:#bd93f9">2</span>, max_length<span style="color:#ff79c6">=</span><span style="color:#bd93f9">16</span>))
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="datasets库">datasets库<a hidden class="anchor" aria-hidden="true" href="#datasets库">#</a></h4>
<p>用于快速加载、处理的公共数据集。</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">from</span> datasets <span style="color:#ff79c6">import</span> load_dataset
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> torch.utils.data <span style="color:#ff79c6">import</span> DataLoader
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 加载GLUE中的SST-2情感分析任务数据集</span>
</span></span><span style="display:flex;"><span>dataset <span style="color:#ff79c6">=</span> load_dataset(<span style="color:#f1fa8c">&#34;glue&#34;</span>, <span style="color:#f1fa8c">&#34;sst2&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 加载本地 CSV 文件</span>
</span></span><span style="display:flex;"><span>dataset <span style="color:#ff79c6">=</span> load_dataset(<span style="color:#f1fa8c">&#34;csv&#34;</span>, data_files<span style="color:#ff79c6">=</span>{<span style="color:#f1fa8c">&#34;train&#34;</span>: <span style="color:#f1fa8c">&#34;data/train.csv&#34;</span>, <span style="color:#f1fa8c">&#34;test&#34;</span>: <span style="color:#f1fa8c">&#34;data/test.csv&#34;</span>})
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 查看训练集</span>
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">print</span>(dataset[<span style="color:#f1fa8c">&#34;train&#34;</span>][:<span style="color:#bd93f9">5</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 转换为 PyTorch Dataset</span>
</span></span><span style="display:flex;"><span>train_dataloader <span style="color:#ff79c6">=</span> DataLoader(small_train_dataset, shuffle<span style="color:#ff79c6">=</span><span style="color:#ff79c6">True</span>, batch_size<span style="color:#ff79c6">=</span><span style="color:#bd93f9">8</span>)
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="trainer库">Trainer库<a hidden class="anchor" aria-hidden="true" href="#trainer库">#</a></h4>
<p>封装了完整训练循环的高级类，可以简化模型训练、评估、预测等流程。</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">from</span> transformers <span style="color:#ff79c6">import</span> TrainingArguments, Trainer
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 训练</span>
</span></span><span style="display:flex;"><span>trainer <span style="color:#ff79c6">=</span> Trainer(
</span></span><span style="display:flex;"><span>    model<span style="color:#ff79c6">=</span>model,
</span></span><span style="display:flex;"><span>    args<span style="color:#ff79c6">=</span>training_args,
</span></span><span style="display:flex;"><span>    train_dataset<span style="color:#ff79c6">=</span>small_train_dataset,
</span></span><span style="display:flex;"><span>    eval_dataset<span style="color:#ff79c6">=</span>small_eval_dataset,
</span></span><span style="display:flex;"><span>    compute_metrics<span style="color:#ff79c6">=</span>compute_metrics,
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>trainer<span style="color:#ff79c6">.</span>train()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 保存模型，后续可以通过 from_pretrained() 方法重新加载</span>
</span></span><span style="display:flex;"><span>trainer<span style="color:#ff79c6">.</span>save_model(model_dir)
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 保存训练状态，一般和try-except结合，防止训练中断</span>
</span></span><span style="display:flex;"><span>trainer<span style="color:#ff79c6">.</span>save_state()
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="evaluate库">evaluate库<a hidden class="anchor" aria-hidden="true" href="#evaluate库">#</a></h4>
<p>提供一系列标准评估指标，如准确率、F1、BLEU、ROUGE 等。</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">import</span> evaluate
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">import</span> numpy <span style="color:#ff79c6">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 加载评估指标-准确率</span>
</span></span><span style="display:flex;"><span>accuracy <span style="color:#ff79c6">=</span> evaluate<span style="color:#ff79c6">.</span>load(<span style="color:#f1fa8c">&#34;accuracy&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 计算准确率</span>
</span></span><span style="display:flex;"><span>labels <span style="color:#ff79c6">=</span> small_eval_dataset[:][<span style="color:#f1fa8c">&#34;label&#34;</span>]
</span></span><span style="display:flex;"><span>preds <span style="color:#ff79c6">=</span> np<span style="color:#ff79c6">.</span>argmax(predictions<span style="color:#ff79c6">.</span>predictions, axis<span style="color:#ff79c6">=-</span><span style="color:#bd93f9">1</span>)
</span></span><span style="display:flex;"><span>acc <span style="color:#ff79c6">=</span> accuracy<span style="color:#ff79c6">.</span>compute(predictions<span style="color:#ff79c6">=</span>preds, references<span style="color:#ff79c6">=</span>labels)
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">f</span><span style="color:#f1fa8c">&#34;Accuracy: </span><span style="color:#f1fa8c">{</span>acc[<span style="color:#f1fa8c">&#39;accuracy&#39;</span>]<span style="color:#f1fa8c">}</span><span style="color:#f1fa8c">&#34;</span>)
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="peft库">PEFT库<a hidden class="anchor" aria-hidden="true" href="#peft库">#</a></h4>
<p>提供微调支持。</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">from</span> peft <span style="color:#ff79c6">import</span> LoraConfig, PeftModel, PeftConfig, TaskType, get_peft_model
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>peft_config <span style="color:#ff79c6">=</span> LoraConfig(
</span></span><span style="display:flex;"><span>    task_type<span style="color:#ff79c6">=</span>TaskType<span style="color:#ff79c6">.</span>SEQ_CLS,   <span style="color:#6272a4"># 任务类型：序列分类</span>
</span></span><span style="display:flex;"><span>    inference_mode<span style="color:#ff79c6">=</span><span style="color:#ff79c6">False</span>,         <span style="color:#6272a4"># 训练模式</span>
</span></span><span style="display:flex;"><span>    r<span style="color:#ff79c6">=</span><span style="color:#bd93f9">8</span>,                          <span style="color:#6272a4"># LoRA秩</span>
</span></span><span style="display:flex;"><span>    lora_alpha<span style="color:#ff79c6">=</span><span style="color:#bd93f9">16</span>,                <span style="color:#6272a4"># 缩放因子</span>
</span></span><span style="display:flex;"><span>    lora_dropout<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0.1</span>              <span style="color:#6272a4"># dropout率</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 将 PEFT 层添加到原始模型中</span>
</span></span><span style="display:flex;"><span>model <span style="color:#ff79c6">=</span> get_peft_model(model, peft_config)
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="transformers微调demo">Transformers微调demo<a hidden class="anchor" aria-hidden="true" href="#transformers微调demo">#</a></h4>
<h5 id="文本分类">文本分类<a hidden class="anchor" aria-hidden="true" href="#文本分类">#</a></h5>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 30
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 31
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 32
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 33
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 34
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 35
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 36
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 37
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 38
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 39
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 40
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 41
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 42
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 43
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 44
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 45
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 46
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 47
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 48
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 49
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 50
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 51
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 52
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 53
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 54
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 55
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 56
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 57
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 58
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 59
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 60
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 61
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 62
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 63
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 64
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 65
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 66
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 67
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 68
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 69
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 70
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 71
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 72
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 73
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 74
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 75
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 76
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 77
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 78
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 79
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 80
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 81
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 82
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 83
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 84
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 85
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 86
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 87
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 88
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 89
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 90
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 91
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 92
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 93
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 94
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 95
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 96
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 97
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 98
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 99
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">100
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">101
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">102
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">103
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">104
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">105
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">106
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">107
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">108
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">109
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">110
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">111
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">112
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">113
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">114
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">from</span> transformers <span style="color:#ff79c6">import</span> AutoTokenizer
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> datasets <span style="color:#ff79c6">import</span> load_dataset
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">import</span> random
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">import</span> pandas <span style="color:#ff79c6">as</span> pd
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">import</span> datasets
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> transformers <span style="color:#ff79c6">import</span> AutoModelForSequenceClassification
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">import</span> numpy <span style="color:#ff79c6">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">import</span> evaluate
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> transformers <span style="color:#ff79c6">import</span> TrainingArguments, Trainer
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">def</span> <span style="color:#50fa7b">show_random_elements</span>(dataset, num_examples<span style="color:#ff79c6">=</span><span style="color:#bd93f9">10</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#f1fa8c">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">    查看数据集
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">    :param dataset: 目标数据集
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">    :param num_examples: 输出条数
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">    :return:
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">assert</span> num_examples <span style="color:#ff79c6">&lt;=</span> <span style="color:#8be9fd;font-style:italic">len</span>(dataset), <span style="color:#f1fa8c">f</span><span style="color:#f1fa8c">&#34;数据集长度 </span><span style="color:#f1fa8c">{</span><span style="color:#8be9fd;font-style:italic">len</span>(dataset)<span style="color:#f1fa8c">}</span><span style="color:#f1fa8c"> 小于 num_examples </span><span style="color:#f1fa8c">{</span>num_examples<span style="color:#f1fa8c">}</span><span style="color:#f1fa8c">&#34;</span>
</span></span><span style="display:flex;"><span>    picks <span style="color:#ff79c6">=</span> []
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">for</span> _ <span style="color:#ff79c6">in</span> <span style="color:#8be9fd;font-style:italic">range</span>(num_examples):
</span></span><span style="display:flex;"><span>        pick <span style="color:#ff79c6">=</span> random<span style="color:#ff79c6">.</span>randint(<span style="color:#bd93f9">0</span>, <span style="color:#8be9fd;font-style:italic">len</span>(dataset) <span style="color:#ff79c6">-</span> <span style="color:#bd93f9">1</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#ff79c6">while</span> pick <span style="color:#ff79c6">in</span> picks:
</span></span><span style="display:flex;"><span>            pick <span style="color:#ff79c6">=</span> random<span style="color:#ff79c6">.</span>randint(<span style="color:#bd93f9">0</span>, <span style="color:#8be9fd;font-style:italic">len</span>(dataset) <span style="color:#ff79c6">-</span> <span style="color:#bd93f9">1</span>)
</span></span><span style="display:flex;"><span>        picks<span style="color:#ff79c6">.</span>append(pick)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    df <span style="color:#ff79c6">=</span> pd<span style="color:#ff79c6">.</span>DataFrame(dataset[picks])
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">for</span> column, typ <span style="color:#ff79c6">in</span> dataset<span style="color:#ff79c6">.</span>featrues<span style="color:#ff79c6">.</span>items():
</span></span><span style="display:flex;"><span>        <span style="color:#ff79c6">if</span> <span style="color:#8be9fd;font-style:italic">isinstance</span>(typ, datasets<span style="color:#ff79c6">.</span>ClassLabel):
</span></span><span style="display:flex;"><span>            df[column] <span style="color:#ff79c6">=</span> df[column]<span style="color:#ff79c6">.</span>transform(<span style="color:#ff79c6">lambda</span> i: typ<span style="color:#ff79c6">.</span>names[i])
</span></span><span style="display:flex;"><span>    <span style="color:#8be9fd;font-style:italic">print</span>(df)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 下载数据集</span>
</span></span><span style="display:flex;"><span>dataset <span style="color:#ff79c6">=</span> load_dataset(<span style="color:#f1fa8c">&#34;yelp_review_full&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 查看下载的数据集格式</span>
</span></span><span style="display:flex;"><span>show_random_elements(dataset[<span style="color:#f1fa8c">&#34;train&#34;</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 数据预处理</span>
</span></span><span style="display:flex;"><span>tokenizer <span style="color:#ff79c6">=</span> AutoTokenizer<span style="color:#ff79c6">.</span>from_pretrained(<span style="color:#f1fa8c">&#34;bert-base-cased&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">def</span> <span style="color:#50fa7b">tokenize_function</span>(examples):
</span></span><span style="display:flex;"><span>    <span style="color:#f1fa8c">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">    对超过长度的文本进行截断,长度不够的进行填充
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">return</span> tokenizer(examples[<span style="color:#f1fa8c">&#34;text&#34;</span>], padding<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;max_length&#34;</span>, truncation<span style="color:#ff79c6">=</span><span style="color:#ff79c6">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># datasets的map方法支持在整个数据集上应用预处理函数</span>
</span></span><span style="display:flex;"><span>tokenized_datasets <span style="color:#ff79c6">=</span> dataset<span style="color:#ff79c6">.</span>map(tokenize_function, batched<span style="color:#ff79c6">=</span><span style="color:#ff79c6">True</span>)
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 删除原始文本，节省内存</span>
</span></span><span style="display:flex;"><span>tokenized_datasets <span style="color:#ff79c6">=</span> tokenized_datasets<span style="color:#ff79c6">.</span>remove_columns([<span style="color:#f1fa8c">&#34;text&#34;</span>])
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 查看预处理后的数据集</span>
</span></span><span style="display:flex;"><span>show_random_elements(tokenized_datasets[<span style="color:#f1fa8c">&#34;train&#34;</span>], num_examples<span style="color:#ff79c6">=</span><span style="color:#bd93f9">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 数据抽样，全量跑的时间太长，测试的时候就取一点</span>
</span></span><span style="display:flex;"><span>small_train_dataset <span style="color:#ff79c6">=</span> tokenized_datasets[<span style="color:#f1fa8c">&#34;train&#34;</span>]<span style="color:#ff79c6">.</span>shuffle(seed<span style="color:#ff79c6">=</span><span style="color:#bd93f9">42</span>)<span style="color:#ff79c6">.</span>select(<span style="color:#8be9fd;font-style:italic">range</span>(<span style="color:#bd93f9">1000</span>))
</span></span><span style="display:flex;"><span>small_eval_dataset <span style="color:#ff79c6">=</span> tokenized_datasets[<span style="color:#f1fa8c">&#34;test&#34;</span>]<span style="color:#ff79c6">.</span>shuffle(seed<span style="color:#ff79c6">=</span><span style="color:#bd93f9">42</span>)<span style="color:#ff79c6">.</span>select(<span style="color:#8be9fd;font-style:italic">range</span>(<span style="color:#bd93f9">1000</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 加载模型,数据集是5分类，所以这里指定num_labels也是5</span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># AutoModelForSequenceClassification 是专门为序列分类任务设计的,相对于AutoModel，AutoModelForSequenceClassification多了一些处理，比如分类操作，将概率转化为标签</span>
</span></span><span style="display:flex;"><span>model <span style="color:#ff79c6">=</span> AutoModelForSequenceClassification<span style="color:#ff79c6">.</span>from_pretrained(<span style="color:#f1fa8c">&#34;bert-base-cased&#34;</span>, num_labels<span style="color:#ff79c6">=</span><span style="color:#bd93f9">5</span>)
</span></span><span style="display:flex;"><span>model_dir <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;models/bert-base-cased-finetune-yelp&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># evaluation_strategy是评估策略，每完成1个epoch（遍历一次整个训练集）进行一次评估</span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># batch size是16，如果是单核GPU的话就是每次向模型输入16条数据；epoch是3，整个数据集会被训练3次</span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># logging_steps 默认值为500，根据我们的训练数据和步长，将其设置为每30步记录一次日志，比如只有1核GPU，就是每训练30 * 16条数据后记录一次日志</span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 完整的参数：https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/trainer#transformers.TrainingArguments</span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 配置在源码中的定义：https://github.com/huggingface/transformers/blob/v4.36.1/src/transformers/training_args.py#L161</span>
</span></span><span style="display:flex;"><span>training_args <span style="color:#ff79c6">=</span> TrainingArguments(output_dir<span style="color:#ff79c6">=</span>model_dir,
</span></span><span style="display:flex;"><span>                                  eval_strategy<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;epoch&#34;</span>,
</span></span><span style="display:flex;"><span>                                  per_device_train_batch_size<span style="color:#ff79c6">=</span><span style="color:#bd93f9">16</span>,
</span></span><span style="display:flex;"><span>                                  num_train_epochs<span style="color:#ff79c6">=</span><span style="color:#bd93f9">3</span>,
</span></span><span style="display:flex;"><span>                                  logging_steps<span style="color:#ff79c6">=</span><span style="color:#bd93f9">30</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 简单的加载一个准确率指标</span>
</span></span><span style="display:flex;"><span>metric <span style="color:#ff79c6">=</span> evaluate<span style="color:#ff79c6">.</span>load(<span style="color:#f1fa8c">&#34;accuracy&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">def</span> <span style="color:#50fa7b">compute_metrics</span>(eval_pred):
</span></span><span style="display:flex;"><span>    <span style="color:#f1fa8c">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">    统计模型能力，这里只根据准确率评估
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">    :param eval_pred: 模型输出
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">    :return: 概率最高的标签
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#6272a4"># labels是实际的标签</span>
</span></span><span style="display:flex;"><span>    <span style="color:#6272a4"># logits是2维数组，行数是batch size，列数是分类大小，就是说每行都是对一个样本的所有标签的分数预测</span>
</span></span><span style="display:flex;"><span>    logits, labels <span style="color:#ff79c6">=</span> eval_pred
</span></span><span style="display:flex;"><span>    <span style="color:#6272a4"># 获取样本得分最高的标签，就是模型预测的标签</span>
</span></span><span style="display:flex;"><span>    predictions <span style="color:#ff79c6">=</span> np<span style="color:#ff79c6">.</span>argmax(logits, axis<span style="color:#ff79c6">=-</span><span style="color:#bd93f9">1</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#6272a4"># 使用模型预测的标签和真实标签比较，得到准确率</span>
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">return</span> metric<span style="color:#ff79c6">.</span>compute(predictions<span style="color:#ff79c6">=</span>predictions, references<span style="color:#ff79c6">=</span>labels)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 开始训练</span>
</span></span><span style="display:flex;"><span>trainer <span style="color:#ff79c6">=</span> Trainer(
</span></span><span style="display:flex;"><span>    model<span style="color:#ff79c6">=</span>model,
</span></span><span style="display:flex;"><span>    args<span style="color:#ff79c6">=</span>training_args,
</span></span><span style="display:flex;"><span>    train_dataset<span style="color:#ff79c6">=</span>small_train_dataset,
</span></span><span style="display:flex;"><span>    eval_dataset<span style="color:#ff79c6">=</span>small_eval_dataset,
</span></span><span style="display:flex;"><span>    compute_metrics<span style="color:#ff79c6">=</span>compute_metrics,
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>trainer<span style="color:#ff79c6">.</span>train()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 保存模型，后续可以通过 from_pretrained() 方法重新加载</span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 训练完就要保存，要不后面抛异常就白训练了</span>
</span></span><span style="display:flex;"><span>trainer<span style="color:#ff79c6">.</span>save_model(model_dir)
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 保存训练状态，一般和try-except结合，防止训练中断</span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># trainer.save_state()</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>small_test_dataset <span style="color:#ff79c6">=</span> tokenized_datasets[<span style="color:#f1fa8c">&#34;test&#34;</span>]<span style="color:#ff79c6">.</span>shuffle(seed<span style="color:#ff79c6">=</span><span style="color:#bd93f9">64</span>)<span style="color:#ff79c6">.</span>select(<span style="color:#8be9fd;font-style:italic">range</span>(<span style="color:#bd93f9">100</span>))
</span></span><span style="display:flex;"><span>test_results <span style="color:#ff79c6">=</span> trainer<span style="color:#ff79c6">.</span>evaluate(small_test_dataset)
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">f</span><span style="color:#f1fa8c">&#34;Test Accuracy: </span><span style="color:#f1fa8c">{</span>test_results[<span style="color:#f1fa8c">&#39;eval_accuracy&#39;</span>]<span style="color:#f1fa8c">:</span><span style="color:#f1fa8c">.4f</span><span style="color:#f1fa8c">}</span><span style="color:#f1fa8c">&#34;</span>)
</span></span></code></pre></td></tr></table>
</div>
</div><h5 id="量化">量化<a hidden class="anchor" aria-hidden="true" href="#量化">#</a></h5>
<p>量化就是对模型中系数（权重、激活值等）的压缩，用较少的信息表示数据，同时尽量不降低太多准确性。量化主要分2种：</p>
<ol>
<li>后训练量化（PTQ）：在模型训练后降低系数的精度，就好比打印东西，本来是10页，为了省纸改小字体变成5页了。没有降低训练成本，可以节省部署资源。GPTQ、AWQ、BitsAndBytes(BNB)等。</li>
<li>量化感知训练（QAT）：训练时就考虑量化，效果更好，并且微调时训练成本更低，PyTorch Quantization Toolkit、NVIDIA TensorRT等。</li>
</ol>
<p>当权重很大时，训练过程中矩阵相乘后产生的小数位会不断加长，这个时候量化后的精度过低就会导致精度丢失，也是梯度消失，所以模型的系数越大，参数的数据类型也越大。</p>
<p><strong>如果需要加载量化后的模型(比如bitsandbytes)必须要在Intel CPU上</strong>。</p>
<h5 id="lora微调demo">LoRA微调demo<a hidden class="anchor" aria-hidden="true" href="#lora微调demo">#</a></h5>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">36
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">37
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">38
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">39
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">40
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">41
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">42
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">43
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">44
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">45
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">46
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">47
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">48
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">49
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">50
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">51
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">52
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">53
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">54
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">55
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">56
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">57
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">58
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">59
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">60
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> transformers <span style="color:#ff79c6">import</span> GPT2Tokenizer, AutoConfig, AutoModelForCausalLM, TrainingArguments, Trainer
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> peft <span style="color:#ff79c6">import</span> LoraConfig, get_peft_model
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> datasets <span style="color:#ff79c6">import</span> load_dataset, ClassLabel, Sequence
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model_id <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;facebook/opt-125m&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 加载模型</span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 如果要加载量化后的模型可以使用model = AutoModelForCausalLM.from_pretrained(model_id, load_8bit=True)</span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 这种写法会使用bitsandbytes，但是必须要在Intel GPU基础上，下面的代码使用torch_dtype=torch.float16也可以减少内存的占用</span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 目前bitsandbytes要比quanto或者pytorch自带的量化工具效率高</span>
</span></span><span style="display:flex;"><span>model <span style="color:#ff79c6">=</span> AutoModelForCausalLM<span style="color:#ff79c6">.</span>from_pretrained(model_id, torch_dtype<span style="color:#ff79c6">=</span>torch<span style="color:#ff79c6">.</span>float16, device_map<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;cpu&#34;</span>)
</span></span><span style="display:flex;"><span>tokenizer <span style="color:#ff79c6">=</span> GPT2Tokenizer<span style="color:#ff79c6">.</span>from_pretrained(model_id)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 配置 LoRA</span>
</span></span><span style="display:flex;"><span>peft_config <span style="color:#ff79c6">=</span> LoraConfig(
</span></span><span style="display:flex;"><span>    r<span style="color:#ff79c6">=</span><span style="color:#bd93f9">8</span>, <span style="color:#6272a4"># LoRA的秩</span>
</span></span><span style="display:flex;"><span>    lora_alpha<span style="color:#ff79c6">=</span><span style="color:#bd93f9">16</span>, <span style="color:#6272a4"># 适应的比例因子</span>
</span></span><span style="display:flex;"><span>    target_modules<span style="color:#ff79c6">=</span>[<span style="color:#f1fa8c">&#34;q_proj&#34;</span>, <span style="color:#f1fa8c">&#34;v_proj&#34;</span>], <span style="color:#6272a4"># 对哪些模块加 LoRA，可选的还有k_proj、out_proj</span>
</span></span><span style="display:flex;"><span>    lora_dropout<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0.1</span>, <span style="color:#6272a4"># 随机失效的神经元概率，防止过拟合，仅作用于lora的适配矩阵</span>
</span></span><span style="display:flex;"><span>    bias<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;none&#34;</span>, <span style="color:#6272a4"># 不使用偏置，可选&#34;all&#34;、&#34;lora_only&#34;</span>
</span></span><span style="display:flex;"><span>    task_type<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;CAUSAL_LM&#34;</span> <span style="color:#6272a4"># 表示这是因果(自回归)语言模型任务</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 自动包装模型，适配 LoRA（无需 prepare_model_for_int8_training）</span>
</span></span><span style="display:flex;"><span>model <span style="color:#ff79c6">=</span> get_peft_model(model, peft_config)
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 看一下模型大小</span>
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">f</span><span style="color:#f1fa8c">&#34;</span><span style="color:#f1fa8c">{</span>model<span style="color:#ff79c6">.</span>get_memory_footprint() <span style="color:#ff79c6">/</span> (<span style="color:#bd93f9">1024</span> <span style="color:#ff79c6">**</span> <span style="color:#bd93f9">3</span>)<span style="color:#f1fa8c">:</span><span style="color:#f1fa8c">.2f</span><span style="color:#f1fa8c">}</span><span style="color:#f1fa8c">GB&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 数据集处理</span>
</span></span><span style="display:flex;"><span>dataset <span style="color:#ff79c6">=</span> load_dataset(<span style="color:#f1fa8c">&#34;Abirate/english_quotes&#34;</span>)
</span></span><span style="display:flex;"><span>tokenized_dataset <span style="color:#ff79c6">=</span> dataset<span style="color:#ff79c6">.</span>map(<span style="color:#ff79c6">lambda</span> samples: tokenizer(samples[<span style="color:#f1fa8c">&#34;quote&#34;</span>]), batched<span style="color:#ff79c6">=</span><span style="color:#ff79c6">True</span>)
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> transformers <span style="color:#ff79c6">import</span> DataCollatorForLanguageModeling
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 数据集收集器，处理语言模型数据，mlm设置为不使用掩码语言模型</span>
</span></span><span style="display:flex;"><span>data_collator <span style="color:#ff79c6">=</span> DataCollatorForLanguageModeling(tokenizer, mlm<span style="color:#ff79c6">=</span><span style="color:#ff79c6">False</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 微调</span>
</span></span><span style="display:flex;"><span>model_dir <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;models&#34;</span>
</span></span><span style="display:flex;"><span>training_args <span style="color:#ff79c6">=</span> TrainingArguments(output_dir<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">f</span><span style="color:#f1fa8c">&#34;</span><span style="color:#f1fa8c">{</span>model_dir<span style="color:#f1fa8c">}</span><span style="color:#f1fa8c">/</span><span style="color:#f1fa8c">{</span>model_id<span style="color:#f1fa8c">}</span><span style="color:#f1fa8c">-lora&#34;</span>,
</span></span><span style="display:flex;"><span>                                  learning_rate<span style="color:#ff79c6">=</span><span style="color:#bd93f9">2e-4</span>, <span style="color:#6272a4"># 学习率，定义梯度下降更新参数时的步长</span>
</span></span><span style="display:flex;"><span>                                  per_device_train_batch_size<span style="color:#ff79c6">=</span><span style="color:#bd93f9">16</span>,
</span></span><span style="display:flex;"><span>                                  fp16<span style="color:#ff79c6">=</span><span style="color:#ff79c6">True</span>, <span style="color:#6272a4"># 使用混合精度模型，上面设置了存的时候使用int8精度，但是计算的时候还是使用f16</span>
</span></span><span style="display:flex;"><span>                                  max_steps<span style="color:#ff79c6">=</span><span style="color:#bd93f9">2</span>, <span style="color:#6272a4"># 最大训练步长</span>
</span></span><span style="display:flex;"><span>                                  logging_steps<span style="color:#ff79c6">=</span><span style="color:#bd93f9">30</span>,
</span></span><span style="display:flex;"><span>                                 save_safetensors<span style="color:#ff79c6">=</span><span style="color:#ff79c6">False</span>) <span style="color:#6272a4"># 有些模型会出现多个模块复用权重，训练时重复保存相同的权重会报错，需要把save_safetensors关掉</span>
</span></span><span style="display:flex;"><span>trainer <span style="color:#ff79c6">=</span> Trainer(
</span></span><span style="display:flex;"><span>    model<span style="color:#ff79c6">=</span>model,
</span></span><span style="display:flex;"><span>    args<span style="color:#ff79c6">=</span>training_args,
</span></span><span style="display:flex;"><span>    train_dataset<span style="color:#ff79c6">=</span>tokenized_dataset[<span style="color:#f1fa8c">&#34;train&#34;</span>],
</span></span><span style="display:flex;"><span>    data_collator<span style="color:#ff79c6">=</span>data_collator
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>trainer<span style="color:#ff79c6">.</span>train()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 保存和使用</span>
</span></span><span style="display:flex;"><span>model<span style="color:#ff79c6">.</span>save_pretrained(<span style="color:#f1fa8c">f</span><span style="color:#f1fa8c">&#34;</span><span style="color:#f1fa8c">{</span>model_dir<span style="color:#f1fa8c">}</span><span style="color:#f1fa8c">/</span><span style="color:#f1fa8c">{</span>model_id<span style="color:#f1fa8c">}</span><span style="color:#f1fa8c">-lora-int8&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>lora_model <span style="color:#ff79c6">=</span> trainer<span style="color:#ff79c6">.</span>model
</span></span><span style="display:flex;"><span>text <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;two things are infinite:&#34;</span>
</span></span><span style="display:flex;"><span>inputs <span style="color:#ff79c6">=</span> tokenizer(text, return_tensors<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;pt&#34;</span>)
</span></span><span style="display:flex;"><span>out <span style="color:#ff79c6">=</span> lora_model<span style="color:#ff79c6">.</span>generate(<span style="color:#ff79c6">**</span>inputs, max_new_tokens<span style="color:#ff79c6">=</span><span style="color:#bd93f9">48</span>)
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">print</span>(tokenizer<span style="color:#ff79c6">.</span>decode(out[<span style="color:#bd93f9">0</span>], skip_special_tokens<span style="color:#ff79c6">=</span><span style="color:#ff79c6">True</span>))
</span></span></code></pre></td></tr></table>
</div>
</div>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/">大模型应用开发</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/posts/%E6%A1%86%E6%9E%B6/spring/cloud-alibaba/dubbo/9.%E9%AB%98%E9%98%B6%E7%89%B9%E6%80%A7%E6%BA%90%E7%A0%81/">
    <span class="title">« Prev</span>
    <br>
    <span>9.dubbo-高阶特性源码</span>
  </a>
  <a class="next" href="http://localhost:1313/posts/%E5%9F%BA%E7%A1%80/%E7%AE%97%E6%B3%95/leecode/9.%E6%95%B4%E6%95%B0%E5%8F%8D%E8%BD%AC/">
    <span class="title">Next »</span>
    <br>
    <span>9.整数反转</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 9.大模型微调 on x"
            href="https://x.com/intent/tweet/?text=9.%e5%a4%a7%e6%a8%a1%e5%9e%8b%e5%be%ae%e8%b0%83&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fai%2f9.%25E5%25A4%25A7%25E6%25A8%25A1%25E5%259E%258B%25E5%25BE%25AE%25E8%25B0%2583%2f&amp;hashtags=%e5%a4%a7%e6%a8%a1%e5%9e%8b%e5%ba%94%e7%94%a8%e5%bc%80%e5%8f%91">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 9.大模型微调 on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fai%2f9.%25E5%25A4%25A7%25E6%25A8%25A1%25E5%259E%258B%25E5%25BE%25AE%25E8%25B0%2583%2f&amp;title=9.%e5%a4%a7%e6%a8%a1%e5%9e%8b%e5%be%ae%e8%b0%83&amp;summary=9.%e5%a4%a7%e6%a8%a1%e5%9e%8b%e5%be%ae%e8%b0%83&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2fai%2f9.%25E5%25A4%25A7%25E6%25A8%25A1%25E5%259E%258B%25E5%25BE%25AE%25E8%25B0%2583%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 9.大模型微调 on reddit"
            href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2fai%2f9.%25E5%25A4%25A7%25E6%25A8%25A1%25E5%259E%258B%25E5%25BE%25AE%25E8%25B0%2583%2f&title=9.%e5%a4%a7%e6%a8%a1%e5%9e%8b%e5%be%ae%e8%b0%83">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 9.大模型微调 on facebook"
            href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2fai%2f9.%25E5%25A4%25A7%25E6%25A8%25A1%25E5%259E%258B%25E5%25BE%25AE%25E8%25B0%2583%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 9.大模型微调 on whatsapp"
            href="https://api.whatsapp.com/send?text=9.%e5%a4%a7%e6%a8%a1%e5%9e%8b%e5%be%ae%e8%b0%83%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2fai%2f9.%25E5%25A4%25A7%25E6%25A8%25A1%25E5%259E%258B%25E5%25BE%25AE%25E8%25B0%2583%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 9.大模型微调 on telegram"
            href="https://telegram.me/share/url?text=9.%e5%a4%a7%e6%a8%a1%e5%9e%8b%e5%be%ae%e8%b0%83&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fai%2f9.%25E5%25A4%25A7%25E6%25A8%25A1%25E5%259E%258B%25E5%25BE%25AE%25E8%25B0%2583%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 9.大模型微调 on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=9.%e5%a4%a7%e6%a8%a1%e5%9e%8b%e5%be%ae%e8%b0%83&u=http%3a%2f%2flocalhost%3a1313%2fposts%2fai%2f9.%25E5%25A4%25A7%25E6%25A8%25A1%25E5%259E%258B%25E5%25BE%25AE%25E8%25B0%2583%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">王小红的笔记</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
