<!DOCTYPE html>
<html lang="en" dir="auto">

<head>
	<meta name="generator" content="Hugo 0.150.0"><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>王小红的笔记</title>

<meta name="description" content="">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.08f7d74f0ada0f975d29ae436285b61ed7a719d05f350cb888d00341642995a2.css" integrity="sha256-CPfXTwraD5ddKa5DYoW2HtenGdBfNQy4iNADQWQplaI=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" type="application/rss+xml" href="http://localhost:1313/index.xml">
<link rel="alternate" hreflang="en" href="http://localhost:1313/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="/katex/katex.min.css">
<script defer src="/katex/katex.min.js"></script>
<script defer src="/katex/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: "$$", right: "$$", display: true},
        {left: "$", right: "$", display: false},
        {left: "\\(", right: "\\)", display: false},
        {left: "\\[", right: "\\]", display: true}
      ]
    });
  });
</script>

</head>

<body class="list" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="王小红的笔记 (Alt + H)">王小红的笔记</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/posts/" title="笔记">
                    <span>笔记</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/categories/" title="分类">
                    <span>分类</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="标签">
                    <span>标签</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main"> 

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">
    </h2>
  </header>
  <div class="entry-content">
    <p>虚拟机安装 虚拟机管理软件：virtual box；
虚拟机镜像版本：CentOS-7-x86_64-Everything-1810；
安装完成后启动配置固定ip，由于不知道那些ip可以分配，所以先设置成动态获取，在把获取的ip设成固定的：
1 2 # 编辑ifcfg-eth0文件 vi /etc/sysconfig/network-scripts/ifcfg-eth0 1 2 3 4 5 DEVICE=eth0 TYPE=Ethernet ONBOOT=yes # 动态ip BOOTPROTO=dhcp 1 2 3 4 # 重启network服务 service network restart #--&gt;查看当前ip，比如192.168.0.3 ifconfig 再次编辑ifcfg-eth0文件
...</p>
  </div>
  <footer class="entry-footer">1 min</footer>
  <a class="entry-link" aria-label="post link to " href="http://localhost:1313/posts/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/%E7%BC%93%E5%AD%98%E6%9E%B6%E6%9E%84/1.%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">
    </h2>
  </header>
  <div class="entry-content">
    <p> 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 public class ZookeeperSession { private static CountDownLatch connectedSemaphore = new CountDownLatch(1); private ZooKeeper zooKeeper; public ZookeeperSession() { try { // 建立zk连接，异步建立，需要些一个WatchedEvent监听回调 zooKeeper = new ZooKeeper(&#34;192.168.0.3:2181,192.168.0.5:2181,192.168.0.6:2181&#34;, 5000, new ZookeeperWatcher()); log.info(&#34;zk连接状态:{}&#34;, zooKeeper.getState()); connectedSemaphore.await(); log.info(&#34;zk连接成功&#34;); } catch (IOException e) { e.printStackTrace(); } catch (InterruptedException e) { e.printStackTrace(); } } /** * zk连接回调 */ public class ZookeeperWatcher implements Watcher { @Override public void process(WatchedEvent watchedEvent) { log.info(&#34;Receive watched event: &#34; &#43; watchedEvent.getState()); if(Event.KeeperState.SyncConnected == watchedEvent.getState()) { connectedSemaphore.countDown(); } } } /** * 获取分布式锁 * @param id */ public void acpuireDistributeLock(String id) { String path = &#34;/produt-lock-&#34; &#43; id; try{ // 创建一个临时节点 zooKeeper.create(path, // 节点路径 &#34;&#34;.getBytes(), // 节点数据 ZooDefs.Ids.OPEN_ACL_UNSAFE, // 节点权限 CreateMode.EPHEMERAL // 临时节点 ); log.info(&#34;成功创建分布式锁节点:{}&#34;, path); }catch(KeeperException e){ int count = 0; while(true){ try{ Thread.sleep(20); zooKeeper.create(path, &#34;&#34;.getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL); }catch(InterruptedException e1){ e.printStackTrace(); }catch(KeeperException e1){ count&#43;&#43;; continue; } log.info(&#34;重试:{}次后成功创建分布式锁节点:{}&#34;, count, path); break; } }catch(InterruptedException e){ e.printStackTrace(); } } /** * 释放分布式锁 * @param id */ public void releaseDistributeLock(String id) { String path = &#34;/produt-lock-&#34; &#43; id; try { zooKeeper.delete(path, -1); } catch (InterruptedException e) { e.printStackTrace(); } catch (KeeperException e) { e.printStackTrace(); } } /** * 单例实现静态内部类 */ private static class Singleton { private static ZookeeperSession zookeeperSession; static { zookeeperSession = new ZookeeperSession(); } public static ZookeeperSession getZookeeperSession() { return zookeeperSession; } } public static ZookeeperSession getInstance() { return Singleton.getZookeeperSession(); } public static void init() { getInstance(); } } </p>
  </div>
  <footer class="entry-footer">2 min</footer>
  <a class="entry-link" aria-label="post link to " href="http://localhost:1313/posts/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/%E7%BC%93%E5%AD%98%E6%9E%B6%E6%9E%84/10.zk%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">
    </h2>
  </header>
  <div class="entry-content">
    <p>集群架构 并行度和流分组 task就是并行度，当excutor中只有一个task的时候，excutor也可以被说为并行度；
每个task执行的都是spout或者bolt的代码副本，当task执行完spout或者bold的代码的代码后如何向下游的bolt传递数据就叫流分组：
Shuffle Grouping：随机发射，负载均衡； Fields Grouping：每条数据都存储到tuple中，根据tuple的某一个，或者某些个字段进行分组，值相同的会被分到下游bolt的固定task中； All Grouping：有点像MQ的广播模式，下游bold的每个tak都会接到数据； Global Grouping：根据taskId选择分给那个task； None Grouping：和Shuffle Grouping一样； Direct Grouping：自己指定发给那个task； Local or Shuffle Grouping：只会对同一个进程（excutor）中的下游bolt的task发送数据，并且使用Shuffle Grouping； wordcount 大数据的入门不是写hello word，是单词计数，写一个spout随机发送一个句子，然后创建两个bolt，一个拆分句子，一个统计单词数。使用的是storm1.1.1
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 public class WordCountTopology { /** * spout * 生产或者获取数据 */ public static class RandomSentenceSpout extends BaseRichSpout { private static final long serialVersionUID = 1938838116523430511L; private SpoutOutputCollector spoutOutputCollector; private Random random; /** * 对spout初始化 * 比如说创建线程池或者数据库连接池或者http客户端 * 被woork下的某个excutor下的某个task运行 * @param map * @param topologyContext * @param collector */ @Override public void open(Map map, TopologyContext topologyContext, SpoutOutputCollector collector) { /** * open初始化的时候会传进来一个SpoutOutputCollector，他是用来向下游传递数据的 */ this.spoutOutputCollector = collector; // 构造一个随机数生产对象 this.random = new Random(); } /** * work下的excutor下的task会无限调用nextTuple方法 */ @Override public void nextTuple() { Utils.sleep(100); // 定义一些句子 String[] sentences = new String[]{&#34;the cow jumped over the moon&#34;, &#34;an apple a day keeps the doctor away&#34;, &#34;four score and seven years ago&#34;, &#34;snow white and the seven dwarfs&#34;, &#34;i am at two with natrue&#34;}; String sentence = sentences[random.nextInt(sentences.length)]; System.out.println(&#34;发射数据&#34; &#43; sentence); // new Values就是构建tuple，一个tuple里面是一条数据，多个tuple组成stream spoutOutputCollector.emit(new Values(sentence)); } /** * 定义发射出去的每个tuple的每个字段是什么 * @param outputFieldsDeclarer */ @Override public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) { outputFieldsDeclarer.declare(new Fields(&#34;sentence&#34;)); } } /** * bolt * 处理数据 * 被woork下的某个excutor下的某个task运行 * * 将收到的句子分割然后发射出去 */ public static class SplitSentence extends BaseRichBolt { private static final long serialVersionUID = -7296430331665354940L; // 发射数据 private OutputCollector outputCollector; /** * Bolt初始化方法 * @param stormConf * @param context * @param collector */ @Override public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) { this.outputCollector = collector; } /** * 每次收到数据后都会交给execute方法执行 * @param input */ @Override public void execute(Tuple input) { String sentence = input.getStringByField(&#34;sentence&#34;); String[] words = sentence.split(&#34; &#34;); for(String word : words){ outputCollector.emit(new Values(word)); } } /** * 定义发射出去的tuple的字段名 * @param declarer */ @Override public void declareOutputFields(OutputFieldsDeclarer declarer) { declarer.declare(new Fields(&#34;word&#34;)); } } /** * 第二个bolt * 收到单词，放到map中，value是单词出现的次数 */ public static class WordCount extends BaseRichBolt { private static final long serialVersionUID = 3392070844258731973L; private OutputCollector outputCollector; Map&lt;String, Long&gt; counts = new HashMap&lt;&gt;(); @Override public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) { this.outputCollector = collector; } @Override public void execute(Tuple input) { String word = input.getStringByField(&#34;word&#34;); Long count = counts.get(word); if(count == null){ count = 0L; } count&#43;&#43;; counts.put(word, count); System.out.println(&#34;单词:&#34; &#43; word &#43; &#34;出现的次数:&#34; &#43; count); outputCollector.emit(new Values(word, count)); } @Override public void declareOutputFields(OutputFieldsDeclarer declarer) { declarer.declare(new Fields(&#34;word&#34;, &#34;count&#34;)); } } public static void main(String[] args) { // 将spout和bolt组合成一个拓扑 TopologyBuilder topologyBuilder = new TopologyBuilder(); // 设置spout topologyBuilder.setSpout(&#34;randomSentence&#34;, // 名字 new RandomSentenceSpout(), 2 // 设置excutor数量 ); // 设置第一个bolt topologyBuilder.setBolt(&#34;splitSentence&#34;, new SplitSentence(), 5 // 设置excutor数量 ).setNumTasks(10).shuffleGrouping(&#34;randomSentence&#34;); // 设置task数量 // 设置第二个bolt // 相同的单词从splitSentence发射出来后一定会进入到下游的同一个task中 topologyBuilder.setBolt(&#34;wordCount&#34;, new WordCount(), 10) .setNumTasks(20).fieldsGrouping(&#34;splitSentence&#34;, new Fields(&#34;word&#34;)); Config config = new Config(); // 如果args有值说明是在命令行执行，提交到strom集群上去 if(args != null &amp;&amp; args.length &gt; 0){ // 指定几个wordker执行 config.setNumWorkers(3); try { StormSubmitter.submitTopology(args[0], config, topologyBuilder.createTopology()); } catch (AlreadyAliveException e) { e.printStackTrace(); } catch (InvalidTopologyException e) { e.printStackTrace(); } catch (AuthorizationException e) { e.printStackTrace(); } }else{ config.setMaxTaskParallelism(5); LocalCluster cluster = new LocalCluster(); cluster.submitTopology(&#34;wordCountTopology&#34;, config, topologyBuilder.createTopology()); Utils.sleep(50000); cluster.shutdown(); } } } 集群部署 需要先安装java和python，之后下载storm压缩包传到虚拟机03上然后解压重命名为storm；然后配置环境变量：
...</p>
  </div>
  <footer class="entry-footer">4 min</footer>
  <a class="entry-link" aria-label="post link to " href="http://localhost:1313/posts/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/%E7%BC%93%E5%AD%98%E6%9E%B6%E6%9E%84/11.storm/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">
    </h2>
  </header>
  <div class="entry-content">
    <p>在nginx应用层安装lua-resty-kafka：
1 2 3 4 5 6 7 8 cd /usr/local wget https://github.com/doujiang24/lua-resty-kafka/archive/master.zip unzip master.zip cd /usr/hello_nginx/lualib/ cp -rf /usr/local/lua-resty-kafka-master/lib/resty /usr/hello_nginx/lualib cd /usr/servers/nginx/conf/ vi nginx.conf # http部分添加dns解析：resolver 8.8.8.8; 修改/usr/hello_nginx/lua下的product.lua：
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 -- 之前下面定义了一个cjson，把它拿到上面来 local cjson = require(&#34;cjson&#34;) -- 声明kafka的producer local producer = require(&#34;resty.kafka.producer&#34;) local broker_list = { { host = &#34;192.168.0.3&#34;, port = 9092 }, { host = &#34;192.168.0.5&#34;, port = 9092 }, { host = &#34;192.168.0.6&#34;, port = 9092 } } local log_json = {} log_json[&#34;request_module&#34;] = &#34;product_detail_info&#34; log_json[&#34;headers&#34;] = ngx.req.get_headers() log_json[&#34;uri_args&#34;] = ngx.req.get_uri_args() log_json[&#34;body&#34;] = ngx.req.read_body() log_json[&#34;http_version&#34;] = ngx.req.http_version() log_json[&#34;method&#34;] =ngx.req.get_method() log_json[&#34;raw_reader&#34;] = ngx.req.raw_header() log_json[&#34;body_data&#34;] = ngx.req.get_body_data() local message = cjson.encode(log_json); local productId = ngx.req.get_uri_args()[&#34;productId&#34;] -- 异步发送 local async_producer = producer:new(broker_list, { producer_type = &#34;async&#34; }) -- 发送消息 local ok, err = async_producer:send(&#34;access-log&#34;, productId, message) if not ok then ngx.log(ngx.ERR, &#34;kafka send err:&#34;, err) return end </p>
  </div>
  <footer class="entry-footer">1 min</footer>
  <a class="entry-link" aria-label="post link to " href="http://localhost:1313/posts/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/%E7%BC%93%E5%AD%98%E6%9E%B6%E6%9E%84/12.nginx&#43;lua&#43;kafka&#43;storm%E7%83%AD%E6%95%B0%E6%8D%AE%E7%BB%9F%E8%AE%A1/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">
    </h2>
  </header>
  <div class="entry-content">
    <p>Hystrix最主要的功能就是资源隔离，有两种实现方式：
线程池的资源隔离：被隔离的资源被访问时，需要先经过tomcat等网络容器的线程池，如果访问资源的线程超过规定的数量会先进缓冲队列，队列满了之后就会走fallback降级处理，但是hystrix控制不了tomcat的线程，只是控制线程的执行，默认是10个；一个command group对应一个线程池； 信号量的资源隔离：只是判断访问资源的线程数，接受请求和访问资源的请求是一个线程，通过计数实现的隔离，同一个服务内务需要隔离的时候信号量更合适； 这两个隔离技术的区别的是线程池使用自己的线程访问资源，所以信号量隔离技术无法做超时判断。除了资源隔离还有限流的作用。
一个command就是对一个服务的一个接口的请求，所以一般来说一个command对应一个接口，一个command group对应一个服务；
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // 使用线程池隔离技术 super(HystrixCommandGroupKey.Factory.asKey(&#34;GetProductInfoGroup&#34;)); // 使用信号量隔离技术 super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(&#34;ExampleGroup&#34;)) .andCommandPropertiesDefaults(HystrixCommandProperties.Setter() .withExecutionIsolationStrategy(ExecutionIsolationStrategy.SEMAPHORE))); // 设置command名称：GetProductInfoCommand； // 线程池名称：GetProductInfoPool； // 线程池大小：15、缓冲队列大小：10； super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(&#34;ProductInfoService&#34;)) .andCommandKey(HystrixCommandKey.Factory.asKey(&#34;GetProductInfoCommand&#34;)) .andThreadPoolKey(HystrixThreadPoolKey.Factory.asKey(&#34;GetProductInfoPool&#34;)) .andThreadPoolPropertiesDefaults(HystrixThreadPoolProperties.Setter() .withCoreSize(15) .withQueueSizeRejectionThreshold(10)) ); // 设置超时时间是500ms .andCommandPropertiesDefaults(HystrixCommandProperties.Setter() .withExecutionTimeoutInMilliseconds(500)) 还可以设置filter、request cache，command报错的fallback降级等等；
...</p>
  </div>
  <footer class="entry-footer">1 min</footer>
  <a class="entry-link" aria-label="post link to " href="http://localhost:1313/posts/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/%E7%BC%93%E5%AD%98%E6%9E%B6%E6%9E%84/13.hystrix/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">
    </h2>
  </header>
  <div class="entry-content">
    <p>数据库&#43;缓存的读写模式（cache aside pattern）： 读的时候先去差redis，如果没有再去查数据库，将结果写到redis； 修改的时候先删除缓存，在更新数据库，不更新redis是为了比如更新100次才被读一次，不用缓存的时候没有必要频繁的计算，类似懒加载； 28法则：最重要的东西永远占用大约20%的部分，20%的数据是被频繁读取，所以80%的数据没有必要再更新数据库的时候更新缓存。
数据库加缓存双写不一致问题 在高并发的情况下，如果一个请求在删除缓存之后修改数据库时，另一个请求来获取数据，此时缓存中没有就去读取数据库，数据库中的数据是还没修改完，比如事物是读已提交，那么此时读取到的就是旧数据，读取完放到缓存里的还是旧数据，这时候就会出现双写不一致问题，解决办法：创建多个队列并绑定线程消费，同一条数据路由到同一台服务下的同一个队列，实现同一条数据的操作异步串行。
双写优化点： 同一条数据，当redis中没有的时候要去数据库中读取，这条请求会被放到队列等待消费，如果读请求很多的时候，队列中就会堆积大量的读操作，然后这些读操作在读取完数据库又会重新写入缓存，这些重复的操作即费资源又没必要，所以当redis中没有数据的时候要判断队列中是否已经有针对这条数据的毒操作，如果有的话就把这个请求hang一会，比如while(true)循环，在循环中判断循环的时间，然后再去读取redis，如果太长时间还是没有读到数据就自己去服务器中读；这样就会产生一个问题，如果读请求特别多，比如10000个读请求hang了200ms还没有在redis中读取到数据，这10000个请求会直接打到数据库上，就产生雪崩了，这种问题只能加服务器，加完服务器之后队列中的更新请求和读请求就少了，读的就快了。
</p>
  </div>
  <footer class="entry-footer">1 min</footer>
  <a class="entry-link" aria-label="post link to " href="http://localhost:1313/posts/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/%E7%BC%93%E5%AD%98%E6%9E%B6%E6%9E%84/5.%E5%8F%8C%E5%86%99/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">
    </h2>
  </header>
  <div class="entry-content">
    <p>一不小心安了个5.6，很烦
使用yum安装服务端
1 yum install -y mysql-server 启动服务端并设置开机启动
1 2 service mysqld start chkconfig mysqld on 安装客户端
1 yum install -y mysql-connector-java 修改密码
1 2 3 4 5 6 7 8 9 10 11 12 # 1进入mysql mysql -uroot -p # 2进入mysql数据库 use mysql; # 3修改密码 update user set password=password(&#34;root&#34;) where user=&#34;root&#34;; # 4刷新 flush privileges; # 退出重进查看密码是否生效，设置允许远程客户端访问 use mysql grant all privileges on *.* to &#39;root&#39;@&#39;%&#39; identified by &#39;root&#39;; flush privileges; </p>
  </div>
  <footer class="entry-footer">1 min</footer>
  <a class="entry-link" aria-label="post link to " href="http://localhost:1313/posts/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/%E7%BC%93%E5%AD%98%E6%9E%B6%E6%9E%84/6.mysql%E5%AE%89%E8%A3%85/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">
    </h2>
  </header>
  <div class="entry-content">
    <p>引入依赖：
1 2 3 4 5 6 7 8 9 &lt;!-- ehcache --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;net.sf.ehcache&lt;/groupId&gt; &lt;artifactId&gt;ehcache&lt;/artifactId&gt; &lt;/dependency&gt; 添加配置文件ehcache.xml到resource目录下：
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 &lt;?xml version=&#34;1.0&#34; encoding=&#34;UTF-8&#34;?&gt; &lt;ehcache xmlns:xsi=&#34;http://www.w3.org/2001/XMLSchema-instance&#34; xsi:noNamespaceSchemaLocation=&#34;http://ehcache.org/ehcache.xsd&#34; updateCheck=&#34;false&#34;&gt; &lt;diskStore path=&#34;java.io.tmpdir/Tmp_EhCache&#34; /&gt; &lt;!-- eternal:缓存不过期，timeout不生效 maxElementsInMemory：最多存多少个对象，这个自己估出来的，比如给ehcahce1M，每个对象平均1k，这样这个值大约就是1000 overflowToDisk：如果内存不够是否溢出到磁盘 diskPersistent：在jvm崩溃或者重启时是否开启磁盘持久化，使用磁盘就会变得很慢 timeToIdleSeconds：对象的最大闲置时间，超过了就会过期 timeToLiveSeconds：对象存活时间 memoryStoreEvictionPolicy：当对象达到maxElementsInMemory时使用什么算法清除 --&gt; &lt;defaultCache eternal=&#34;false&#34; maxElementsInMemory=&#34;1000&#34; overflowToDisk=&#34;false&#34; diskPersistent=&#34;false&#34; timeToIdleSeconds=&#34;0&#34; timeToLiveSeconds=&#34;0&#34; memoryStoreEvictionPolicy=&#34;LRU&#34; /&gt; &lt;!-- 手动指定的缓存策略，指定调用下面的local就不会使用上面的defaultCache --&gt; &lt;cache name=&#34;local&#34; eternal=&#34;false&#34; maxElementsInMemory=&#34;1000&#34; overflowToDisk=&#34;false&#34; diskPersistent=&#34;false&#34; timeToIdleSeconds=&#34;0&#34; timeToLiveSeconds=&#34;0&#34; memoryStoreEvictionPolicy=&#34;LRU&#34; /&gt; &lt;/ehcache&gt; 创建配置类：
...</p>
  </div>
  <footer class="entry-footer">1 min</footer>
  <a class="entry-link" aria-label="post link to " href="http://localhost:1313/posts/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/%E7%BC%93%E5%AD%98%E6%9E%B6%E6%9E%84/7.ehcache/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">
    </h2>
  </header>
  <div class="entry-content">
    <p>zookeeper安装 现在eshoop-cache01上安装，将zk压缩包传到/usr/local/下，解压并重命名为zk：
1 2 3 tar -zxvf zookeeper-3.4.5.tar.gz mv zookeeper-3.4.5 zk 配置环境变量：
1 vi /etc/profile path中添加zk的文件位置：
1 2 export ZOOKEEPER_HOME=/usr/local/zk export PATH=$ZOOKEEPER_HOME/bin 保存之后刷新配置文件：
1 source /etc/profile 进入zk/conf/目录下复制zoo_sample.cfg并修改复制出来的文件：
1 2 cp zoo_sample.cfg zoo.cfg vi zoo.cfg 修改data地址，添加集群信息：
1 2 3 4 5 6 # 修改 dataDir=/usr/local/zk/data # 新增 server.0=eshoop-cache01:2888:3888	server.1=eshoop-cache02:2888:3888 server.2=eshoop-cache03:2888:3888 在zk下新建data目录和myid文件，写个0退出：
...</p>
  </div>
  <footer class="entry-footer">2 min</footer>
  <a class="entry-link" aria-label="post link to " href="http://localhost:1313/posts/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/%E7%BC%93%E5%AD%98%E6%9E%B6%E6%9E%84/8.zk&#43;kafka%E5%AE%89%E8%A3%85/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">
    </h2>
  </header>
  <div class="entry-content">
    <p>低命中率问题 在nginx集群中，每个请求如果均匀被打到nginx集群中，同一条数据就会被多次加载到不同的nginx中，多次访问后台，并且nginx存储了很多的重复数据，低命中率的问题解决方案就是双层nginx架构，第一层是分发层，负责请求分发，用数据的某一个字段hash取模之类的，使同一条数据的请求打到同一台nginx；第二层叫应用层，是真正缓存数据的服务器。分发或者缓存的逻辑使用lua实现，nginx&#43;lua的方案使用的是openresty，它里面还有redis、mysql什么的客户端等等。
OpenResty部署 新建文件夹：
1 2 mkdir -p /usr/servers cd /usr/servers/ 安装readline库、编译器之类的东西：
1 yum install -y readline-devel pcre-devel openssl-devel gcc 下载openresty并解压：
1 2 wget https://openresty.org/download/openresty-1.13.6.2.tar.gz tar -zxvf openresty-1.13.6.2.tar.gz 进入openresty-1.13.6.2/bundle/LuaJIT-2.1-20180420/下安装luajit
1 2 3 make clean &amp;&amp; make &amp;&amp; make install # 安装完之后会提示我们建立软连接，复制执行一下 ln -sf luajit-2.1.0-beta3 /usr/local/bin/luajit 回到bundle目录安装一些模块
...</p>
  </div>
  <footer class="entry-footer">3 min</footer>
  <a class="entry-link" aria-label="post link to " href="http://localhost:1313/posts/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/%E7%BC%93%E5%AD%98%E6%9E%B6%E6%9E%84/9.nginx%E7%BC%93%E5%AD%98/"></a>
</article>
<footer class="page-footer">
  <nav class="pagination">
    <a class="prev" href="http://localhost:1313/page/4/">
      «&nbsp;Prev&nbsp;
    </a>
    <a class="next" href="http://localhost:1313/page/6/">Next&nbsp;&nbsp;»
    </a>
  </nav>
</footer>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">王小红的笔记</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
