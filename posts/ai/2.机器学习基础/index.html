<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>2.机器学习基础 | 王小红的笔记</title><meta name=keywords content="机器学习"><meta name=description content='机器学习就是模仿人类处理问题，人类是先从经验归纳，机器学习也是自动从数据中获取模型，用模型对位置数据进行预测。白话讲就是从历史数据里找规律，有了规律之后根据输入的数据输出结果，这个规律就是模型。比如说某个样子的物体是猫，那下次看见类似的也可以把他当成猫。
机器学习不能解决的问题，跨域：机器学习学习的是历史数据的特征，新的数据不可能和历史数据有一样的特征分布
1. 计算机视觉
用摄像机、电脑或者其他设备模拟生物视觉，让计算机理解图片或者视频中的内容。可以分为三类：

图像分类：将图像结构化成类别信息，用实现确定好的类别来描述图片。
目标检测：关注特定的物体目标，要求获取这一目标的类别和位置信息，他和分类的区别是，分类将图片当做整体，目标检测可以在一张图片中获取多种目标及位置。
图像分割：分割是在检测的基础上还需要获取像素信息。

应用领域：人脸识别、视频监控、智能驾驶、图片识别（以图搜图，图片鉴黄），比如抖音送礼的眼镜特效属于人脸识别里的人脸关键点定位。
2. 自然语言处理
语言模型是用来计算下一个句子概率的模型。
3 时间序列
时间序列是一种 有序的、依赖时间的结构化数据，其核心任务是研究和预测数据随时间变化的规律。深度学习为时间序列任务提供了更强大的建模工具，尤其是在处理复杂非线性关系或长时间依赖时。他是一个跨领域的问题，还会涉及到统计学等。
3. 机器学习的工作流程：
获取数据->对数据进行基本处理->特征工程->机器学习(模型训练，也是算法应用的过程)->模型评估，如果模型评估没有达到要求需要从对数据进行基本处理重新进行一遍。
3.1 获取数据
拿到的数据类似于表格，一行就是一个样本，一列就是一个特征，涉及到判断的列不叫特征，叫目标值，不是所有数据都有目标值。数据分为训练数据和测试数据，一般比例为3/7或者2/8。
3.2 数据基本处理
修改数据的空值、异常值、类型转换等。
3.3 特征工程
对数据的进一步处理。包括特征提取(比如将文本或者图片转换成可以用于机器学习的数字)，特征预处理(通过一些函数将数据转换成适合算法模型的特征数据)和特征降维(降低特征个数)。
3.3.1 特征预处理
将数据转换成机器更好识别，更好处理的数据。当特征数据的单位或者大小相差较大，或者某个特征的方差相比其他的方差大出好几个数量级，这种情况可能这个特征对结果的影响比较大，使得算法无法学习到其他特征。
3.3.1.1 归一化
把原始数据映射到某个区间内，默认0~1。计算公式为：
$$
X&rsquo; = \frac {x - min}{max - min}\
X&rsquo;&rsquo; = X&rsquo; * (mx - mi) + mi
$$
上面的公式中，X&rsquo;&lsquo;就是归一化处理后的特征值，max和min表示初始特征值的最大值和最小值，mx和mi表示想要将特征值映射到区间的最大值和最小值。
归一化的时候如果出现一条特征统计不正确，比如有1条数据比其他的大了很多倍或者小了很多倍，这种情况对其他数据的影响很大，所以这种方法鲁棒性较差，只适合精确小数据场景。


1
2
3
4
5
6
7
8
9


import pandas as pd
from sklearn.preprocessing import MinMaxScaler

data = pd.read_csv(&#39;./data/dating.txt&#39;)
# 实例化转化器
transfer = MinMaxScaler(featrue_range=(3, 5))
# 将目标列转换到指定区间，这里是3~5
ret_data = transfer.fit_transform(data[["列1名称", "列2名称"]])
print("转化后的数据:\n", ret_data)


3.3.1.2 标准化


 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11


import pandas as pd
from sklearn.preprocessing import StandardScaler

data = pd.read_csv(&#39;./data/dating.txt&#39;)
# 实例化转化器
transfer = StandardScaler()
# 将目标列转标准化
ret_data = transfer.fit_transform(data[["列1名称", "列2名称"]])
print("转化后的数据:\n", ret_data)
print("每一列的方差为:\n", transfer.var_)
print("每一列的平均值为:\n", transfer.mean_)


第一步就是去均值，将平均值变成0，然后在比上标准差，这样能让所有维度的数相差不大：$X&rsquo; = \frac{x - avg}{\sigma}$。'><meta name=author content><link rel=canonical href=https://wangxiaohong123.github.io/posts/ai/2.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/><link crossorigin=anonymous href=/assets/css/stylesheet.08f7d74f0ada0f975d29ae436285b61ed7a719d05f350cb888d00341642995a2.css integrity="sha256-CPfXTwraD5ddKa5DYoW2HtenGdBfNQy4iNADQWQplaI=" rel="preload stylesheet" as=style><link rel=icon href=https://wangxiaohong123.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://wangxiaohong123.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://wangxiaohong123.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://wangxiaohong123.github.io/apple-touch-icon.png><link rel=mask-icon href=https://wangxiaohong123.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://wangxiaohong123.github.io/posts/ai/2.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://wangxiaohong123.github.io/posts/ai/2.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"><meta property="og:site_name" content="王小红的笔记"><meta property="og:title" content="2.机器学习基础"><meta property="og:description" content='机器学习就是模仿人类处理问题，人类是先从经验归纳，机器学习也是自动从数据中获取模型，用模型对位置数据进行预测。白话讲就是从历史数据里找规律，有了规律之后根据输入的数据输出结果，这个规律就是模型。比如说某个样子的物体是猫，那下次看见类似的也可以把他当成猫。
机器学习不能解决的问题，跨域：机器学习学习的是历史数据的特征，新的数据不可能和历史数据有一样的特征分布
1. 计算机视觉 用摄像机、电脑或者其他设备模拟生物视觉，让计算机理解图片或者视频中的内容。可以分为三类：
图像分类：将图像结构化成类别信息，用实现确定好的类别来描述图片。 目标检测：关注特定的物体目标，要求获取这一目标的类别和位置信息，他和分类的区别是，分类将图片当做整体，目标检测可以在一张图片中获取多种目标及位置。 图像分割：分割是在检测的基础上还需要获取像素信息。 应用领域：人脸识别、视频监控、智能驾驶、图片识别（以图搜图，图片鉴黄），比如抖音送礼的眼镜特效属于人脸识别里的人脸关键点定位。
2. 自然语言处理 语言模型是用来计算下一个句子概率的模型。
3 时间序列 时间序列是一种 有序的、依赖时间的结构化数据，其核心任务是研究和预测数据随时间变化的规律。深度学习为时间序列任务提供了更强大的建模工具，尤其是在处理复杂非线性关系或长时间依赖时。他是一个跨领域的问题，还会涉及到统计学等。
3. 机器学习的工作流程： 获取数据->对数据进行基本处理->特征工程->机器学习(模型训练，也是算法应用的过程)->模型评估，如果模型评估没有达到要求需要从对数据进行基本处理重新进行一遍。
3.1 获取数据 拿到的数据类似于表格，一行就是一个样本，一列就是一个特征，涉及到判断的列不叫特征，叫目标值，不是所有数据都有目标值。数据分为训练数据和测试数据，一般比例为3/7或者2/8。
3.2 数据基本处理 修改数据的空值、异常值、类型转换等。
3.3 特征工程 对数据的进一步处理。包括特征提取(比如将文本或者图片转换成可以用于机器学习的数字)，特征预处理(通过一些函数将数据转换成适合算法模型的特征数据)和特征降维(降低特征个数)。
3.3.1 特征预处理 将数据转换成机器更好识别，更好处理的数据。当特征数据的单位或者大小相差较大，或者某个特征的方差相比其他的方差大出好几个数量级，这种情况可能这个特征对结果的影响比较大，使得算法无法学习到其他特征。
3.3.1.1 归一化 把原始数据映射到某个区间内，默认0~1。计算公式为： $$ X’ = \frac {x - min}{max - min}\
X’’ = X’ * (mx - mi) + mi $$ 上面的公式中，X’‘就是归一化处理后的特征值，max和min表示初始特征值的最大值和最小值，mx和mi表示想要将特征值映射到区间的最大值和最小值。
归一化的时候如果出现一条特征统计不正确，比如有1条数据比其他的大了很多倍或者小了很多倍，这种情况对其他数据的影响很大，所以这种方法鲁棒性较差，只适合精确小数据场景。
1 2 3 4 5 6 7 8 9 import pandas as pd from sklearn.preprocessing import MinMaxScaler data = pd.read_csv(&#39;./data/dating.txt&#39;) # 实例化转化器 transfer = MinMaxScaler(featrue_range=(3, 5)) # 将目标列转换到指定区间，这里是3~5 ret_data = transfer.fit_transform(data[["列1名称", "列2名称"]]) print("转化后的数据:\n", ret_data) 3.3.1.2 标准化 1 2 3 4 5 6 7 8 9 10 11 import pandas as pd from sklearn.preprocessing import StandardScaler data = pd.read_csv(&#39;./data/dating.txt&#39;) # 实例化转化器 transfer = StandardScaler() # 将目标列转标准化 ret_data = transfer.fit_transform(data[["列1名称", "列2名称"]]) print("转化后的数据:\n", ret_data) print("每一列的方差为:\n", transfer.var_) print("每一列的平均值为:\n", transfer.mean_) 第一步就是去均值，将平均值变成0，然后在比上标准差，这样能让所有维度的数相差不大：$X’ = \frac{x - avg}{\sigma}$。'><meta property="og:locale" content="zh-CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:tag" content="机器学习"><meta name=twitter:card content="summary"><meta name=twitter:title content="2.机器学习基础"><meta name=twitter:description content='机器学习就是模仿人类处理问题，人类是先从经验归纳，机器学习也是自动从数据中获取模型，用模型对位置数据进行预测。白话讲就是从历史数据里找规律，有了规律之后根据输入的数据输出结果，这个规律就是模型。比如说某个样子的物体是猫，那下次看见类似的也可以把他当成猫。
机器学习不能解决的问题，跨域：机器学习学习的是历史数据的特征，新的数据不可能和历史数据有一样的特征分布
1. 计算机视觉
用摄像机、电脑或者其他设备模拟生物视觉，让计算机理解图片或者视频中的内容。可以分为三类：

图像分类：将图像结构化成类别信息，用实现确定好的类别来描述图片。
目标检测：关注特定的物体目标，要求获取这一目标的类别和位置信息，他和分类的区别是，分类将图片当做整体，目标检测可以在一张图片中获取多种目标及位置。
图像分割：分割是在检测的基础上还需要获取像素信息。

应用领域：人脸识别、视频监控、智能驾驶、图片识别（以图搜图，图片鉴黄），比如抖音送礼的眼镜特效属于人脸识别里的人脸关键点定位。
2. 自然语言处理
语言模型是用来计算下一个句子概率的模型。
3 时间序列
时间序列是一种 有序的、依赖时间的结构化数据，其核心任务是研究和预测数据随时间变化的规律。深度学习为时间序列任务提供了更强大的建模工具，尤其是在处理复杂非线性关系或长时间依赖时。他是一个跨领域的问题，还会涉及到统计学等。
3. 机器学习的工作流程：
获取数据->对数据进行基本处理->特征工程->机器学习(模型训练，也是算法应用的过程)->模型评估，如果模型评估没有达到要求需要从对数据进行基本处理重新进行一遍。
3.1 获取数据
拿到的数据类似于表格，一行就是一个样本，一列就是一个特征，涉及到判断的列不叫特征，叫目标值，不是所有数据都有目标值。数据分为训练数据和测试数据，一般比例为3/7或者2/8。
3.2 数据基本处理
修改数据的空值、异常值、类型转换等。
3.3 特征工程
对数据的进一步处理。包括特征提取(比如将文本或者图片转换成可以用于机器学习的数字)，特征预处理(通过一些函数将数据转换成适合算法模型的特征数据)和特征降维(降低特征个数)。
3.3.1 特征预处理
将数据转换成机器更好识别，更好处理的数据。当特征数据的单位或者大小相差较大，或者某个特征的方差相比其他的方差大出好几个数量级，这种情况可能这个特征对结果的影响比较大，使得算法无法学习到其他特征。
3.3.1.1 归一化
把原始数据映射到某个区间内，默认0~1。计算公式为：
$$
X&rsquo; = \frac {x - min}{max - min}\
X&rsquo;&rsquo; = X&rsquo; * (mx - mi) + mi
$$
上面的公式中，X&rsquo;&lsquo;就是归一化处理后的特征值，max和min表示初始特征值的最大值和最小值，mx和mi表示想要将特征值映射到区间的最大值和最小值。
归一化的时候如果出现一条特征统计不正确，比如有1条数据比其他的大了很多倍或者小了很多倍，这种情况对其他数据的影响很大，所以这种方法鲁棒性较差，只适合精确小数据场景。


1
2
3
4
5
6
7
8
9


import pandas as pd
from sklearn.preprocessing import MinMaxScaler

data = pd.read_csv(&#39;./data/dating.txt&#39;)
# 实例化转化器
transfer = MinMaxScaler(featrue_range=(3, 5))
# 将目标列转换到指定区间，这里是3~5
ret_data = transfer.fit_transform(data[["列1名称", "列2名称"]])
print("转化后的数据:\n", ret_data)


3.3.1.2 标准化


 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11


import pandas as pd
from sklearn.preprocessing import StandardScaler

data = pd.read_csv(&#39;./data/dating.txt&#39;)
# 实例化转化器
transfer = StandardScaler()
# 将目标列转标准化
ret_data = transfer.fit_transform(data[["列1名称", "列2名称"]])
print("转化后的数据:\n", ret_data)
print("每一列的方差为:\n", transfer.var_)
print("每一列的平均值为:\n", transfer.mean_)


第一步就是去均值，将平均值变成0，然后在比上标准差，这样能让所有维度的数相差不大：$X&rsquo; = \frac{x - avg}{\sigma}$。'><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://wangxiaohong123.github.io/posts/"},{"@type":"ListItem","position":2,"name":"2.机器学习基础","item":"https://wangxiaohong123.github.io/posts/ai/2.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"2.机器学习基础","name":"2.机器学习基础","description":"机器学习就是模仿人类处理问题，人类是先从经验归纳，机器学习也是自动从数据中获取模型，用模型对位置数据进行预测。白话讲就是从历史数据里找规律，有了规律之后根据输入的数据输出结果，这个规律就是模型。比如说某个样子的物体是猫，那下次看见类似的也可以把他当成猫。\n机器学习不能解决的问题，跨域：机器学习学习的是历史数据的特征，新的数据不可能和历史数据有一样的特征分布\n1. 计算机视觉 用摄像机、电脑或者其他设备模拟生物视觉，让计算机理解图片或者视频中的内容。可以分为三类：\n图像分类：将图像结构化成类别信息，用实现确定好的类别来描述图片。 目标检测：关注特定的物体目标，要求获取这一目标的类别和位置信息，他和分类的区别是，分类将图片当做整体，目标检测可以在一张图片中获取多种目标及位置。 图像分割：分割是在检测的基础上还需要获取像素信息。 应用领域：人脸识别、视频监控、智能驾驶、图片识别（以图搜图，图片鉴黄），比如抖音送礼的眼镜特效属于人脸识别里的人脸关键点定位。\n2. 自然语言处理 语言模型是用来计算下一个句子概率的模型。\n3 时间序列 时间序列是一种 有序的、依赖时间的结构化数据，其核心任务是研究和预测数据随时间变化的规律。深度学习为时间序列任务提供了更强大的建模工具，尤其是在处理复杂非线性关系或长时间依赖时。他是一个跨领域的问题，还会涉及到统计学等。\n3. 机器学习的工作流程： 获取数据-\u0026gt;对数据进行基本处理-\u0026gt;特征工程-\u0026gt;机器学习(模型训练，也是算法应用的过程)-\u0026gt;模型评估，如果模型评估没有达到要求需要从对数据进行基本处理重新进行一遍。\n3.1 获取数据 拿到的数据类似于表格，一行就是一个样本，一列就是一个特征，涉及到判断的列不叫特征，叫目标值，不是所有数据都有目标值。数据分为训练数据和测试数据，一般比例为3/7或者2/8。\n3.2 数据基本处理 修改数据的空值、异常值、类型转换等。\n3.3 特征工程 对数据的进一步处理。包括特征提取(比如将文本或者图片转换成可以用于机器学习的数字)，特征预处理(通过一些函数将数据转换成适合算法模型的特征数据)和特征降维(降低特征个数)。\n3.3.1 特征预处理 将数据转换成机器更好识别，更好处理的数据。当特征数据的单位或者大小相差较大，或者某个特征的方差相比其他的方差大出好几个数量级，这种情况可能这个特征对结果的影响比较大，使得算法无法学习到其他特征。\n3.3.1.1 归一化 把原始数据映射到某个区间内，默认0~1。计算公式为： $$ X\u0026rsquo; = \\frac {x - min}{max - min}\\\nX\u0026rsquo;\u0026rsquo; = X\u0026rsquo; * (mx - mi) + mi $$ 上面的公式中，X\u0026rsquo;\u0026lsquo;就是归一化处理后的特征值，max和min表示初始特征值的最大值和最小值，mx和mi表示想要将特征值映射到区间的最大值和最小值。\n归一化的时候如果出现一条特征统计不正确，比如有1条数据比其他的大了很多倍或者小了很多倍，这种情况对其他数据的影响很大，所以这种方法鲁棒性较差，只适合精确小数据场景。\n1 2 3 4 5 6 7 8 9 import pandas as pd from sklearn.preprocessing import MinMaxScaler data = pd.read_csv(\u0026#39;./data/dating.txt\u0026#39;) # 实例化转化器 transfer = MinMaxScaler(featrue_range=(3, 5)) # 将目标列转换到指定区间，这里是3~5 ret_data = transfer.fit_transform(data[[\u0026#34;列1名称\u0026#34;, \u0026#34;列2名称\u0026#34;]]) print(\u0026#34;转化后的数据:\\n\u0026#34;, ret_data) 3.3.1.2 标准化 1 2 3 4 5 6 7 8 9 10 11 import pandas as pd from sklearn.preprocessing import StandardScaler data = pd.read_csv(\u0026#39;./data/dating.txt\u0026#39;) # 实例化转化器 transfer = StandardScaler() # 将目标列转标准化 ret_data = transfer.fit_transform(data[[\u0026#34;列1名称\u0026#34;, \u0026#34;列2名称\u0026#34;]]) print(\u0026#34;转化后的数据:\\n\u0026#34;, ret_data) print(\u0026#34;每一列的方差为:\\n\u0026#34;, transfer.var_) print(\u0026#34;每一列的平均值为:\\n\u0026#34;, transfer.mean_) 第一步就是去均值，将平均值变成0，然后在比上标准差，这样能让所有维度的数相差不大：$X\u0026rsquo; = \\frac{x - avg}{\\sigma}$。\n","keywords":["机器学习"],"articleBody":"机器学习就是模仿人类处理问题，人类是先从经验归纳，机器学习也是自动从数据中获取模型，用模型对位置数据进行预测。白话讲就是从历史数据里找规律，有了规律之后根据输入的数据输出结果，这个规律就是模型。比如说某个样子的物体是猫，那下次看见类似的也可以把他当成猫。\n机器学习不能解决的问题，跨域：机器学习学习的是历史数据的特征，新的数据不可能和历史数据有一样的特征分布\n1. 计算机视觉 用摄像机、电脑或者其他设备模拟生物视觉，让计算机理解图片或者视频中的内容。可以分为三类：\n图像分类：将图像结构化成类别信息，用实现确定好的类别来描述图片。 目标检测：关注特定的物体目标，要求获取这一目标的类别和位置信息，他和分类的区别是，分类将图片当做整体，目标检测可以在一张图片中获取多种目标及位置。 图像分割：分割是在检测的基础上还需要获取像素信息。 应用领域：人脸识别、视频监控、智能驾驶、图片识别（以图搜图，图片鉴黄），比如抖音送礼的眼镜特效属于人脸识别里的人脸关键点定位。\n2. 自然语言处理 语言模型是用来计算下一个句子概率的模型。\n3 时间序列 时间序列是一种 有序的、依赖时间的结构化数据，其核心任务是研究和预测数据随时间变化的规律。深度学习为时间序列任务提供了更强大的建模工具，尤其是在处理复杂非线性关系或长时间依赖时。他是一个跨领域的问题，还会涉及到统计学等。\n3. 机器学习的工作流程： 获取数据-\u003e对数据进行基本处理-\u003e特征工程-\u003e机器学习(模型训练，也是算法应用的过程)-\u003e模型评估，如果模型评估没有达到要求需要从对数据进行基本处理重新进行一遍。\n3.1 获取数据 拿到的数据类似于表格，一行就是一个样本，一列就是一个特征，涉及到判断的列不叫特征，叫目标值，不是所有数据都有目标值。数据分为训练数据和测试数据，一般比例为3/7或者2/8。\n3.2 数据基本处理 修改数据的空值、异常值、类型转换等。\n3.3 特征工程 对数据的进一步处理。包括特征提取(比如将文本或者图片转换成可以用于机器学习的数字)，特征预处理(通过一些函数将数据转换成适合算法模型的特征数据)和特征降维(降低特征个数)。\n3.3.1 特征预处理 将数据转换成机器更好识别，更好处理的数据。当特征数据的单位或者大小相差较大，或者某个特征的方差相比其他的方差大出好几个数量级，这种情况可能这个特征对结果的影响比较大，使得算法无法学习到其他特征。\n3.3.1.1 归一化 把原始数据映射到某个区间内，默认0~1。计算公式为： $$ X’ = \\frac {x - min}{max - min}\\\nX’’ = X’ * (mx - mi) + mi $$ 上面的公式中，X’‘就是归一化处理后的特征值，max和min表示初始特征值的最大值和最小值，mx和mi表示想要将特征值映射到区间的最大值和最小值。\n归一化的时候如果出现一条特征统计不正确，比如有1条数据比其他的大了很多倍或者小了很多倍，这种情况对其他数据的影响很大，所以这种方法鲁棒性较差，只适合精确小数据场景。\n1 2 3 4 5 6 7 8 9 import pandas as pd from sklearn.preprocessing import MinMaxScaler data = pd.read_csv('./data/dating.txt') # 实例化转化器 transfer = MinMaxScaler(featrue_range=(3, 5)) # 将目标列转换到指定区间，这里是3~5 ret_data = transfer.fit_transform(data[[\"列1名称\", \"列2名称\"]]) print(\"转化后的数据:\\n\", ret_data) 3.3.1.2 标准化 1 2 3 4 5 6 7 8 9 10 11 import pandas as pd from sklearn.preprocessing import StandardScaler data = pd.read_csv('./data/dating.txt') # 实例化转化器 transfer = StandardScaler() # 将目标列转标准化 ret_data = transfer.fit_transform(data[[\"列1名称\", \"列2名称\"]]) print(\"转化后的数据:\\n\", ret_data) print(\"每一列的方差为:\\n\", transfer.var_) print(\"每一列的平均值为:\\n\", transfer.mean_) 第一步就是去均值，将平均值变成0，然后在比上标准差，这样能让所有维度的数相差不大：$X’ = \\frac{x - avg}{\\sigma}$。\n3.3.2 特征提取 就是把数据转换成数字，主要包括字典特征提取，文本特征提取（统计单词次数），图像特征提取。\n字典特征提取：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from sklearn.featrue_extraction import DictVectorizer def dict_demo(): \"\"\" 字典特征提取 :return: None \"\"\" data = [{'city': '北京', 'temperatrue': 100}, {'city': '上海', 'temperatrue': 60}, {'city': '深圳', 'temperatrue': 30}] # 字典特征提取 # 非sparse矩阵，比sparse矩阵遍历慢，费空间 transfer = DictVectorizer(sparse=False) new_data = transfer.fit_transform(data) print(\"列名：\\n\", transfer.get_featrue_names_out()) print(new_data) # 控制台输出 属性名： ['city=上海' 'city=北京' 'city=深圳' 'temperatrue'] [[ 0. 1. 0. 100.] [ 1. 0. 0. 60.] [ 0. 0. 1. 30.]] 英文文本特征提取：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 from sklearn.featrue_extraction.text import CountVectorizer def text_count_demo(): \"\"\" 英文文本特征提取 :return: None \"\"\" text = [\"life is short,i like python\", \"life is too long,i dislike python\"] # 可以使用stop_words属性指定停用词 transfer = CountVectorizer() new_data = transfer.fit_transform(text) print(\"特征名:\\n\", transfer.get_featrue_names_out()) print(new_data) # 控制台输出，默认就是sparse矩阵，(0, 2)\t1 表示在第一句话中，输出的特征名中的第二个单词出现1次，也就是在第一句话中is单词出现了1次。 特征名: ['dislike' 'is' 'life' 'like' 'long' 'python' 'short' 'too'] (0, 2)\t1 (0, 1)\t1 (0, 6)\t1 (0, 3)\t1 (0, 5)\t1 (1, 2)\t1 (1, 1)\t1 (1, 5)\t1 (1, 7)\t1 (1, 4)\t1 (1, 0)\t1 中文文本特征提取：中文使用jieba分词现将中文分开，然后在使用CountVectorizer提取特征\nTf-idf文本特征提取：如果某个词或者某个短语在一篇文章中出现的概率比其他文章高，就认为这个词或短语具有很好的类别区分能力，适合是用来分类。Tf就是词频，idf是逆向文档频率，idf分2步得到，第一步先用总文件数除以包含该词的文件数，第二步将第一步得到的商取10为底的对数。Tf * idf就是最终的重要程度。\n3.3.3 特征降维 减少特征的数量，一般都是将相关性的特征去掉一些或者合并成1个，比如湿度和降雨量可能降维之后就剩一个降雨量。降维有2种方式：特征选择和主成分分析。\n3.3.3.1 特征选择 就是提取主要特征，主要有2中实现：\n过滤式，探究特征和目标值之间的关联，使用低方差特征过滤(方差相差小表示很相近，就删掉)或者相关系数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 import pandas as pd from sklearn.featrue_selection import VarianceThreshold from scipy.stats import pearsonr from scipy.stats import spearmanr def var_thr(): \"\"\" 特征降维：低方差特征过滤 :return: \"\"\" data = pd.read_csv('csv文件') # 实例化,方差小于1的会被过滤 transfer = VarianceThreshold(threshold=1) # 转换 transfer_data = transfer.fit_transform(data.iloc[:, 1:10]) # # def pea_demo(): \"\"\" 特征降维：相关系数 :return: \"\"\" x1 = [12.5, 43, 15.3, 14.4] x2 = [21.2, 23.1, 44.2, 19.5] # 皮尔逊相关系数 ret = pearsonr(x1, x2) # 斯皮尔曼相关系数 # ret = spearmanr(x1, x2) # 无论哪个相关性公式都会输出2个值 # 第1个值大于0表示正相关，小于0表示负相关，等于0表示没有关系，|r|=1表示完全相关，|r|\u003c0.4是低度相关，0.4 \u003c=|r|\u003c0.7是显著相关，0.7 \u003c=|r|是高度相关 # 第二个值当样本数超过500时有参考意义，越接近0表示相关性越强 print(ret) embedded嵌入式，算法自动选择特征，比如决策树、L1正则化。\n3.3.3.2 主成分分析 降维时舍弃原有数据，创建新变量:\n1 2 3 4 5 6 7 8 9 10 11 12 from sklearn.decomposition import PCA data = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] # n_components传整数表示多留多少特征，小数表示保留特征的百分比 transfer = PCA(n_components=2) transfer_data = transfer.fit_transform(data) print(\"保留2列特征的结果:\", transfer_data) # 保留2列特征的结果: [[ 5.19615242 0. ] # [-0. 0. ] # [-5.19615242 0. ]] 3.4 机器学习 选择合适的算法对模型进行训练。使用梯度下降算法时涉及到3个概念：\nepoch：将整个训练集完整送入模型进行1次训练，称为1代训练； batch：将训练集分成多个小数据集，每个批次会被模型单独进行1次向前传播和反向传播； iteration：每次模型完成一个 Batch 的前向传播和反向传播，进行一次参数更新，称为一次 Iteration； 3.5 模型评估 对训练好的模型进行评估，训练的好就上线，不好就重来一遍。\n3.5.1 分类评估方法 3.5.1.1 混淆矩阵 测试集和预测结果可以组成四种情况：\n预测结果正例 预测结果假例 真实结果正例 真正例TP 伪反例FN 真实结果假例 伪正例FP 真反例TN 混淆矩阵中，真正例TP和真反例TN表示预测结果正确，通过混淆矩阵可以计算出几个指标：\n准确率：$\\dfrac {真正例TP + 真反例TN} {真正例TP + 真反例TN + 伪正例FP + 伪反例FN}$ 精确率：$\\dfrac 真正例TP {真正例TP + 伪正例FP}$ 召回率：也叫查全率，$\\dfrac 真正例TP {真正例TP + 伪反例FN}$ F1-score：用来反映模型的稳健性：$\\dfrac {2真正例TP} {2真正例TP + 伪正例FP + 伪反例FN}$ 上面这些指标在样本不均衡的时候是无法对模型进行评估的，比如一共有100条结果，99条真，1条假，如果机器不进行判断将所有结果都预测为真，精确率也能达到99%，召回率已经达到100%了，虽然模型在胡说，一般正例和反例的比例低于1/4就说名样本是不均衡的，不均衡的样本应该使用ROC曲线和AUC指标进行评估。\n3.5.1.2 ROC与AUC ROC曲线由TPR和FPR组成，TPR就是召回率，当做纵坐标；FPR是$\\dfrac 伪正例FP {伪正例FP + 真反例TN}$​，当做横坐标，ROC可以看出来分类器是否在胡说。\nAUC是随机取一堆正负样本，正样本得分大于负样本得分的概率，AUC的范围在[0,1]之间，越接近1效果越好。\n3.5.2 交叉验证，网格搜索 交叉验证是说将数据集进一步划分，原来只分成训练集和测试集，现在需要将训练集分成验证集和训练集，比如将训练集分成4份，叫做4折交叉验证，然后对数据进行4次的测试，第一次前1/4当作验证集，第二次事1/2到1/4当作验证集……，将4次结果的平均值当作结果，这样可以让模型更加准确，但是不能提高准确率。\n提高准确率需要使用网格搜索，算法中需要手动指定的参数叫做超参数，比如KNeighborsClassifier(n_neighbors=5)中的5就是超参数，网格搜索就是预先定义n个超参数，每组超参数都采用交叉验证来评估，最后选出最优的组合。\n3.5.3 欠拟合和过拟合 过拟合是在训练上表现好，测试上表现不好，这个时候就是模型过于复杂，出现过拟合；欠拟合就是训练和测试表现都不好，模型过于简单，出现欠拟合。\n过拟合可能是因为特征过多，解决的办法是重新清洗数据、增加训练数据量、正则化和减少特征维度；解决欠拟合就是增加特征项。\n3.5.3.1 正则化 正则化是通过在损失函数中增加惩罚项对模型的参数进行约束，正则化分为2种\nL1正则化：可以让一些模型参数变成0，比如$w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4^2$会变成$w_1x_1 + w_2x_2 + w_3x_3$​，也叫lasso回归； L2正则化：认为一些参数是有用的，不会变成0，会变成一个比较小的常数值比如$w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4^2$会变成$w_1x_1 + w_2x_2 + w_3x_3 + b$。也叫岭回归（Ridge 回归）。 3.5.4 损失函数 损失函数是用于衡量模型预测结果与真实结果之间差异的函数。损失过大的时候需要使用正规方程或者梯度下降进行优化。\n3.5.4.1 正规方程 $w = (x^Tx)^-1x^Ty$，X是特征值矩阵，y是目标值矩阵，-1表示逆矩阵。他是根据损失函数求偏导推到出来的。他可以求出误差最小的系数，但是求解逆矩阵是非常困难的，所以正规方程一般用到小数据集里。正规方程不能解决拟合问题。\n3.5.4.2 梯度下降 在单变量中，梯度表示某一点切线的斜率，也就是函数的微分；在多变量函数中，梯度是一个向量，梯度的方向就是函数在给定点上升最快的方向。$\\theta_{i+1} = \\theta_1 - \\alpha{\\frac \\partial{\\partial \\theta_1}}J(\\theta)$。α表示步长，即每次下降多少，太小容易增加计算复杂度，太大容易错过最低点，并且他没法保证找到的是最小值。\n两种方法的demo：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 from sklearn.datasets import load_boston from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression, SGDRegressor from sklearn.metrics import mean_squared_error # 获取数据 boston = load_boston() # 数据基本处理-分割 x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.2) # 特征工程-标准化 transfer = StandardScaler() x_train = transfer.fit_transform(x_train) x_test = transfer.fit_transform(x_test) # 机器学习-正规方程 # estimator = LinearRegression() # 机器学习-梯度下降 estimator = SGDRegressor() estimator.fit(x_train, y_train) print(\"模型的偏置是:\\n\", estimator.intercept_) print(\"模型的系数是:\\n\", estimator.coef_) # 模型评估 y_pre = estimator.predict(x_test) print(\"预测值是:\\n\", y_pre) ret = mean_squared_error(y_test, y_pre) print(\"均方误差:\\n\", ret) 3.5.5 模型的保存和加载 使用joblib包实现模型的保存和加载：\n1 2 3 4 5 import joblib # 保存模型 joblib.dump(estimator, '路径/xxx.pkl') #加载模型 estimator = joblib.load('路径/xxx.pkl') 4 迁移学习 迁移学习是一种机器学习技术，它的核心思想是将从一个任务中学到的知识迁移到另一个相关的任务中，从而加速学习过程或提高模型的性能。说白了就是把别人训练好的模型，拿来改一改变成适合自己的。主要包含2个步骤：\n预训练模型：一般情况下，预训练模型都是大模型，具备复杂的网络结构，众多的参数量以及足够大的数据集下进行训练产生的模型，常见的预训练模型：BERT及其变体、GPT、GPT-2、roBERTa、transformer-XL、XLNet、XLM、DistilBERT、ALBERT、T5、XLM-RoBERTa。\n预训练模型的使用：首先安装依赖：\n1 pip3 install tqdm boto3 requests regex sentencepiece sacremoses 使用pytorch.hub加载模型：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import torch # 选定预训练模型的来源 source = 'huggingface/pytorch-transformers' # 选定加载模型的哪一部分,常用的4个选项： # tokenizer：映射器 # model：不带头的模型，这个头不是多头注意力的头，而是表示输出层，不带头的意思是仅对输入文本进行特征表示 # modelWithLMHead：带语言模型头的模型 # ModelForSequenceClassification：带类型头的模型 # modelForQuestionAnswering：带问答头的模型 part = 'tokenizer' # 加载模型的名称 model_name = 'bert-base-chinese' # 加载预训练模型映射器 tokenizer = torch.hub.load(source, part, model_name) # 将测试文本转换成模型可以处理的张量 token_tensor = torch.tensor([model.encode('测试文本')]) print(\"token_tensor:\", token_tensor) # 推理阶段不需要计算梯度，使用torch.no_grad()禁用梯度计算 with torch.no_grad(): encoded_layers, _ = model(token_tensor) # 输出结果，这是隐藏层的输出，不是最终的输出 print(encoded_layers) 微调(Fine-tuning)：根据给定的预训练模型，改变他的部分参数或者小分部结构，然后在小部分的数据集上训练，让整个模型更好的适应特定任务。所以微调也是一个训练过程。\n5 下游任务 下游任务是指利用一个预训练模型在某些特定的应用场景中执行的任务，比如问答系统，机器翻译等。\nllama：Meta开源的语言模型，他可以让很少的权重参数达到大模型的效果。 LoRA：正常的微调大模型需要重跑权重参数，LoRA可以实现微调时冻住大模型的权重参数，单独跑个小模型然后合并到大模型中。 Self-Instruct：把样本数据变成方便理解，更能突出重点的过程叫润色，比如 “取快递”润色成“您好，请问我的快递在哪里”，Self-Instruct可以让都有一个标准的格式。 PEFT：Hugging Face将llama、LoRA和Self-Instruct整合到了一起。\n","wordCount":"862","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://wangxiaohong123.github.io/posts/ai/2.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"},"publisher":{"@type":"Organization","name":"王小红的笔记","logo":{"@type":"ImageObject","url":"https://wangxiaohong123.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://wangxiaohong123.github.io/ accesskey=h title="王小红的笔记 (Alt + H)">王小红的笔记</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://wangxiaohong123.github.io/posts/ title=笔记><span>笔记</span></a></li><li><a href=https://wangxiaohong123.github.io/categories/ title=分类><span>分类</span></a></li><li><a href=https://wangxiaohong123.github.io/tags/ title=标签><span>标签</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">2.机器学习基础</h1><div class=post-meta>5 min</div></header><div class=post-content><p>机器学习就是模仿人类处理问题，人类是先从经验归纳，机器学习也是自动从数据中获取模型，用模型对位置数据进行预测。白话讲就是从历史数据里找规律，有了规律之后根据输入的数据输出结果，这个规律就是模型。比如说某个样子的物体是猫，那下次看见类似的也可以把他当成猫。</p><p>机器学习不能解决的问题，跨域：机器学习学习的是历史数据的特征，新的数据不可能和历史数据有一样的特征分布</p><h3 id=1-计算机视觉>1. 计算机视觉<a hidden class=anchor aria-hidden=true href=#1-计算机视觉>#</a></h3><p>用摄像机、电脑或者其他设备模拟生物视觉，让计算机理解图片或者视频中的内容。可以分为三类：</p><ol><li>图像分类：将图像结构化成类别信息，用实现确定好的类别来描述图片。</li><li>目标检测：关注特定的物体目标，要求获取这一目标的类别和位置信息，他和分类的区别是，分类将图片当做整体，目标检测可以在一张图片中获取多种目标及位置。</li><li>图像分割：分割是在检测的基础上还需要获取像素信息。</li></ol><p>应用领域：人脸识别、视频监控、智能驾驶、图片识别（以图搜图，图片鉴黄），比如抖音送礼的眼镜特效属于人脸识别里的人脸关键点定位。</p><h3 id=2-自然语言处理>2. 自然语言处理<a hidden class=anchor aria-hidden=true href=#2-自然语言处理>#</a></h3><p>语言模型是用来计算下一个句子概率的模型。</p><h3 id=3-时间序列>3 时间序列<a hidden class=anchor aria-hidden=true href=#3-时间序列>#</a></h3><p>时间序列是一种 <strong>有序的、依赖时间的结构化数据</strong>，其核心任务是研究和预测数据随时间变化的规律。深度学习为时间序列任务提供了更强大的建模工具，尤其是在处理复杂非线性关系或长时间依赖时。他是一个跨领域的问题，还会涉及到统计学等。</p><h3 id=3-机器学习的工作流程>3. 机器学习的工作流程：<a hidden class=anchor aria-hidden=true href=#3-机器学习的工作流程>#</a></h3><p>获取数据->对数据进行基本处理->特征工程->机器学习(模型训练，也是算法应用的过程)->模型评估，如果模型评估没有达到要求需要从对数据进行基本处理重新进行一遍。</p><h4 id=31-获取数据>3.1 获取数据<a hidden class=anchor aria-hidden=true href=#31-获取数据>#</a></h4><p>拿到的数据类似于表格，一行就是一个样本，一列就是一个特征，涉及到判断的列不叫特征，叫目标值，不是所有数据都有目标值。数据分为训练数据和测试数据，一般比例为3/7或者2/8。</p><h4 id=32-数据基本处理>3.2 数据基本处理<a hidden class=anchor aria-hidden=true href=#32-数据基本处理>#</a></h4><p>修改数据的空值、异常值、类型转换等。</p><h4 id=33-特征工程>3.3 特征工程<a hidden class=anchor aria-hidden=true href=#33-特征工程>#</a></h4><p>对数据的进一步处理。包括特征提取(比如将文本或者图片转换成可以用于机器学习的数字)，特征预处理(通过一些函数将数据转换成适合算法模型的特征数据)和特征降维(降低特征个数)。</p><h5 id=331-特征预处理>3.3.1 特征预处理<a hidden class=anchor aria-hidden=true href=#331-特征预处理>#</a></h5><p>将数据转换成机器更好识别，更好处理的数据。当特征数据的单位或者大小相差较大，或者某个特征的方差相比其他的方差大出好几个数量级，这种情况可能这个特征对结果的影响比较大，使得算法无法学习到其他特征。</p><h6 id=3311-归一化>3.3.1.1 归一化<a hidden class=anchor aria-hidden=true href=#3311-归一化>#</a></h6><p>把原始数据映射到某个区间内，默认0~1。计算公式为：
$$
X&rsquo; = \frac {x - min}{max - min}\</p><p>X&rsquo;&rsquo; = X&rsquo; * (mx - mi) + mi
$$
上面的公式中，X&rsquo;&lsquo;就是归一化处理后的特征值，max和min表示初始特征值的最大值和最小值，mx和mi表示想要将特征值映射到区间的最大值和最小值。</p><p>归一化的时候如果出现一条特征统计不正确，比如有1条数据比其他的大了很多倍或者小了很多倍，这种情况对其他数据的影响很大，所以这种方法鲁棒性较差，只适合精确小数据场景。</p><div class=highlight><div style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">9
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>import</span> pandas <span style=color:#ff79c6>as</span> pd
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.preprocessing <span style=color:#ff79c6>import</span> MinMaxScaler
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>data <span style=color:#ff79c6>=</span> pd<span style=color:#ff79c6>.</span>read_csv(<span style=color:#f1fa8c>&#39;./data/dating.txt&#39;</span>)
</span></span><span style=display:flex><span><span style=color:#6272a4># 实例化转化器</span>
</span></span><span style=display:flex><span>transfer <span style=color:#ff79c6>=</span> MinMaxScaler(featrue_range<span style=color:#ff79c6>=</span>(<span style=color:#bd93f9>3</span>, <span style=color:#bd93f9>5</span>))
</span></span><span style=display:flex><span><span style=color:#6272a4># 将目标列转换到指定区间，这里是3~5</span>
</span></span><span style=display:flex><span>ret_data <span style=color:#ff79c6>=</span> transfer<span style=color:#ff79c6>.</span>fit_transform(data[[<span style=color:#f1fa8c>&#34;列1名称&#34;</span>, <span style=color:#f1fa8c>&#34;列2名称&#34;</span>]])
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;转化后的数据:</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#34;</span>, ret_data)
</span></span></code></pre></td></tr></table></div></div><h6 id=3312-标准化>3.3.1.2 标准化<a hidden class=anchor aria-hidden=true href=#3312-标准化>#</a></h6><div class=highlight><div style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>import</span> pandas <span style=color:#ff79c6>as</span> pd
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.preprocessing <span style=color:#ff79c6>import</span> StandardScaler
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>data <span style=color:#ff79c6>=</span> pd<span style=color:#ff79c6>.</span>read_csv(<span style=color:#f1fa8c>&#39;./data/dating.txt&#39;</span>)
</span></span><span style=display:flex><span><span style=color:#6272a4># 实例化转化器</span>
</span></span><span style=display:flex><span>transfer <span style=color:#ff79c6>=</span> StandardScaler()
</span></span><span style=display:flex><span><span style=color:#6272a4># 将目标列转标准化</span>
</span></span><span style=display:flex><span>ret_data <span style=color:#ff79c6>=</span> transfer<span style=color:#ff79c6>.</span>fit_transform(data[[<span style=color:#f1fa8c>&#34;列1名称&#34;</span>, <span style=color:#f1fa8c>&#34;列2名称&#34;</span>]])
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;转化后的数据:</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#34;</span>, ret_data)
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;每一列的方差为:</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#34;</span>, transfer<span style=color:#ff79c6>.</span>var_)
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;每一列的平均值为:</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#34;</span>, transfer<span style=color:#ff79c6>.</span>mean_)
</span></span></code></pre></td></tr></table></div></div><p>第一步就是去均值，将平均值变成0，然后在比上标准差，这样能让所有维度的数相差不大：$X&rsquo; = \frac{x - avg}{\sigma}$。</p><h6 id=332-特征提取>3.3.2 特征提取<a hidden class=anchor aria-hidden=true href=#332-特征提取>#</a></h6><p>就是把数据转换成数字，主要包括字典特征提取，文本特征提取（统计单词次数），图像特征提取。</p><ul><li><p>字典特征提取：</p><div class=highlight><div style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">26
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.featrue_extraction <span style=color:#ff79c6>import</span> DictVectorizer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>dict_demo</span>():
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    字典特征提取
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    :return: None
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    data <span style=color:#ff79c6>=</span> [{<span style=color:#f1fa8c>&#39;city&#39;</span>: <span style=color:#f1fa8c>&#39;北京&#39;</span>, <span style=color:#f1fa8c>&#39;temperatrue&#39;</span>: <span style=color:#bd93f9>100</span>},
</span></span><span style=display:flex><span>            {<span style=color:#f1fa8c>&#39;city&#39;</span>: <span style=color:#f1fa8c>&#39;上海&#39;</span>, <span style=color:#f1fa8c>&#39;temperatrue&#39;</span>: <span style=color:#bd93f9>60</span>},
</span></span><span style=display:flex><span>            {<span style=color:#f1fa8c>&#39;city&#39;</span>: <span style=color:#f1fa8c>&#39;深圳&#39;</span>, <span style=color:#f1fa8c>&#39;temperatrue&#39;</span>: <span style=color:#bd93f9>30</span>}]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 字典特征提取</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 非sparse矩阵，比sparse矩阵遍历慢，费空间</span>
</span></span><span style=display:flex><span>    transfer <span style=color:#ff79c6>=</span> DictVectorizer(sparse<span style=color:#ff79c6>=</span><span style=color:#ff79c6>False</span>)
</span></span><span style=display:flex><span>    new_data <span style=color:#ff79c6>=</span> transfer<span style=color:#ff79c6>.</span>fit_transform(data)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;列名：</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#34;</span>, transfer<span style=color:#ff79c6>.</span>get_featrue_names_out())
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(new_data)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 控制台输出</span>
</span></span><span style=display:flex><span>属性名：
</span></span><span style=display:flex><span> [<span style=color:#f1fa8c>&#39;city=上海&#39;</span> <span style=color:#f1fa8c>&#39;city=北京&#39;</span> <span style=color:#f1fa8c>&#39;city=深圳&#39;</span> <span style=color:#f1fa8c>&#39;temperatrue&#39;</span>]
</span></span><span style=display:flex><span>[[  <span style=color:#bd93f9>0.</span>   <span style=color:#bd93f9>1.</span>   <span style=color:#bd93f9>0.</span> <span style=color:#bd93f9>100.</span>]
</span></span><span style=display:flex><span> [  <span style=color:#bd93f9>1.</span>   <span style=color:#bd93f9>0.</span>   <span style=color:#bd93f9>0.</span>  <span style=color:#bd93f9>60.</span>]
</span></span><span style=display:flex><span> [  <span style=color:#bd93f9>0.</span>   <span style=color:#bd93f9>0.</span>   <span style=color:#bd93f9>1.</span>  <span style=color:#bd93f9>30.</span>]]
</span></span></code></pre></td></tr></table></div></div></li><li><p>英文文本特征提取：</p><div class=highlight><div style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">29
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.featrue_extraction.text <span style=color:#ff79c6>import</span> CountVectorizer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>text_count_demo</span>():
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    英文文本特征提取
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    :return: None
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    text <span style=color:#ff79c6>=</span> [<span style=color:#f1fa8c>&#34;life is short,i like python&#34;</span>, <span style=color:#f1fa8c>&#34;life is too long,i dislike python&#34;</span>]
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 可以使用stop_words属性指定停用词</span>
</span></span><span style=display:flex><span>    transfer <span style=color:#ff79c6>=</span> CountVectorizer()
</span></span><span style=display:flex><span>    new_data <span style=color:#ff79c6>=</span> transfer<span style=color:#ff79c6>.</span>fit_transform(text)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;特征名:</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#34;</span>, transfer<span style=color:#ff79c6>.</span>get_featrue_names_out())
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(new_data)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 控制台输出，默认就是sparse矩阵，(0, 2)	1 表示在第一句话中，输出的特征名中的第二个单词出现1次，也就是在第一句话中is单词出现了1次。</span>
</span></span><span style=display:flex><span>特征名:
</span></span><span style=display:flex><span> [<span style=color:#f1fa8c>&#39;dislike&#39;</span> <span style=color:#f1fa8c>&#39;is&#39;</span> <span style=color:#f1fa8c>&#39;life&#39;</span> <span style=color:#f1fa8c>&#39;like&#39;</span> <span style=color:#f1fa8c>&#39;long&#39;</span> <span style=color:#f1fa8c>&#39;python&#39;</span> <span style=color:#f1fa8c>&#39;short&#39;</span> <span style=color:#f1fa8c>&#39;too&#39;</span>]
</span></span><span style=display:flex><span>  (<span style=color:#bd93f9>0</span>, <span style=color:#bd93f9>2</span>)	<span style=color:#bd93f9>1</span>
</span></span><span style=display:flex><span>  (<span style=color:#bd93f9>0</span>, <span style=color:#bd93f9>1</span>)	<span style=color:#bd93f9>1</span>
</span></span><span style=display:flex><span>  (<span style=color:#bd93f9>0</span>, <span style=color:#bd93f9>6</span>)	<span style=color:#bd93f9>1</span>
</span></span><span style=display:flex><span>  (<span style=color:#bd93f9>0</span>, <span style=color:#bd93f9>3</span>)	<span style=color:#bd93f9>1</span>
</span></span><span style=display:flex><span>  (<span style=color:#bd93f9>0</span>, <span style=color:#bd93f9>5</span>)	<span style=color:#bd93f9>1</span>
</span></span><span style=display:flex><span>  (<span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>2</span>)	<span style=color:#bd93f9>1</span>
</span></span><span style=display:flex><span>  (<span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>1</span>)	<span style=color:#bd93f9>1</span>
</span></span><span style=display:flex><span>  (<span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>5</span>)	<span style=color:#bd93f9>1</span>
</span></span><span style=display:flex><span>  (<span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>7</span>)	<span style=color:#bd93f9>1</span>
</span></span><span style=display:flex><span>  (<span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>4</span>)	<span style=color:#bd93f9>1</span>
</span></span><span style=display:flex><span>  (<span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>0</span>)	<span style=color:#bd93f9>1</span>
</span></span></code></pre></td></tr></table></div></div></li><li><p>中文文本特征提取：中文使用jieba分词现将中文分开，然后在使用CountVectorizer提取特征</p></li><li><p>Tf-idf文本特征提取：如果某个词或者某个短语在一篇文章中出现的概率比其他文章高，就认为这个词或短语具有很好的类别区分能力，适合是用来分类。Tf就是词频，idf是逆向文档频率，idf分2步得到，第一步先用总文件数除以包含该词的文件数，第二步将第一步得到的商取10为底的对数。Tf * idf就是最终的重要程度。</p></li></ul><h5 id=333-特征降维>3.3.3 特征降维<a hidden class=anchor aria-hidden=true href=#333-特征降维>#</a></h5><p>减少特征的数量，一般都是将相关性的特征去掉一些或者合并成1个，比如湿度和降雨量可能降维之后就剩一个降雨量。降维有2种方式：特征选择和主成分分析。</p><h6 id=3331-特征选择>3.3.3.1 特征选择<a hidden class=anchor aria-hidden=true href=#3331-特征选择>#</a></h6><p>就是提取主要特征，主要有2中实现：</p><ol><li><p>过滤式，探究特征和目标值之间的关联，使用低方差特征过滤(方差相差小表示很相近，就删掉)或者相关系数。</p><div class=highlight><div style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">30
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">31
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">32
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">33
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">34
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">35
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">36
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>import</span> pandas <span style=color:#ff79c6>as</span> pd
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.featrue_selection <span style=color:#ff79c6>import</span> VarianceThreshold
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> scipy.stats <span style=color:#ff79c6>import</span> pearsonr
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> scipy.stats <span style=color:#ff79c6>import</span> spearmanr
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>var_thr</span>():
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    特征降维：低方差特征过滤
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    :return:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    data <span style=color:#ff79c6>=</span> pd<span style=color:#ff79c6>.</span>read_csv(<span style=color:#f1fa8c>&#39;csv文件&#39;</span>)
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 实例化,方差小于1的会被过滤</span>
</span></span><span style=display:flex><span>    transfer <span style=color:#ff79c6>=</span> VarianceThreshold(threshold<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1</span>)
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 转换</span>
</span></span><span style=display:flex><span>    transfer_data <span style=color:#ff79c6>=</span> transfer<span style=color:#ff79c6>.</span>fit_transform(data<span style=color:#ff79c6>.</span>iloc[:, <span style=color:#bd93f9>1</span>:<span style=color:#bd93f9>10</span>])
</span></span><span style=display:flex><span>    <span style=color:#6272a4>#</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4>#</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>pea_demo</span>():
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    特征降维：相关系数
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    :return:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    x1 <span style=color:#ff79c6>=</span> [<span style=color:#bd93f9>12.5</span>, <span style=color:#bd93f9>43</span>, <span style=color:#bd93f9>15.3</span>, <span style=color:#bd93f9>14.4</span>]
</span></span><span style=display:flex><span>    x2 <span style=color:#ff79c6>=</span> [<span style=color:#bd93f9>21.2</span>, <span style=color:#bd93f9>23.1</span>, <span style=color:#bd93f9>44.2</span>, <span style=color:#bd93f9>19.5</span>]
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 皮尔逊相关系数</span>
</span></span><span style=display:flex><span>    ret <span style=color:#ff79c6>=</span> pearsonr(x1, x2)
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 斯皮尔曼相关系数</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># ret = spearmanr(x1, x2)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 无论哪个相关性公式都会输出2个值</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 第1个值大于0表示正相关，小于0表示负相关，等于0表示没有关系，|r|=1表示完全相关，|r|&lt;0.4是低度相关，0.4 &lt;=|r|&lt;0.7是显著相关，0.7 &lt;=|r|是高度相关</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 第二个值当样本数超过500时有参考意义，越接近0表示相关性越强</span>
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(ret)
</span></span></code></pre></td></tr></table></div></div></li><li><p>embedded嵌入式，算法自动选择特征，比如决策树、L1正则化。</p></li></ol><h6 id=3332-主成分分析>3.3.3.2 主成分分析<a hidden class=anchor aria-hidden=true href=#3332-主成分分析>#</a></h6><p>降维时舍弃原有数据，创建新变量:</p><div class=highlight><div style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.decomposition <span style=color:#ff79c6>import</span> PCA
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>data <span style=color:#ff79c6>=</span> [[<span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>2</span>, <span style=color:#bd93f9>3</span>], [<span style=color:#bd93f9>4</span>, <span style=color:#bd93f9>5</span>, <span style=color:#bd93f9>6</span>], [<span style=color:#bd93f9>7</span>, <span style=color:#bd93f9>8</span>, <span style=color:#bd93f9>9</span>]]
</span></span><span style=display:flex><span><span style=color:#6272a4># n_components传整数表示多留多少特征，小数表示保留特征的百分比</span>
</span></span><span style=display:flex><span>transfer <span style=color:#ff79c6>=</span> PCA(n_components<span style=color:#ff79c6>=</span><span style=color:#bd93f9>2</span>)
</span></span><span style=display:flex><span>transfer_data <span style=color:#ff79c6>=</span> transfer<span style=color:#ff79c6>.</span>fit_transform(data)
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;保留2列特征的结果:&#34;</span>, transfer_data)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 保留2列特征的结果: [[ 5.19615242  0.        ]</span>
</span></span><span style=display:flex><span><span style=color:#6272a4>#  [-0.          0.        ]</span>
</span></span><span style=display:flex><span><span style=color:#6272a4>#  [-5.19615242  0.        ]]</span>
</span></span></code></pre></td></tr></table></div></div><h4 id=34-机器学习>3.4 机器学习<a hidden class=anchor aria-hidden=true href=#34-机器学习>#</a></h4><p>选择合适的算法对模型进行训练。使用梯度下降算法时涉及到3个概念：</p><ol><li>epoch：将整个训练集完整送入模型进行1次训练，称为1代训练；</li><li>batch：将训练集分成多个小数据集，每个批次会被模型单独进行1次向前传播和反向传播；</li><li>iteration：每次模型完成一个 Batch 的前向传播和反向传播，进行一次参数更新，称为一次 Iteration；</li></ol><h4 id=35-模型评估>3.5 模型评估<a hidden class=anchor aria-hidden=true href=#35-模型评估>#</a></h4><p>对训练好的模型进行评估，训练的好就上线，不好就重来一遍。</p><h5 id=351-分类评估方法>3.5.1 分类评估方法<a hidden class=anchor aria-hidden=true href=#351-分类评估方法>#</a></h5><h6 id=3511-混淆矩阵>3.5.1.1 混淆矩阵<a hidden class=anchor aria-hidden=true href=#3511-混淆矩阵>#</a></h6><p>测试集和预测结果可以组成四种情况：</p><table><thead><tr><th></th><th>预测结果正例</th><th>预测结果假例</th></tr></thead><tbody><tr><td>真实结果正例</td><td>真正例TP</td><td>伪反例FN</td></tr><tr><td>真实结果假例</td><td>伪正例FP</td><td>真反例TN</td></tr></tbody></table><p>混淆矩阵中，真正例TP和真反例TN表示预测结果正确，通过混淆矩阵可以计算出几个指标：</p><ul><li>准确率：$\dfrac {真正例TP + 真反例TN} {真正例TP + 真反例TN + 伪正例FP + 伪反例FN}$</li><li>精确率：$\dfrac 真正例TP {真正例TP + 伪正例FP}$</li><li>召回率：也叫查全率，$\dfrac 真正例TP {真正例TP + 伪反例FN}$</li><li>F1-score：用来反映模型的稳健性：$\dfrac {2真正例TP} {2真正例TP + 伪正例FP + 伪反例FN}$</li></ul><p>上面这些指标在样本不均衡的时候是无法对模型进行评估的，比如一共有100条结果，99条真，1条假，如果机器不进行判断将所有结果都预测为真，精确率也能达到99%，召回率已经达到100%了，虽然模型在胡说，一般正例和反例的比例低于1/4就说名样本是<strong>不均衡</strong>的，不均衡的样本应该使用ROC曲线和AUC指标进行评估。</p><h6 id=3512-roc与auc>3.5.1.2 ROC与AUC<a hidden class=anchor aria-hidden=true href=#3512-roc与auc>#</a></h6><p>ROC曲线由TPR和FPR组成，TPR就是召回率，当做纵坐标；FPR是$\dfrac 伪正例FP {伪正例FP + 真反例TN}$​，当做横坐标，ROC可以看出来分类器是否在胡说。</p><p><img alt=ROC loading=lazy src=https://raw.githubusercontent.com/wangxiaohong123/p-bed/main/uPic/ROC.jpg></p><p>AUC是随机取一堆正负样本，正样本得分大于负样本得分的概率，AUC的范围在[0,1]之间，越接近1效果越好。</p><h5 id=352-交叉验证网格搜索>3.5.2 交叉验证，网格搜索<a hidden class=anchor aria-hidden=true href=#352-交叉验证网格搜索>#</a></h5><p>交叉验证是说将数据集进一步划分，原来只分成训练集和测试集，现在需要将训练集分成验证集和训练集，比如将训练集分成4份，叫做<strong>4折交叉验证</strong>，然后对数据进行4次的测试，第一次前1/4当作验证集，第二次事1/2到1/4当作验证集……，将4次结果的平均值当作结果，这样可以<strong>让模型更加准确，但是不能提高准确率</strong>。</p><p>提高准确率需要使用网格搜索，算法中需要手动指定的参数叫做<strong>超参数</strong>，比如<code>KNeighborsClassifier(n_neighbors=5)</code>中的5就是超参数，网格搜索就是预先定义n个超参数，每组超参数都采用交叉验证来评估，最后选出最优的组合。</p><h5 id=353-欠拟合和过拟合>3.5.3 欠拟合和过拟合<a hidden class=anchor aria-hidden=true href=#353-欠拟合和过拟合>#</a></h5><p>过拟合是在训练上表现好，测试上表现不好，这个时候就是模型过于复杂，出现过拟合；欠拟合就是训练和测试表现都不好，模型过于简单，出现欠拟合。</p><p>过拟合可能是因为特征过多，解决的办法是重新清洗数据、增加训练数据量、正则化和减少特征维度；解决欠拟合就是增加特征项。</p><h6 id=3531-正则化>3.5.3.1 正则化<a hidden class=anchor aria-hidden=true href=#3531-正则化>#</a></h6><p>正则化是通过在损失函数中增加惩罚项对模型的参数进行约束，正则化分为2种</p><ul><li>L1正则化：可以让一些模型参数变成0，比如$w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4^2$会变成$w_1x_1 + w_2x_2 + w_3x_3$​，也叫lasso回归；</li><li>L2正则化：认为一些参数是有用的，不会变成0，会变成一个比较小的常数值比如$w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4^2$会变成$w_1x_1 + w_2x_2 + w_3x_3 + b$。也叫岭回归（Ridge 回归）。</li></ul><h5 id=354-损失函数>3.5.4 损失函数<a hidden class=anchor aria-hidden=true href=#354-损失函数>#</a></h5><p>损失函数是用于衡量模型预测结果与真实结果之间差异的函数。损失过大的时候需要使用正规方程或者梯度下降进行优化。</p><h6 id=3541-正规方程>3.5.4.1 正规方程<a hidden class=anchor aria-hidden=true href=#3541-正规方程>#</a></h6><p>$w = (x^Tx)^-1x^Ty$，X是特征值矩阵，y是目标值矩阵，-1表示逆矩阵。他是根据损失函数求偏导推到出来的。他可以求出误差最小的系数，但是求解逆矩阵是非常困难的，所以正规方程一般用到小数据集里。正规方程不能解决拟合问题。</p><h6 id=3542-梯度下降>3.5.4.2 梯度下降<a hidden class=anchor aria-hidden=true href=#3542-梯度下降>#</a></h6><p>在单变量中，梯度表示某一点切线的斜率，也就是函数的微分；在多变量函数中，梯度是一个向量，梯度的方向就是函数在给定点上升最快的方向。$\theta_{i+1} = \theta_1 - \alpha{\frac \partial{\partial \theta_1}}J(\theta)$。α表示步长，即每次下降多少，太小容易增加计算复杂度，太大容易错过最低点，并且他没法保证找到的是最小值。</p><p>两种方法的demo：</p><div class=highlight><div style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">30
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.datasets <span style=color:#ff79c6>import</span> load_boston
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.model_selection <span style=color:#ff79c6>import</span> train_test_split
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.preprocessing <span style=color:#ff79c6>import</span> StandardScaler
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.linear_model <span style=color:#ff79c6>import</span> LinearRegression, SGDRegressor
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.metrics <span style=color:#ff79c6>import</span> mean_squared_error
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 获取数据</span>
</span></span><span style=display:flex><span>boston <span style=color:#ff79c6>=</span> load_boston()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 数据基本处理-分割</span>
</span></span><span style=display:flex><span>x_train, x_test, y_train, y_test <span style=color:#ff79c6>=</span> train_test_split(boston<span style=color:#ff79c6>.</span>data, boston<span style=color:#ff79c6>.</span>target, test_size<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.2</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 特征工程-标准化</span>
</span></span><span style=display:flex><span>transfer <span style=color:#ff79c6>=</span> StandardScaler()
</span></span><span style=display:flex><span>x_train <span style=color:#ff79c6>=</span> transfer<span style=color:#ff79c6>.</span>fit_transform(x_train)
</span></span><span style=display:flex><span>x_test <span style=color:#ff79c6>=</span> transfer<span style=color:#ff79c6>.</span>fit_transform(x_test)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 机器学习-正规方程</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># estimator = LinearRegression()</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 机器学习-梯度下降</span>
</span></span><span style=display:flex><span>estimator <span style=color:#ff79c6>=</span> SGDRegressor()
</span></span><span style=display:flex><span>estimator<span style=color:#ff79c6>.</span>fit(x_train, y_train)
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;模型的偏置是:</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#34;</span>, estimator<span style=color:#ff79c6>.</span>intercept_)
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;模型的系数是:</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#34;</span>, estimator<span style=color:#ff79c6>.</span>coef_)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 模型评估</span>
</span></span><span style=display:flex><span>y_pre <span style=color:#ff79c6>=</span> estimator<span style=color:#ff79c6>.</span>predict(x_test)
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;预测值是:</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#34;</span>, y_pre)
</span></span><span style=display:flex><span>ret <span style=color:#ff79c6>=</span> mean_squared_error(y_test, y_pre)
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;均方误差:</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#34;</span>, ret)
</span></span></code></pre></td></tr></table></div></div><h5 id=355-模型的保存和加载>3.5.5 模型的保存和加载<a hidden class=anchor aria-hidden=true href=#355-模型的保存和加载>#</a></h5><p>使用joblib包实现模型的保存和加载：</p><div class=highlight><div style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">5
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>import</span> joblib
</span></span><span style=display:flex><span><span style=color:#6272a4># 保存模型</span>
</span></span><span style=display:flex><span>joblib<span style=color:#ff79c6>.</span>dump(estimator, <span style=color:#f1fa8c>&#39;路径/xxx.pkl&#39;</span>)
</span></span><span style=display:flex><span><span style=color:#6272a4>#加载模型</span>
</span></span><span style=display:flex><span>estimator <span style=color:#ff79c6>=</span> joblib<span style=color:#ff79c6>.</span>load(<span style=color:#f1fa8c>&#39;路径/xxx.pkl&#39;</span>)
</span></span></code></pre></td></tr></table></div></div><h3 id=4-迁移学习>4 迁移学习<a hidden class=anchor aria-hidden=true href=#4-迁移学习>#</a></h3><p>迁移学习是一种机器学习技术，它的核心思想是将从一个任务中学到的知识迁移到另一个相关的任务中，从而加速学习过程或提高模型的性能。说白了就是把别人训练好的模型，拿来改一改变成适合自己的。主要包含2个步骤：</p><p>预训练模型：一般情况下，预训练模型都是大模型，具备复杂的网络结构，众多的参数量以及足够大的数据集下进行训练产生的模型，常见的预训练模型：BERT及其变体、GPT、GPT-2、roBERTa、transformer-XL、XLNet、XLM、DistilBERT、ALBERT、T5、XLM-RoBERTa。</p><p>预训练模型的使用：首先安装依赖：</p><div class=highlight><div style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>pip3 install tqdm boto3 requests regex sentencepiece sacremoses
</span></span></code></pre></td></tr></table></div></div><p>使用pytorch.hub加载模型：</p><div class=highlight><div style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">26
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>import</span> torch
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 选定预训练模型的来源</span>
</span></span><span style=display:flex><span>source <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#39;huggingface/pytorch-transformers&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 选定加载模型的哪一部分,常用的4个选项：</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># tokenizer：映射器</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># model：不带头的模型，这个头不是多头注意力的头，而是表示输出层，不带头的意思是仅对输入文本进行特征表示</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># modelWithLMHead：带语言模型头的模型</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># ModelForSequenceClassification：带类型头的模型</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># modelForQuestionAnswering：带问答头的模型</span>
</span></span><span style=display:flex><span>part <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#39;tokenizer&#39;</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 加载模型的名称</span>
</span></span><span style=display:flex><span>model_name <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#39;bert-base-chinese&#39;</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 加载预训练模型映射器</span>
</span></span><span style=display:flex><span>tokenizer <span style=color:#ff79c6>=</span> torch<span style=color:#ff79c6>.</span>hub<span style=color:#ff79c6>.</span>load(source, part, model_name)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 将测试文本转换成模型可以处理的张量</span>
</span></span><span style=display:flex><span>token_tensor <span style=color:#ff79c6>=</span> torch<span style=color:#ff79c6>.</span>tensor([model<span style=color:#ff79c6>.</span>encode(<span style=color:#f1fa8c>&#39;测试文本&#39;</span>)])
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;token_tensor:&#34;</span>, token_tensor)
</span></span><span style=display:flex><span><span style=color:#6272a4># 推理阶段不需要计算梯度，使用torch.no_grad()禁用梯度计算</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>with</span> torch<span style=color:#ff79c6>.</span>no_grad():
</span></span><span style=display:flex><span>    encoded_layers, _ <span style=color:#ff79c6>=</span> model(token_tensor)
</span></span><span style=display:flex><span><span style=color:#6272a4># 输出结果，这是隐藏层的输出，不是最终的输出</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(encoded_layers)
</span></span></code></pre></td></tr></table></div></div><p>微调(Fine-tuning)：根据给定的预训练模型，改变他的部分参数或者小分部结构，然后在小部分的数据集上训练，让整个模型更好的适应特定任务。所以微调也是一个训练过程。</p><h3 id=5-下游任务>5 下游任务<a hidden class=anchor aria-hidden=true href=#5-下游任务>#</a></h3><p><strong>下游任务</strong>是指利用一个预训练模型在某些特定的应用场景中执行的任务，比如问答系统，机器翻译等。</p><ol><li>llama：Meta开源的语言模型，他可以让很少的权重参数达到大模型的效果。</li><li>LoRA：正常的微调大模型需要重跑权重参数，LoRA可以实现微调时冻住大模型的权重参数，单独跑个小模型然后合并到大模型中。</li><li>Self-Instruct：把样本数据变成方便理解，更能突出重点的过程叫润色，比如 “取快递”润色成“您好，请问我的快递在哪里”，Self-Instruct可以让都有一个标准的格式。</li></ol><p>PEFT：Hugging Face将llama、LoRA和Self-Instruct整合到了一起。</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://wangxiaohong123.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a></li></ul><nav class=paginav><a class=prev href=https://wangxiaohong123.github.io/posts/%E5%9F%BA%E7%A1%80/%E6%95%B0%E6%8D%AE%E5%BA%93/es/2.%E5%AE%89%E8%A3%85/><span class=title>« Prev</span><br><span>2.安装es</span>
</a><a class=next href=https://wangxiaohong123.github.io/posts/%E6%A1%86%E6%9E%B6/spring/framework/2.%E6%B3%A8%E5%85%A5%E7%9B%B8%E5%85%B3/><span class=title>Next »</span><br><span>2.注入相关</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share 2.机器学习基础 on x" href="https://x.com/intent/tweet/?text=2.%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e5%9f%ba%e7%a1%80&amp;url=https%3a%2f%2fwangxiaohong123.github.io%2fposts%2fai%2f2.%25E6%259C%25BA%25E5%2599%25A8%25E5%25AD%25A6%25E4%25B9%25A0%25E5%259F%25BA%25E7%25A1%2580%2f&amp;hashtags=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 2.机器学习基础 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fwangxiaohong123.github.io%2fposts%2fai%2f2.%25E6%259C%25BA%25E5%2599%25A8%25E5%25AD%25A6%25E4%25B9%25A0%25E5%259F%25BA%25E7%25A1%2580%2f&amp;title=2.%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e5%9f%ba%e7%a1%80&amp;summary=2.%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e5%9f%ba%e7%a1%80&amp;source=https%3a%2f%2fwangxiaohong123.github.io%2fposts%2fai%2f2.%25E6%259C%25BA%25E5%2599%25A8%25E5%25AD%25A6%25E4%25B9%25A0%25E5%259F%25BA%25E7%25A1%2580%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 2.机器学习基础 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fwangxiaohong123.github.io%2fposts%2fai%2f2.%25E6%259C%25BA%25E5%2599%25A8%25E5%25AD%25A6%25E4%25B9%25A0%25E5%259F%25BA%25E7%25A1%2580%2f&title=2.%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e5%9f%ba%e7%a1%80"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 2.机器学习基础 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fwangxiaohong123.github.io%2fposts%2fai%2f2.%25E6%259C%25BA%25E5%2599%25A8%25E5%25AD%25A6%25E4%25B9%25A0%25E5%259F%25BA%25E7%25A1%2580%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 2.机器学习基础 on whatsapp" href="https://api.whatsapp.com/send?text=2.%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e5%9f%ba%e7%a1%80%20-%20https%3a%2f%2fwangxiaohong123.github.io%2fposts%2fai%2f2.%25E6%259C%25BA%25E5%2599%25A8%25E5%25AD%25A6%25E4%25B9%25A0%25E5%259F%25BA%25E7%25A1%2580%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 2.机器学习基础 on telegram" href="https://telegram.me/share/url?text=2.%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e5%9f%ba%e7%a1%80&amp;url=https%3a%2f%2fwangxiaohong123.github.io%2fposts%2fai%2f2.%25E6%259C%25BA%25E5%2599%25A8%25E5%25AD%25A6%25E4%25B9%25A0%25E5%259F%25BA%25E7%25A1%2580%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 2.机器学习基础 on ycombinator" href="https://news.ycombinator.com/submitlink?t=2.%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e5%9f%ba%e7%a1%80&u=https%3a%2f%2fwangxiaohong123.github.io%2fposts%2fai%2f2.%25E6%259C%25BA%25E5%2599%25A8%25E5%25AD%25A6%25E4%25B9%25A0%25E5%259F%25BA%25E7%25A1%2580%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://wangxiaohong123.github.io/>王小红的笔记</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>