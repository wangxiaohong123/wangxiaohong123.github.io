<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>3.机器学习算法 | 王小红的笔记</title><meta name=keywords content="机器学习"><meta name=description content="距离度量
一些算法会需要距离度量，比如K近邻、SVM、聚类等，距离有4个特性：

非负性：两点的距离不能小于0；
同一性：两点的距离=0说明时同1个点；
对称性：x到y的距离时0时，y到x的距离也是0；
直递性：dist(i,j)<=dist(i,k) + dist(k,j);

常见的距离公式：

欧式距离：两个点就是勾股定理，n个点就开根号n；
曼哈顿距离：曼哈顿距离是当两个点不能练成直线时的距离，计算公式：$\sum_{k=1}^n\vert X_{k}-X_{k-1} \vert$；
切比雪夫距离：类比于国际象棋中的国王走棋的方式，国王可以在一个步长内向任何方向移动，两点的距离公式为$D(P,Q)=max(\vert x_1−x_2\vert,\vert y_1−y_2\vert,&mldr;,\vert x_n−x_{n+1}\vert)$​；
闵氏距离：闵氏距离是将上面3个变成了1个公式，当p=1的时候是曼哈顿距离，p=2的时候是欧式距离，p>=3的时候是切比雪夫距离

1 K近邻算法
根据最近的距离判断类别，最近的样本数据是什么类别，你就是什么类别，这里的样本数量可以取n个。也叫KNN算法。
求两个坐标的距离使用勾股定理，多维也是一样的。
比如现有数据：

  
      
          电影名
          搞笑镜头
          拥抱镜头
          打斗镜头
          电影类型
      
  
  
      
          功夫熊猫
          39
          0
          31
          喜剧片
      
      
          叶问3
          3
          2
          65
          动作片
      
      
          二次曝光
          2
          3
          55
          爱情片
      
      
          代理情人
          9
          38
          2
          爱情片
      
      
          步步惊心
          8
          34
          17
          爱情片
      
      
          谍影重重
          5
          3
          57
          动作片
      
      
          美人鱼
          21
          17
          5
          喜剧片
      
      
          小鬼当家
          45
          2
          9
          喜剧片
      
      
          唐人街探案
          23
          3
          17
          
      
  

唐人街探案时测试数据，上面的事样本数据，需要判断唐人街探案是什么类型的电影时，就要先求出唐人街探案距离每个电影的距离："><meta name=author content><link rel=canonical href=https://wangxiaohong123.github.io/posts/ai/3.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/><link crossorigin=anonymous href=/assets/css/stylesheet.08f7d74f0ada0f975d29ae436285b61ed7a719d05f350cb888d00341642995a2.css integrity="sha256-CPfXTwraD5ddKa5DYoW2HtenGdBfNQy4iNADQWQplaI=" rel="preload stylesheet" as=style><link rel=icon href=https://wangxiaohong123.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://wangxiaohong123.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://wangxiaohong123.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://wangxiaohong123.github.io/apple-touch-icon.png><link rel=mask-icon href=https://wangxiaohong123.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://wangxiaohong123.github.io/posts/ai/3.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://wangxiaohong123.github.io/posts/ai/3.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"><meta property="og:site_name" content="王小红的笔记"><meta property="og:title" content="3.机器学习算法"><meta property="og:description" content="距离度量 一些算法会需要距离度量，比如K近邻、SVM、聚类等，距离有4个特性：
非负性：两点的距离不能小于0； 同一性：两点的距离=0说明时同1个点； 对称性：x到y的距离时0时，y到x的距离也是0； 直递性：dist(i,j)<=dist(i,k) + dist(k,j); 常见的距离公式：
欧式距离：两个点就是勾股定理，n个点就开根号n； 曼哈顿距离：曼哈顿距离是当两个点不能练成直线时的距离，计算公式：$\sum_{k=1}^n\vert X_{k}-X_{k-1} \vert$； 切比雪夫距离：类比于国际象棋中的国王走棋的方式，国王可以在一个步长内向任何方向移动，两点的距离公式为$D(P,Q)=max(\vert x_1−x_2\vert,\vert y_1−y_2\vert,…,\vert x_n−x_{n+1}\vert)$​； 闵氏距离：闵氏距离是将上面3个变成了1个公式，当p=1的时候是曼哈顿距离，p=2的时候是欧式距离，p>=3的时候是切比雪夫距离 1 K近邻算法 根据最近的距离判断类别，最近的样本数据是什么类别，你就是什么类别，这里的样本数量可以取n个。也叫KNN算法。
求两个坐标的距离使用勾股定理，多维也是一样的。
比如现有数据：
电影名 搞笑镜头 拥抱镜头 打斗镜头 电影类型 功夫熊猫 39 0 31 喜剧片 叶问3 3 2 65 动作片 二次曝光 2 3 55 爱情片 代理情人 9 38 2 爱情片 步步惊心 8 34 17 爱情片 谍影重重 5 3 57 动作片 美人鱼 21 17 5 喜剧片 小鬼当家 45 2 9 喜剧片 唐人街探案 23 3 17 唐人街探案时测试数据，上面的事样本数据，需要判断唐人街探案是什么类型的电影时，就要先求出唐人街探案距离每个电影的距离："><meta property="og:locale" content="zh-CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:tag" content="机器学习"><meta name=twitter:card content="summary"><meta name=twitter:title content="3.机器学习算法"><meta name=twitter:description content="距离度量
一些算法会需要距离度量，比如K近邻、SVM、聚类等，距离有4个特性：

非负性：两点的距离不能小于0；
同一性：两点的距离=0说明时同1个点；
对称性：x到y的距离时0时，y到x的距离也是0；
直递性：dist(i,j)<=dist(i,k) + dist(k,j);

常见的距离公式：

欧式距离：两个点就是勾股定理，n个点就开根号n；
曼哈顿距离：曼哈顿距离是当两个点不能练成直线时的距离，计算公式：$\sum_{k=1}^n\vert X_{k}-X_{k-1} \vert$；
切比雪夫距离：类比于国际象棋中的国王走棋的方式，国王可以在一个步长内向任何方向移动，两点的距离公式为$D(P,Q)=max(\vert x_1−x_2\vert,\vert y_1−y_2\vert,&mldr;,\vert x_n−x_{n+1}\vert)$​；
闵氏距离：闵氏距离是将上面3个变成了1个公式，当p=1的时候是曼哈顿距离，p=2的时候是欧式距离，p>=3的时候是切比雪夫距离

1 K近邻算法
根据最近的距离判断类别，最近的样本数据是什么类别，你就是什么类别，这里的样本数量可以取n个。也叫KNN算法。
求两个坐标的距离使用勾股定理，多维也是一样的。
比如现有数据：

  
      
          电影名
          搞笑镜头
          拥抱镜头
          打斗镜头
          电影类型
      
  
  
      
          功夫熊猫
          39
          0
          31
          喜剧片
      
      
          叶问3
          3
          2
          65
          动作片
      
      
          二次曝光
          2
          3
          55
          爱情片
      
      
          代理情人
          9
          38
          2
          爱情片
      
      
          步步惊心
          8
          34
          17
          爱情片
      
      
          谍影重重
          5
          3
          57
          动作片
      
      
          美人鱼
          21
          17
          5
          喜剧片
      
      
          小鬼当家
          45
          2
          9
          喜剧片
      
      
          唐人街探案
          23
          3
          17
          
      
  

唐人街探案时测试数据，上面的事样本数据，需要判断唐人街探案是什么类型的电影时，就要先求出唐人街探案距离每个电影的距离："><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://wangxiaohong123.github.io/posts/"},{"@type":"ListItem","position":2,"name":"3.机器学习算法","item":"https://wangxiaohong123.github.io/posts/ai/3.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"3.机器学习算法","name":"3.机器学习算法","description":"距离度量 一些算法会需要距离度量，比如K近邻、SVM、聚类等，距离有4个特性：\n非负性：两点的距离不能小于0； 同一性：两点的距离=0说明时同1个点； 对称性：x到y的距离时0时，y到x的距离也是0； 直递性：dist(i,j)\u0026lt;=dist(i,k) + dist(k,j); 常见的距离公式：\n欧式距离：两个点就是勾股定理，n个点就开根号n； 曼哈顿距离：曼哈顿距离是当两个点不能练成直线时的距离，计算公式：$\\sum_{k=1}^n\\vert X_{k}-X_{k-1} \\vert$； 切比雪夫距离：类比于国际象棋中的国王走棋的方式，国王可以在一个步长内向任何方向移动，两点的距离公式为$D(P,Q)=max(\\vert x_1−x_2\\vert,\\vert y_1−y_2\\vert,\u0026hellip;,\\vert x_n−x_{n+1}\\vert)$​； 闵氏距离：闵氏距离是将上面3个变成了1个公式，当p=1的时候是曼哈顿距离，p=2的时候是欧式距离，p\u0026gt;=3的时候是切比雪夫距离 1 K近邻算法 根据最近的距离判断类别，最近的样本数据是什么类别，你就是什么类别，这里的样本数量可以取n个。也叫KNN算法。\n求两个坐标的距离使用勾股定理，多维也是一样的。\n比如现有数据：\n电影名 搞笑镜头 拥抱镜头 打斗镜头 电影类型 功夫熊猫 39 0 31 喜剧片 叶问3 3 2 65 动作片 二次曝光 2 3 55 爱情片 代理情人 9 38 2 爱情片 步步惊心 8 34 17 爱情片 谍影重重 5 3 57 动作片 美人鱼 21 17 5 喜剧片 小鬼当家 45 2 9 喜剧片 唐人街探案 23 3 17 唐人街探案时测试数据，上面的事样本数据，需要判断唐人街探案是什么类型的电影时，就要先求出唐人街探案距离每个电影的距离：\n","keywords":["机器学习"],"articleBody":"距离度量 一些算法会需要距离度量，比如K近邻、SVM、聚类等，距离有4个特性：\n非负性：两点的距离不能小于0； 同一性：两点的距离=0说明时同1个点； 对称性：x到y的距离时0时，y到x的距离也是0； 直递性：dist(i,j)\u003c=dist(i,k) + dist(k,j); 常见的距离公式：\n欧式距离：两个点就是勾股定理，n个点就开根号n； 曼哈顿距离：曼哈顿距离是当两个点不能练成直线时的距离，计算公式：$\\sum_{k=1}^n\\vert X_{k}-X_{k-1} \\vert$； 切比雪夫距离：类比于国际象棋中的国王走棋的方式，国王可以在一个步长内向任何方向移动，两点的距离公式为$D(P,Q)=max(\\vert x_1−x_2\\vert,\\vert y_1−y_2\\vert,…,\\vert x_n−x_{n+1}\\vert)$​； 闵氏距离：闵氏距离是将上面3个变成了1个公式，当p=1的时候是曼哈顿距离，p=2的时候是欧式距离，p\u003e=3的时候是切比雪夫距离 1 K近邻算法 根据最近的距离判断类别，最近的样本数据是什么类别，你就是什么类别，这里的样本数量可以取n个。也叫KNN算法。\n求两个坐标的距离使用勾股定理，多维也是一样的。\n比如现有数据：\n电影名 搞笑镜头 拥抱镜头 打斗镜头 电影类型 功夫熊猫 39 0 31 喜剧片 叶问3 3 2 65 动作片 二次曝光 2 3 55 爱情片 代理情人 9 38 2 爱情片 步步惊心 8 34 17 爱情片 谍影重重 5 3 57 动作片 美人鱼 21 17 5 喜剧片 小鬼当家 45 2 9 喜剧片 唐人街探案 23 3 17 唐人街探案时测试数据，上面的事样本数据，需要判断唐人街探案是什么类型的电影时，就要先求出唐人街探案距离每个电影的距离：\n比如唐人街探案距离功夫熊猫的距离：$\\sqrt[3]{(23 - 39)^2 + (3 - 0)^2 + (17 - 31)^2} \\approx 21.47$，依次算出唐人街探案和所有电影的距离：\n电影名 搞笑镜头 拥抱镜头 打斗镜头 电影类型 距离 功夫熊猫 39 0 31 喜剧片 21.47 叶问3 3 2 65 动作片 52.01 二次曝光 2 3 55 爱情片 43.42 代理情人 9 38 2 爱情片 40.57 步步惊心 8 34 17 爱情片 34.44 谍影重重 5 3 57 动作片 43.87 美人鱼 21 17 5 喜剧片 18.55 小鬼当家 45 2 9 喜剧片 23.43 唐人街探案 23 3 17 假设K=5时，就是找到距离最小的5个电影：美人鱼、小鬼当家、功夫熊猫、步步惊心、代理情人，这里有3个喜剧片和2个爱情片，所以推测唐人街探案是喜剧片。\n1.1 基本使用 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from sklearn.neighbors import KNeighborsClassifier # 构造数据 # 样本 x = [[1], [2], [10], [20]] # 特征 y = [0, 0, 1, 1] # 训练模型 # 实例化估计器 # n_neighbors表示参考几个邻居 estimator = KNeighborsClassifier(n_neighbors=2) # 训练 estimator.fit(x, y) # 预测 print(estimator.predict([[100]])) 1.2 kd树 kd树可以理解成一个多维度的平衡二叉树，每个节点存储多个维度的数据，比如(x，y)，(x，y，z)，就算有多个维度也只能按1个维度进行排序，但是每层排序的维度可能不一样，利用分维度排序的方式，将多维数据划分为超矩形（hyper-rectangle）区域，从而支持快速区域查询和最近邻查询。\n1.2.1 KD树的构建 一般是选择方差较大的维度进行划分，首先将每个维度各拆分成1个数组计算方差，找到方差最大的维度，将方差最大的维度进行排序，找到中位数作为根节点。如果想要在坐标系中划分出超矩阵，那么kd树每层的划分维度就不能相同。\n以数据{(2,3),(5,4),(9,6),(4,7),(8,1),(7,2)}数据举例\n两个维度的数据分别是[2,5,9,4,8,7]和[3,4,6,7,1,2]，第一个维度的方差约等于5.8，第二个维度的方差约等于4.5，所以根节点应该取第一维度的数据的中位数，也就是(5,4)或者(7,2)都可以，假设选择(7,2)当做根节点，此时(2,3),(5,4),(4,7)都在根节点的左侧，(9,6)和(8,1)在根节点的右侧，因为要划分超矩阵，所以第二层使用第二维度来划分(2,3),(5,4),(4,7)和(9,6)和(8,1)，第三层在使用第一维度划分，最后划分出的逻辑上的kd树和超矩阵类似下面这样：\n更多维度是一样的道理，每层都选最分散的维度进行划分。\n1.2.2 KD树的搜索 搜索的时候首先从根节点遍历树，找到里目标点位最近的点，遍历的时候需要把路径上经过的点存到队列中，因为KD树有多个维度，并且每层都是用不同维度，所以当前找到的点可能不是举例最近的点，此时需要计算两点的距离，以这个距离为半径画圆；此时回溯队列计算最短距离，如果园和队列中取出的点划分的矩阵相交就需要将这个矩阵中的点拿出来放到队列中，如果没相交就继续遍历队列中的下一个点。这样就可以找到最短距离的点位。\n1.3 KNN算法完整代码： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 # 获取数据集 from sklearn.datasets import load_iris # 将数据分割成训练数据和测试数据,GridSearchCV用来网格搜索和交叉验证 from sklearn.model_selection import train_test_split, GridSearchCV # 特征与处理 from sklearn.preprocessing import StandardScaler # 导入算法 from sklearn.neighbors import KNeighborsClassifier # 1.获取数据 iris = load_iris() # 2.数据基本处理,因为拿到的数据很规范，这里只把数据进行训练和测试的分割 x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=22) # 3.特征工程-特征预处理 transfer = StandardScaler() x_train = transfer.fit_transform(x_train) x_test = transfer.fit_transform(x_test) # 4.机器学习-KNN # 这里不能指定n_neighbors=5，因为下面需要使用GridSearchCV进行调优 estimator = KNeighborsClassifier() # 模型调优，网格搜索 param_grid = {'n_neighbors': [1, 2, 3, 4, 5]} # param_grid:预设的超参数，cv:几折交叉验证 estimator = GridSearchCV(estimator, param_grid=param_grid, cv=4) # 模型训练 estimator.fit(x_train, y_train) # 5.模型评估 y_pre = estimator.predict(x_test) print(\"预测值是:\\n\", y_pre) print(\"预测值和真实值的对比是:\\n\", y_pre == y_test) score = estimator.score(x_test, y_test) print(\"准确率为:\\n\", score) # 查看交叉验证，网格搜索的属性 print(\"在交叉验证中得到的最好结果是:\\n\", estimator.best_score_) print(\"在交叉验证中得到的最好的模型是:\\n\", estimator.best_estimator_) print(\"在交叉验证中得到的模型结果是:\\n\", estimator.cv_results_) K近邻的有点就是简单，他没有模型，它属于惰性训练，效率不高。适合大样本自动分类，输出可解释性不强。\n2 线性回归 线性回归就是利用回归方程，对一个或者多个特征值(自变量)和目标值(因变量)之间进行建模。只有一个自变量叫单变量回归，多个自变量叫多元回归。\n线性关系回归公式：$h(w) = w_1x_1 + w_1x_1 + ··· + b$，可以理解为两个向量相乘：$\\left(\\begin{matrix}b\\w_1\\w_2\\end{matrix}\\right) * \\left(\\begin{matrix}1\\x_1\\x_2\\end{matrix}\\right)$。\n上面说的是线性关系，如果是非线性关系就需要高次项，比如$h(w) = w_1x_1^2 + w_1x_1^2 + ··· + b$。\n线性回归的简单使用：\n1 2 3 4 5 6 7 8 9 10 11 12 from sklearn.linear_model import LinearRegression x = [[80, 86], [82, 80], [85, 78], [90, 90], [86, 82]] y = [84.2, 80.6, 80.1, 90, 83.2] # 实例化估计器，这里没有手动指定系数 estimator = LinearRegression() # 训练 estimator.fit(x, y) print(\"线性回归的系数是:\\n\", estimator.coef_) print(\"输出预测结果:\\n\", estimator.predict([[100, 80]])) 3 逻辑回归 逻辑回归是一种分类的算法，主要解决而分类问题，适用于垃圾邮件判断、金融诈骗、虚假账号判断等等。\n逻辑回归的输入就是线性回归的输出，将线性回归的输出映射到一个概率值进行分类。\n逻辑函数(Sigmoid函数)：$\\hat y = \\sigma(z) = \\dfrac 1{1 + e^ {-z}}$其中$\\hat y$就是预测的概率值，z就是线性回归的输出h(w)：$h(w) = w_1x_1 + w_1x_1 + ··· + b$。在而分类问题中，通常会有一个阈值(默认是0.5)，概率大于阈值时模型的预测结果为1，否则为0。\n逻辑函数的损失函数使用对数似然损失函数或者交叉熵损失函数：$L(\\beta) = - \\sum_{i=1}^m \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]$，优化使用梯度下降函数。\n逻辑回归demo:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression # 获取数据 names = [\"zz\", \"xxx\", \"dsds\"] data = pd.read_csv(\"文件链接\", names=names) # 数据处理 data = data.replace(to_replace=\"?\", value=np.nan) data = data.dropna() # 特征值 x = data.iloc[:, 1:-1] # 目标值 y = data[\"Class\"] # 数据分隔 x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2) # 特征工程 transfer = StandardScaler() x_train = transfer.fit_transform(x_train) x_test = transfer.fit_transform(x_test) # 机器学习 estimator = LogisticRegression() estimator.fit(x_train, y_train) # 模型评估 res = estimator.score(x_test, y_test) 4 决策树 决策树的每个节点代表一个属性的判断，每个分支代表判断结果的输出，理解起来就像if-else结构。需要注意的是条件判断的优先级，就是如何决定哪个条件是第一次判断，哪个条件是第二次判断，由信息熵决定。\n熵用来衡量物体的混乱程度，系统越混乱或者越分散，熵值越高，越有序熵值越低。在信息熵种，如果系统的有序状态一致，数据越集中的地方熵值越小；当数据量相同的时候系统越有序熵值越低。\n决策树划分方法 信息增益：以某特征划分数据集前后的熵的差值，熵可以表示样本集合的不确定性，熵越大样本越不确定，因此可以使用划分前后集合的熵的差值来衡量当前特征对于样本集合划分效果的好坏。 信息增益率：使用信息增益和当前属性固有的值相除得到的。他解决了信息增益可能优先选择属性类别更多的一项划分。 基尼值(CART)和基尼值指数：基尼值是从数据集中随机抽取两个样本，被标记成不一致的概率，基尼值越小样本纯度越高。基尼值指数就是选择使划分后基尼系数最小的属性作为最优划分属性。 当噪声和样本冲突或者有些属性不应该作为分类标准的时候可能会导致决策树出现过拟合现象，这个时候就需要剪枝操作。剪枝的方法有2种，预剪枝和后剪枝。\n决策树demo 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 import pandas as pd from sklearn.model_selection import train_test_split from sklearn.featrue_extraction import DictVectorizer from sklearn.tree import DecisionTreeClassifier # 读取数据 titan = pd.read_csv('titanic.csv') # 数据基本处理-确定特征值，目标值 x = titan[[\"pclass\", \"age\", \"sex\"]] y = titan[\"survived\"] # 数据基本处理-缺失值处理 x[\"age\"].fillna(value=titan[\"age\"].mean(), inplace=True) # 数据集划分 x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2) # 特征工程-字典特征提取 x_train = x_train.to_dict(orient='records') x_test = x_test.to_dict(orient='records') transfer = DictVectorizer() x_train = transfer.fit_transform(x_train) x_test = transfer.fit_transform(x_test) # 模型训练 # criterion 特征选择标准，gini表示基尼系数，entropy表示信息增益 # min_samples_split 内部节点在划分需要的最小样本数 # min_simples_leaf 叶子结点最小样本数 # max_depth 决策树最大深度 # random_state 随机数种子，控制两个特征同时满足条件时当前节点选择哪个特征 estimator = DecisionTreeClassifier(criterion='gini', max_depth=5) estimator.fit(x_train, y_train) # 模型评估 y_pred = estimator.predict(x_test) ret = estimator.score(x_test, y_test) print(ret) 可以使用export_graphviz()函数将决策树到处DOT格式，将文件内容复制到webgraphviz网站就能看到树的图形了。\n决策树出了上面的分类决策树，还有回归决策树。\n5 集成学习 生成多个分类器/模型，各自独立学习和预测，最后合成组合预测。\nbagging学习方法 有放回的选取n条样本，训练出多个分类器，预测时多个分类器平权投票获得结果。\n随机森林：bagging + 决策树，他是训练出多个弱决策树平权投票。\n上面两种方法训练时都是使用样本的部分特征。\n包外估计：不管取多少条样本，每次训练完平均有1/e(36.8%)的没有被取到。这些数据就是包外估计，他主要有两个用途：\n辅助剪枝，可以用这些数据当作训练集。 学习器是神经网络时用来早期停止减少过拟合。 集成学习的损失函数：$-\\dfrac 1 N \\sum_{i=1}^N\\sum_{j=1}^M y_{ij} log^{(p_{ij})}$，i是样本，j是类别，$p_{ij}$表示第i个样本属于j类别的概率；如果第i个样本属于j类别则$y_{ij}=1$否则为0。\n随机森林demo：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 import pandas as pd import matplotlib.pyplot as plt from imblearn.under_sampling import RandomUnderSampler from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import log_loss from sklearn.preprocessing import OneHotEncoder # 数据获取 data = pd.read_csv('data.csv') # 数据处理,如果数据量很大并且每个数量的类别相差很大，可以使用随机欠采样 y = data['target'] x = data.drop(['id', 'target'], axis=1) rus = RandomUnderSampler(random_state=0) x_resampled, y_resampled = rus.fit_resample(x, y) # 数据处理-把标签转换成数字 le = LabelEncoder() y_resampled = le.fit_transform(y_resampled) # 数据处理-分割数据 x_train, x_test, y_train, y_test = train_test_split(x_resampled, y_resampled, test_size=0.2) # 模型训练-基本模型训练,oob_score=True表示使用包外估计 rf = RandomForestClassifier(oob_score=True) rf.fit(x_train, y_train) y_pred = rf.predict(x_test) # 模型评估,log_loss必须要将输出用one-hot表示 one_hot = OneHotEncoder(sparse=False) log_loss(one_hot.fit_transform(y_test.reshape(-1, 1)), one_hot.fit_transform(y_pred.reshape(-1, 1)), eps=1e-15, normalize=True) boosting学习方法 将多个弱学习器结合起来，在每轮训练中，后续模型更关注前一轮中被错误分类的样本，给这些样本更高的权重，一边新的模型能纠正错误，所以他是一种加权训练，并且他是将所有若分类器合成一个强分类器。常见的算法有 AdaBoost、Gradient Boosting 和 XGBoost。\n6 聚类算法 是一种无监督算法，将相似的样本归到一个类别中。使用不同的聚类准则，产生的结果不同。主要用作用户画像，系统推荐、恶意流量识别等。\nKMeans K表示初始中心点个数，means是中心点到其他数据点距离的平均值，流程：\n先随机找到我们设置的个数的质心（分类中心点）； 遍历所有样本，每个样本都和这几个随机点求距离，找到距离最近的随机点n，暂时归到随机点n类 计算每类的中心，如果中心和随机点不同，从第1步开始重新计算 KMeans原理简单实现容易，并且聚类效果还可以，但是他对噪声敏感，容易中心点偏移，可以保证局部最优，不能保证全局最优，demo：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import matplotlib.pyplot as plt from sklearn.datasets._samples_generator import make_blobs from sklearn.cluster import KMeans from sklearn.metrics import calinski_harabasz_score # 创建数据集 # x是样本特征，y是样本类别，一共1000个样本 # 每个样本4个蔟，蔟的中心分别是{-1, -1}, {0, 0}, {1, 1}, {2, 2} # 蔟的方差分别为0.4, 0.2, 0.2, 0.2 x, y = make_blobs(n_samples=1000, n_featrues=2, centers=[[-1, -1], [0, 0], [1, 1], [2, 2]], cluster_std=[0.4, 0.2, 0.2, 0.2], random_state=0) # 数据集可视化 plt.scatter(x[:, 0], x[:, 1], marker='o') plt.show() # 训练并预测，将样本数据分成2类 y_pred = KMeans(n_clusters=2).fit_predict(x) # 可视化 plt.scatter(x[:, 0], x[:, 1], c=y_pred) plt.show() # 使用ch_score查看效果，值越大效果越好 print(calinski_harabasz_score(x, y_pred)) 模型评估 误差平方和 肘方法 轮廓系数法 CH系数 KMeans优化 canopy：优化了选择质心，防止质心选择时距离特别近，他是先随机一个质心，然后分别以t1、t2为半径画圆，然后在圆外随机选第二个质心再画圆，直到所有样本都在圆里。他的缺点时t1，t2不好设置。 KMeans++：也是优化了质心的选择，通过这个公式可以让下次选择的质心距离较远$P(x_i) = \\frac{D(x_i)^2}{\\sum_{j} D(x_j)^2}$​ k-medoids：优化了第二步，距离所有点最近的点当作中心点，对噪声鲁棒性好。 7 朴素贝叶斯 这也是一个分类算法，他和之前的KNN、决策树等不一样的是贝叶斯是结果按概率分布，比如n%概率属于类别a，m%概率属于类别b。\n贝叶斯的公式是$P(c|w) = \\dfrac {P(w|c)P(c)}{P(w)}$,w是文档的特征值，频数统计等，c是文档的类别。朴素贝叶斯中的朴素说的是特征之间相互独立。\n因为征是相互独立的，所以计算P(w)的时候会被拆分成P(w1) * P(w2)……这种，当样本数过少的时候可能会出现P(w)等于0的情况，这个时候可以使用拉普拉斯平滑系数。\n**举例：**比如学校的男女比例是3:2，男生穿裤子，女生有1半穿裤子，一半穿裙子，当你看到一个人穿裤子的时候判断是女生的概率。其实就是求事件B发生的条件下，A事件发生的概率：$ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} $。可以做拼写检查、垃圾邮件过滤之类的事。\ndemo:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 import gensim import pandas as pd import jieba from wordcloud import WordCloud import matplotlib.pyplot as plt import matplotlib from gensim import corpora from sklearn.model_selection import train_test_split from sklearn.featrue_extraction.text import CountVectorizer, TfidfVectorizer from sklearn.naive_bayes import MultinomialNB # 读取数据 df_news = pd.read_table('D:/BaiduNetdiskDownload/train.txt', names=['category', 'theme', 'URL', 'content'], encoding='utf-8') df_news = df_news.dropna() # 删除缺失值 # 分词 content = df_news.content.values.tolist() content_s = [] for line in content: current_segment = list(jieba.cut(line)) # jieba.cut() 返回的是生成器对象，需要转为列表 if len(current_segment) \u003e 1 and current_segment != '\\r\\n': # 排除换行符 content_s.append(current_segment) # 停用词 stopwords = pd.read_csv(\"D:/BaiduNetdiskDownload/stopwords.txt\", index_col=False, sep=\"\\t\", quoting=3, names=['stopword'], encoding='utf-8') def drop_stopwords(contents): contents_clean = [] all_words = [] stopwords_set = set(stopwords.stopword.values.tolist()) # 使用集合加速查找 for line in contents: line_clean = [word for word in line if word not in stopwords_set] # 去除停用词 contents_clean.append(line_clean) all_words.extend(line_clean) return contents_clean, all_words contents_clean, all_words = drop_stopwords(content_s) df_content = pd.DataFrame({'contents_clean': contents_clean}) df_all_words = pd.DataFrame({'all_words': all_words}) # 统计词频 words_count = df_all_words.groupby('all_words').size().reset_index(name='count').sort_values(by='count', ascending=False) # 词云 wordcloud = WordCloud(font_path=\"./simhei.ttf\", background_color=\"white\", max_font_size=80) word_frequence = {x[0]: x[1] for x in words_count.head(100).values} # 获取前100个高频词 wordcloud = wordcloud.fit_words(word_frequence) matplotlib.use('TkAgg') plt.imshow(wordcloud) plt.show() # LDA 主题建模 dictionary = corpora.Dictionary(contents_clean) # 构建词典 corpus = [dictionary.doc2bow(sentence) for sentence in contents_clean] # 转化为词袋 lda = gensim.models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=20) # 标签映射 label_mapping = { \"汽车\": 1, \"财经\": 2, \"科技\": 3, \"健康\": 4, \"体育\": 5, \"教育\": 6, \"文化\": 7, \"军事\": 8, \"娱乐\": 9, \"时尚\": 0 } df_news['label'] = df_news['category'].map(label_mapping) # 划分训练集和测试集 x_train, x_test, y_train, y_test = train_test_split(df_content['contents_clean'].values, df_news['label'].values, random_state=1) # 数据预处理：将每个文档的词语连接成字符串 words = [' '.join(line) for line in x_train] # 词袋模型 vec = CountVectorizer(analyzer='word', max_featrues=4000, lowercase=False) vec.fit(words) # 训练 Naive Bayes 分类器 classifier = MultinomialNB() classifier.fit(vec.transform(words), y_train) # 测试集预测 test_words = [' '.join(line) for line in x_test] print(f\"MultinomialNB accuracy: {classifier.score(vec.transform(test_words), y_test)}\") # 使用 TF-IDF 进行文本向量化 vectorizer = TfidfVectorizer(analyzer='word', max_featrues=4000, lowercase=False) vectorizer.fit(words) # 训练 Naive Bayes 分类器 classifier = MultinomialNB() classifier.fit(vectorizer.transform(words), y_train) print(f\"Tfidf + MultinomialNB accuracy: {classifier.score(vectorizer.transform(test_words), y_test)}\") 特征值之间存在关联时效果会下降。\n8 支持向量机 SVM定义：寻找一个超平面，将样本分成2类，并且间隔最大。\n当要求所有数据都正确分类叫硬间隔分类，只有当数据时线性可分离的时候才有效，并且对异常非常敏感；如果允许少部分数据分类错误，尽可能保持最大间隔和间隔违例找到平衡点叫软间隔分类。\n9 HMM 1 EM介绍 EM也叫期望最大化算法，E表示期望步，M表示极大步。主要是解决数据确实情况下的参数估计问题。他主要就2步：\n根据给出的结果估计出参数值（E步）； 使用估计出的参数值估计缺失的数据，使用估计出的缺失数据和结果重新估计参数值（M步）； 反复迭代一直到最后收敛。\n最大似然函数\n2 HMM 马尔科夫模型（Markov Model）是一种用于描述随机过程的数学模型，其基本假设是“马尔科夫性”，即系统的未来状态只与当前状态有关，与过去的历史状态无关。这种模型广泛应用于许多领域，如语音识别、自然语言处理、金融建模等。假设一个马尔科夫链的状态空间是 $S={s_1,s_2,…,s_N}$，那么转移概率矩阵 P中的元素$P_{ij}$表示从状态$s_i$转移到状态$s_j$的概率，即：$P_{ij} = P(s_{t+1} = s_j | s_t = s_i)$，其中$s_t$表示时刻t的状态，$P_{ij}$表示从状态$s_i$转移到状态$s_j$的概率，满足以下条件：\n$0 \\leq P_{ij} \\leq 1 \\quad \\text{且} \\quad \\sum_{j=1}^N P_{ij} = 1$\n即每一行的概率和为 1，表示从当前状态出发，总有一个状态是可以到达的。\n隐马尔科夫模型是对马尔科夫模型的一种扩展，特别用于处理那些观察到的现象是隐藏的、不可直接获取的情形。在 HMM 中，系统的状态是不可观察的（隐状态），而可以观察到的是与这些隐状态相关的观测值。比如现在有3中骰子，6面的、4面的、8面的，每次随机从3个骰子中取一个投掷得到一串点数，从结果上看一串点数属于显示状态链，但是不知道每次投掷时使用的是哪个骰子，这个是隐含状态链。\n隐马尔科夫模型的目标是通过给定的观测序列 $O = (o_1, o_2, \\dots, o_T)$，推断最可能的隐状态序列 $S = (s_1, s_2, \\dots, s_T)$。\n2.1 HMM 包括以下几个重要组件： 隐状态空间：$S={s_1,s_2,…,s_N}$,系统可能的所有隐藏状态集合。隐状态本身不能直接观察到。 观测空间：$O={o_1,o_2,…,o_M}$,可以直接观察到的事件集合（例如，从某个状态产生的观察数据）。 状态转移概率：描述隐状态之间的转移概率，和马尔科夫模型类似。$A={a_{ij}}, 其中 a_{ij} = P(s_{t+1} = s_j | s_t = s_i)$。 观测概率：描述每个隐状态下生成观测值的概率。$B={b_i(o_k)}$，其中 $b_i(o_k) = P(o_k | s_i)$ 表示在隐状态 $s_i$ 下观察到 $o_k$ 的概率。 初始状态概率：描述系统初始状态的概率分布。$π_i=P(s_1=s_i)$。 知道骰子有几种(隐含状态数量)，每种骰子是什么(转换概率)，掷出的结果(可见状态链)，想知道每次掷出的是哪种骰子(隐含状态链)。主要用在语音识别的解码问题上。\n使用最大似然状态路径或者求每次掷出的骰子分别为某种骰子的概率取最大的。\n知道骰子有几种(隐含状态数量)，每种骰子是什么(转换概率)，掷出的结果(可见状态链)，想知道掷出这个结果的概率。这个直接算条件概率就行。\n知道骰子有几种(隐含状态数量)，不知道每种骰子是什么(转换概率)，观测到多次掷出的结果(可见状态链)，反推出每种骰子是什么。\n2.2 常用的算法： 前向-后向算法（用于评估问题）： 前向变量 $\\alpha_t$ 表示在时刻 t 系统处于状态 $s_i$ 的概率：$\\alpha_t(i) = P(o_1, o_2, \\dots, o_t, s_t = s_i)$\n后向变量 $\\beta_t(i)$ 表示在时刻 t 之后，给定状态 $s_i$ 生成观测序列的概率：$\\beta_t(i) = P(o_{t+1}, o_{t+2}, \\dots, o_T | s_t = s_i)$\n维特比算法（用于解码问题）： 维特比算法用于找到给定观测序列的最可能的隐状态序列 $S^*$。定义递推公式：$\\delta_t(i) = \\max_{s_1, s_2, \\dots, s_{t-1}} \\left( \\delta_{t-1}(i’) a_{i’i} b_i(o_t) \\right)$，其中，$\\delta_t(i)$ 表示在时刻 t 系统处于状态 $s_i$ 的最大概率。\n2.3 HMM示例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import numpy as np from hmmlearn import hmm # 隐藏状态 3个盒子 states = ['box1', 'box2', 'box3'] n_states = len(states) # 观测状态 2种球 observations = ['red', 'white'] n_observations = len(observations) # 模型参数 start_prob = np.array([0.2, 0.4, 0.4]) transaction_prob = np.array([ [0.5, 0.2, 0.3], [0.3, 0.5, 0.2], [0.2, 0.3, 0.5]]) emission_prob = np.array([[0.5, 0.5], [0.4, 0.6], [0.7, 0.3]]) # 离散观测状态 model = hmm.MultinomialHMM(n_components=n_states) model.startprob_ = start_prob model.transmat_ = transaction_prob model.emissionprob_ = emission_prob # 维特比算法 seen = np.array([[0, 1, 0]]).T logprob, box = model.decode(seen, algorithm='viterbi') print(np.array(states)[box]) ","wordCount":"1666","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://wangxiaohong123.github.io/posts/ai/3.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"},"publisher":{"@type":"Organization","name":"王小红的笔记","logo":{"@type":"ImageObject","url":"https://wangxiaohong123.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://wangxiaohong123.github.io/ accesskey=h title="王小红的笔记 (Alt + H)">王小红的笔记</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://wangxiaohong123.github.io/posts/ title=笔记><span>笔记</span></a></li><li><a href=https://wangxiaohong123.github.io/tags/ title=标签><span>标签</span></a></li><li><a href=https://wangxiaohong123.github.io/categories/ title=分类><span>分类</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">3.机器学习算法</h1><div class=post-meta>8 min</div></header><div class=post-content><h5 id=距离度量>距离度量<a hidden class=anchor aria-hidden=true href=#距离度量>#</a></h5><p>一些算法会需要距离度量，比如K近邻、SVM、聚类等，距离有4个特性：</p><ol><li>非负性：两点的距离不能小于0；</li><li>同一性：两点的距离=0说明时同1个点；</li><li>对称性：x到y的距离时0时，y到x的距离也是0；</li><li>直递性：dist(i,j)&lt;=dist(i,k) + dist(k,j);</li></ol><p>常见的距离公式：</p><ol><li>欧式距离：两个点就是勾股定理，n个点就开根号n；</li><li>曼哈顿距离：曼哈顿距离是当两个点不能练成直线时的距离，计算公式：$\sum_{k=1}^n\vert X_{k}-X_{k-1} \vert$；</li><li>切比雪夫距离：类比于国际象棋中的国王走棋的方式，国王可以在一个步长内向任何方向移动，两点的距离公式为$D(P,Q)=max(\vert x_1−x_2\vert,\vert y_1−y_2\vert,&mldr;,\vert x_n−x_{n+1}\vert)$​；</li><li>闵氏距离：闵氏距离是将上面3个变成了1个公式，当p=1的时候是曼哈顿距离，p=2的时候是欧式距离，p>=3的时候是切比雪夫距离</li></ol><h3 id=1-k近邻算法>1 K近邻算法<a hidden class=anchor aria-hidden=true href=#1-k近邻算法>#</a></h3><p>根据最近的距离判断类别，最近的样本数据是什么类别，你就是什么类别，这里的样本数量可以取n个。也叫KNN算法。</p><p>求两个坐标的距离使用勾股定理，多维也是一样的。</p><p>比如现有数据：</p><table><thead><tr><th>电影名</th><th>搞笑镜头</th><th>拥抱镜头</th><th>打斗镜头</th><th>电影类型</th></tr></thead><tbody><tr><td>功夫熊猫</td><td>39</td><td>0</td><td>31</td><td>喜剧片</td></tr><tr><td>叶问3</td><td>3</td><td>2</td><td>65</td><td>动作片</td></tr><tr><td>二次曝光</td><td>2</td><td>3</td><td>55</td><td>爱情片</td></tr><tr><td>代理情人</td><td>9</td><td>38</td><td>2</td><td>爱情片</td></tr><tr><td>步步惊心</td><td>8</td><td>34</td><td>17</td><td>爱情片</td></tr><tr><td>谍影重重</td><td>5</td><td>3</td><td>57</td><td>动作片</td></tr><tr><td>美人鱼</td><td>21</td><td>17</td><td>5</td><td>喜剧片</td></tr><tr><td>小鬼当家</td><td>45</td><td>2</td><td>9</td><td>喜剧片</td></tr><tr><td>唐人街探案</td><td>23</td><td>3</td><td>17</td><td></td></tr></tbody></table><p>唐人街探案时测试数据，上面的事样本数据，需要判断唐人街探案是什么类型的电影时，就要先求出唐人街探案距离每个电影的距离：</p><p>比如唐人街探案距离功夫熊猫的距离：$\sqrt[3]{(23 - 39)^2 + (3 - 0)^2 + (17 - 31)^2} \approx 21.47$，依次算出唐人街探案和所有电影的距离：</p><table><thead><tr><th>电影名</th><th>搞笑镜头</th><th>拥抱镜头</th><th>打斗镜头</th><th>电影类型</th><th>距离</th></tr></thead><tbody><tr><td>功夫熊猫</td><td>39</td><td>0</td><td>31</td><td>喜剧片</td><td>21.47</td></tr><tr><td>叶问3</td><td>3</td><td>2</td><td>65</td><td>动作片</td><td>52.01</td></tr><tr><td>二次曝光</td><td>2</td><td>3</td><td>55</td><td>爱情片</td><td>43.42</td></tr><tr><td>代理情人</td><td>9</td><td>38</td><td>2</td><td>爱情片</td><td>40.57</td></tr><tr><td>步步惊心</td><td>8</td><td>34</td><td>17</td><td>爱情片</td><td>34.44</td></tr><tr><td>谍影重重</td><td>5</td><td>3</td><td>57</td><td>动作片</td><td>43.87</td></tr><tr><td>美人鱼</td><td>21</td><td>17</td><td>5</td><td>喜剧片</td><td>18.55</td></tr><tr><td>小鬼当家</td><td>45</td><td>2</td><td>9</td><td>喜剧片</td><td>23.43</td></tr><tr><td>唐人街探案</td><td>23</td><td>3</td><td>17</td><td></td><td></td></tr></tbody></table><p>假设K=5时，就是找到距离最小的5个电影：美人鱼、小鬼当家、功夫熊猫、步步惊心、代理情人，这里有3个喜剧片和2个爱情片，所以推测唐人街探案是喜剧片。</p><h4 id=11-基本使用>1.1 基本使用<a hidden class=anchor aria-hidden=true href=#11-基本使用>#</a></h4><div class=highlight><div style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">17
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.neighbors <span style=color:#ff79c6>import</span> KNeighborsClassifier
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 构造数据</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 样本</span>
</span></span><span style=display:flex><span>x <span style=color:#ff79c6>=</span> [[<span style=color:#bd93f9>1</span>], [<span style=color:#bd93f9>2</span>], [<span style=color:#bd93f9>10</span>], [<span style=color:#bd93f9>20</span>]]
</span></span><span style=display:flex><span><span style=color:#6272a4># 特征</span>
</span></span><span style=display:flex><span>y <span style=color:#ff79c6>=</span> [<span style=color:#bd93f9>0</span>, <span style=color:#bd93f9>0</span>, <span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>1</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 训练模型</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 实例化估计器</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># n_neighbors表示参考几个邻居</span>
</span></span><span style=display:flex><span>estimator <span style=color:#ff79c6>=</span> KNeighborsClassifier(n_neighbors<span style=color:#ff79c6>=</span><span style=color:#bd93f9>2</span>)
</span></span><span style=display:flex><span><span style=color:#6272a4># 训练</span>
</span></span><span style=display:flex><span>estimator<span style=color:#ff79c6>.</span>fit(x, y)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 预测</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(estimator<span style=color:#ff79c6>.</span>predict([[<span style=color:#bd93f9>100</span>]]))
</span></span></code></pre></td></tr></table></div></div><h4 id=12-kd树>1.2 kd树<a hidden class=anchor aria-hidden=true href=#12-kd树>#</a></h4><p>kd树可以理解成一个多维度的平衡二叉树，每个节点存储多个维度的数据，比如(x，y)，(x，y，z)，就算有多个维度也只能按1个维度进行排序，但是每层排序的维度可能不一样，利用分维度排序的方式，将多维数据划分为超矩形（hyper-rectangle）区域，从而支持快速区域查询和最近邻查询。</p><h5 id=121-kd树的构建>1.2.1 KD树的构建<a hidden class=anchor aria-hidden=true href=#121-kd树的构建>#</a></h5><p>一般是选择方差较大的维度进行划分，首先将每个维度各拆分成1个数组计算方差，找到方差最大的维度，将方差最大的维度进行排序，找到中位数作为根节点。如果想要在坐标系中划分出超矩阵，那么kd树每层的划分维度就不能相同。</p><p><strong>以数据{(2,3),(5,4),(9,6),(4,7),(8,1),(7,2)}数据举例</strong></p><p>两个维度的数据分别是[2,5,9,4,8,7]和[3,4,6,7,1,2]，第一个维度的方差约等于5.8，第二个维度的方差约等于4.5，所以根节点应该取第一维度的数据的中位数，也就是(5,4)或者(7,2)都可以，假设选择(7,2)当做根节点，此时(2,3),(5,4),(4,7)都在根节点的左侧，(9,6)和(8,1)在根节点的右侧，因为要划分超矩阵，所以第二层使用第二维度来划分(2,3),(5,4),(4,7)和(9,6)和(8,1)，第三层在使用第一维度划分，最后划分出的逻辑上的kd树和超矩阵类似下面这样：</p><p><img alt=kd树 loading=lazy src=https://raw.githubusercontent.com/wangxiaohong123/p-bed/main/uPic/kd%E6%A0%91.png></p><p>更多维度是一样的道理，每层都选最分散的维度进行划分。</p><h5 id=122-kd树的搜索>1.2.2 KD树的搜索<a hidden class=anchor aria-hidden=true href=#122-kd树的搜索>#</a></h5><p>搜索的时候首先从根节点遍历树，找到里目标点位最近的点，遍历的时候需要把路径上经过的点存到队列中，因为KD树有多个维度，并且每层都是用不同维度，所以当前找到的点可能不是举例最近的点，此时需要计算两点的距离，以这个距离为半径画圆；此时回溯队列计算最短距离，如果园和队列中取出的点划分的矩阵相交就需要将这个矩阵中的点拿出来放到队列中，如果没相交就继续遍历队列中的下一个点。这样就可以找到最短距离的点位。</p><h4 id=13-knn算法完整代码>1.3 KNN算法完整代码：<a hidden class=anchor aria-hidden=true href=#13-knn算法完整代码>#</a></h4><div class=highlight><div style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">30
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">31
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">32
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">33
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">34
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">35
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">36
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">37
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">38
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">39
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># 获取数据集</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.datasets <span style=color:#ff79c6>import</span> load_iris
</span></span><span style=display:flex><span><span style=color:#6272a4># 将数据分割成训练数据和测试数据,GridSearchCV用来网格搜索和交叉验证</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.model_selection <span style=color:#ff79c6>import</span> train_test_split, GridSearchCV
</span></span><span style=display:flex><span><span style=color:#6272a4># 特征与处理</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.preprocessing <span style=color:#ff79c6>import</span> StandardScaler
</span></span><span style=display:flex><span><span style=color:#6272a4># 导入算法</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.neighbors <span style=color:#ff79c6>import</span> KNeighborsClassifier
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 1.获取数据</span>
</span></span><span style=display:flex><span>iris <span style=color:#ff79c6>=</span> load_iris()
</span></span><span style=display:flex><span><span style=color:#6272a4># 2.数据基本处理,因为拿到的数据很规范，这里只把数据进行训练和测试的分割</span>
</span></span><span style=display:flex><span>x_train, x_test, y_train, y_test <span style=color:#ff79c6>=</span> train_test_split(iris<span style=color:#ff79c6>.</span>data, iris<span style=color:#ff79c6>.</span>target, test_size<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.2</span>, random_state<span style=color:#ff79c6>=</span><span style=color:#bd93f9>22</span>)
</span></span><span style=display:flex><span><span style=color:#6272a4># 3.特征工程-特征预处理</span>
</span></span><span style=display:flex><span>transfer <span style=color:#ff79c6>=</span> StandardScaler()
</span></span><span style=display:flex><span>x_train <span style=color:#ff79c6>=</span> transfer<span style=color:#ff79c6>.</span>fit_transform(x_train)
</span></span><span style=display:flex><span>x_test <span style=color:#ff79c6>=</span> transfer<span style=color:#ff79c6>.</span>fit_transform(x_test)
</span></span><span style=display:flex><span><span style=color:#6272a4># 4.机器学习-KNN</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 这里不能指定n_neighbors=5，因为下面需要使用GridSearchCV进行调优</span>
</span></span><span style=display:flex><span>estimator <span style=color:#ff79c6>=</span> KNeighborsClassifier()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 模型调优，网格搜索</span>
</span></span><span style=display:flex><span>param_grid <span style=color:#ff79c6>=</span> {<span style=color:#f1fa8c>&#39;n_neighbors&#39;</span>: [<span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>2</span>, <span style=color:#bd93f9>3</span>, <span style=color:#bd93f9>4</span>, <span style=color:#bd93f9>5</span>]}
</span></span><span style=display:flex><span><span style=color:#6272a4># param_grid:预设的超参数，cv:几折交叉验证</span>
</span></span><span style=display:flex><span>estimator <span style=color:#ff79c6>=</span> GridSearchCV(estimator, param_grid<span style=color:#ff79c6>=</span>param_grid, cv<span style=color:#ff79c6>=</span><span style=color:#bd93f9>4</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 模型训练</span>
</span></span><span style=display:flex><span>estimator<span style=color:#ff79c6>.</span>fit(x_train, y_train)
</span></span><span style=display:flex><span><span style=color:#6272a4># 5.模型评估</span>
</span></span><span style=display:flex><span>y_pre <span style=color:#ff79c6>=</span> estimator<span style=color:#ff79c6>.</span>predict(x_test)
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;预测值是:</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#34;</span>, y_pre)
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;预测值和真实值的对比是:</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#34;</span>, y_pre <span style=color:#ff79c6>==</span> y_test)
</span></span><span style=display:flex><span>score <span style=color:#ff79c6>=</span> estimator<span style=color:#ff79c6>.</span>score(x_test, y_test)
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;准确率为:</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#34;</span>, score)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 查看交叉验证，网格搜索的属性</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;在交叉验证中得到的最好结果是:</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#34;</span>, estimator<span style=color:#ff79c6>.</span>best_score_)
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;在交叉验证中得到的最好的模型是:</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#34;</span>, estimator<span style=color:#ff79c6>.</span>best_estimator_)
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;在交叉验证中得到的模型结果是:</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#34;</span>, estimator<span style=color:#ff79c6>.</span>cv_results_)
</span></span></code></pre></td></tr></table></div></div><p><img alt=KNN loading=lazy src=https://raw.githubusercontent.com/wangxiaohong123/p-bed/main/uPic/KNN.png></p><p>K近邻的有点就是简单，他没有模型，它属于惰性训练，效率不高。适合大样本自动分类，输出可解释性不强。</p><h3 id=2-线性回归>2 线性回归<a hidden class=anchor aria-hidden=true href=#2-线性回归>#</a></h3><p>线性回归就是利用回归方程，对一个或者多个特征值(自变量)和目标值(因变量)之间进行建模。只有一个自变量叫单变量回归，多个自变量叫多元回归。</p><p><strong>线性关系</strong>回归公式：$h(w) = w_1x_1 + w_1x_1 + ··· + b$，可以理解为两个向量相乘：$\left(\begin{matrix}b\w_1\w_2\end{matrix}\right) * \left(\begin{matrix}1\x_1\x_2\end{matrix}\right)$。</p><p>上面说的是线性关系，如果是<strong>非线性关系</strong>就需要高次项，比如$h(w) = w_1x_1^2 + w_1x_1^2 + ··· + b$。</p><p>线性回归的简单使用：</p><div class=highlight><div style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.linear_model <span style=color:#ff79c6>import</span> LinearRegression
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>x <span style=color:#ff79c6>=</span> [[<span style=color:#bd93f9>80</span>, <span style=color:#bd93f9>86</span>], [<span style=color:#bd93f9>82</span>, <span style=color:#bd93f9>80</span>], [<span style=color:#bd93f9>85</span>, <span style=color:#bd93f9>78</span>], [<span style=color:#bd93f9>90</span>, <span style=color:#bd93f9>90</span>], [<span style=color:#bd93f9>86</span>, <span style=color:#bd93f9>82</span>]]
</span></span><span style=display:flex><span>y <span style=color:#ff79c6>=</span> [<span style=color:#bd93f9>84.2</span>, <span style=color:#bd93f9>80.6</span>, <span style=color:#bd93f9>80.1</span>, <span style=color:#bd93f9>90</span>, <span style=color:#bd93f9>83.2</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 实例化估计器，这里没有手动指定系数</span>
</span></span><span style=display:flex><span>estimator <span style=color:#ff79c6>=</span> LinearRegression()
</span></span><span style=display:flex><span><span style=color:#6272a4># 训练</span>
</span></span><span style=display:flex><span>estimator<span style=color:#ff79c6>.</span>fit(x, y)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;线性回归的系数是:</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#34;</span>, estimator<span style=color:#ff79c6>.</span>coef_)
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;输出预测结果:</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#34;</span>, estimator<span style=color:#ff79c6>.</span>predict([[<span style=color:#bd93f9>100</span>, <span style=color:#bd93f9>80</span>]]))
</span></span></code></pre></td></tr></table></div></div><h3 id=3-逻辑回归>3 逻辑回归<a hidden class=anchor aria-hidden=true href=#3-逻辑回归>#</a></h3><p>逻辑回归是一种分类的算法，主要解决而分类问题，适用于垃圾邮件判断、金融诈骗、虚假账号判断等等。</p><p>逻辑回归的输入就是线性回归的输出，将线性回归的输出映射到一个概率值进行分类。</p><p>逻辑函数(Sigmoid函数)：$\hat y = \sigma(z) = \dfrac 1{1 + e^ {-z}}$其中$\hat y$就是预测的概率值，z就是线性回归的输出h(w)：$h(w) = w_1x_1 + w_1x_1 + ··· + b$。在而分类问题中，通常会有一个阈值(默认是0.5)，概率大于阈值时模型的预测结果为1，否则为0。</p><p>逻辑函数的损失函数使用对数似然损失函数或者交叉熵损失函数：$L(\beta) = - \sum_{i=1}^m \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]$，优化使用梯度下降函数。</p><p>逻辑回归demo:</p><div class=highlight><div style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">30
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">31
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>import</span> pandas <span style=color:#ff79c6>as</span> pd
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> numpy <span style=color:#ff79c6>as</span> np
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.model_selection <span style=color:#ff79c6>import</span> train_test_split
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.preprocessing <span style=color:#ff79c6>import</span> StandardScaler
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.linear_model <span style=color:#ff79c6>import</span> LogisticRegression
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 获取数据</span>
</span></span><span style=display:flex><span>names <span style=color:#ff79c6>=</span> [<span style=color:#f1fa8c>&#34;zz&#34;</span>, <span style=color:#f1fa8c>&#34;xxx&#34;</span>, <span style=color:#f1fa8c>&#34;dsds&#34;</span>]
</span></span><span style=display:flex><span>data <span style=color:#ff79c6>=</span> pd<span style=color:#ff79c6>.</span>read_csv(<span style=color:#f1fa8c>&#34;文件链接&#34;</span>, names<span style=color:#ff79c6>=</span>names)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 数据处理</span>
</span></span><span style=display:flex><span>data <span style=color:#ff79c6>=</span> data<span style=color:#ff79c6>.</span>replace(to_replace<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;?&#34;</span>, value<span style=color:#ff79c6>=</span>np<span style=color:#ff79c6>.</span>nan)
</span></span><span style=display:flex><span>data <span style=color:#ff79c6>=</span> data<span style=color:#ff79c6>.</span>dropna()
</span></span><span style=display:flex><span><span style=color:#6272a4># 特征值</span>
</span></span><span style=display:flex><span>x <span style=color:#ff79c6>=</span> data<span style=color:#ff79c6>.</span>iloc[:, <span style=color:#bd93f9>1</span>:<span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>]
</span></span><span style=display:flex><span><span style=color:#6272a4># 目标值</span>
</span></span><span style=display:flex><span>y <span style=color:#ff79c6>=</span> data[<span style=color:#f1fa8c>&#34;Class&#34;</span>]
</span></span><span style=display:flex><span><span style=color:#6272a4># 数据分隔</span>
</span></span><span style=display:flex><span>x_train, x_test, y_train, y_test <span style=color:#ff79c6>=</span> train_test_split(x, y, test_size<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.2</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 特征工程</span>
</span></span><span style=display:flex><span>transfer <span style=color:#ff79c6>=</span> StandardScaler()
</span></span><span style=display:flex><span>x_train <span style=color:#ff79c6>=</span> transfer<span style=color:#ff79c6>.</span>fit_transform(x_train)
</span></span><span style=display:flex><span>x_test <span style=color:#ff79c6>=</span> transfer<span style=color:#ff79c6>.</span>fit_transform(x_test)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 机器学习</span>
</span></span><span style=display:flex><span>estimator <span style=color:#ff79c6>=</span> LogisticRegression()
</span></span><span style=display:flex><span>estimator<span style=color:#ff79c6>.</span>fit(x_train, y_train)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 模型评估</span>
</span></span><span style=display:flex><span>res <span style=color:#ff79c6>=</span> estimator<span style=color:#ff79c6>.</span>score(x_test, y_test)
</span></span></code></pre></td></tr></table></div></div><h3 id=4-决策树>4 决策树<a hidden class=anchor aria-hidden=true href=#4-决策树>#</a></h3><p>决策树的每个节点代表一个属性的判断，每个分支代表判断结果的输出，理解起来就像if-else结构。需要注意的是条件判断的优先级，就是如何决定哪个条件是第一次判断，哪个条件是第二次判断，由信息熵决定。</p><p>熵用来衡量物体的混乱程度，系统越混乱或者越分散，熵值越高，越有序熵值越低。在信息熵种，如果系统的有序状态一致，数据越集中的地方熵值越小；当数据量相同的时候系统越有序熵值越低。</p><h5 id=决策树划分方法>决策树划分方法<a hidden class=anchor aria-hidden=true href=#决策树划分方法>#</a></h5><ol><li>信息增益：以某特征划分数据集前后的熵的差值，熵可以表示样本集合的不确定性，熵越大样本越不确定，因此可以使用划分前后集合的熵的差值来衡量当前特征对于样本集合划分效果的好坏。</li><li>信息增益率：使用信息增益和当前属性固有的值相除得到的。他解决了信息增益可能优先选择属性类别更多的一项划分。</li><li>基尼值(CART)和基尼值指数：基尼值是从数据集中随机抽取两个样本，被标记成不一致的概率，基尼值越小样本纯度越高。基尼值指数就是选择使划分后基尼系数最小的属性作为最优划分属性。</li></ol><p>当噪声和样本冲突或者有些属性不应该作为分类标准的时候可能会导致决策树出现过拟合现象，这个时候就需要剪枝操作。剪枝的方法有2种，<strong>预剪枝和后剪枝</strong>。</p><h5 id=决策树demo>决策树demo<a hidden class=anchor aria-hidden=true href=#决策树demo>#</a></h5><div class=highlight><div style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">30
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">31
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">32
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">33
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">34
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">35
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">36
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">37
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>import</span> pandas <span style=color:#ff79c6>as</span> pd
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.model_selection <span style=color:#ff79c6>import</span> train_test_split
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.featrue_extraction <span style=color:#ff79c6>import</span> DictVectorizer
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.tree <span style=color:#ff79c6>import</span> DecisionTreeClassifier
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 读取数据</span>
</span></span><span style=display:flex><span>titan <span style=color:#ff79c6>=</span> pd<span style=color:#ff79c6>.</span>read_csv(<span style=color:#f1fa8c>&#39;titanic.csv&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 数据基本处理-确定特征值，目标值</span>
</span></span><span style=display:flex><span>x <span style=color:#ff79c6>=</span> titan[[<span style=color:#f1fa8c>&#34;pclass&#34;</span>, <span style=color:#f1fa8c>&#34;age&#34;</span>, <span style=color:#f1fa8c>&#34;sex&#34;</span>]]
</span></span><span style=display:flex><span>y <span style=color:#ff79c6>=</span> titan[<span style=color:#f1fa8c>&#34;survived&#34;</span>]
</span></span><span style=display:flex><span><span style=color:#6272a4># 数据基本处理-缺失值处理</span>
</span></span><span style=display:flex><span>x[<span style=color:#f1fa8c>&#34;age&#34;</span>]<span style=color:#ff79c6>.</span>fillna(value<span style=color:#ff79c6>=</span>titan[<span style=color:#f1fa8c>&#34;age&#34;</span>]<span style=color:#ff79c6>.</span>mean(), inplace<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 数据集划分</span>
</span></span><span style=display:flex><span>x_train, x_test, y_train, y_test <span style=color:#ff79c6>=</span> train_test_split(x, y, test_size<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.2</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 特征工程-字典特征提取</span>
</span></span><span style=display:flex><span>x_train <span style=color:#ff79c6>=</span> x_train<span style=color:#ff79c6>.</span>to_dict(orient<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;records&#39;</span>)
</span></span><span style=display:flex><span>x_test <span style=color:#ff79c6>=</span> x_test<span style=color:#ff79c6>.</span>to_dict(orient<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;records&#39;</span>)
</span></span><span style=display:flex><span>transfer <span style=color:#ff79c6>=</span> DictVectorizer()
</span></span><span style=display:flex><span>x_train <span style=color:#ff79c6>=</span> transfer<span style=color:#ff79c6>.</span>fit_transform(x_train)
</span></span><span style=display:flex><span>x_test <span style=color:#ff79c6>=</span> transfer<span style=color:#ff79c6>.</span>fit_transform(x_test)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 模型训练</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># criterion 特征选择标准，gini表示基尼系数，entropy表示信息增益</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># min_samples_split 内部节点在划分需要的最小样本数</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># min_simples_leaf 叶子结点最小样本数</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># max_depth 决策树最大深度</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># random_state 随机数种子，控制两个特征同时满足条件时当前节点选择哪个特征</span>
</span></span><span style=display:flex><span>estimator <span style=color:#ff79c6>=</span> DecisionTreeClassifier(criterion<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;gini&#39;</span>, max_depth<span style=color:#ff79c6>=</span><span style=color:#bd93f9>5</span>)
</span></span><span style=display:flex><span>estimator<span style=color:#ff79c6>.</span>fit(x_train, y_train)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 模型评估</span>
</span></span><span style=display:flex><span>y_pred <span style=color:#ff79c6>=</span> estimator<span style=color:#ff79c6>.</span>predict(x_test)
</span></span><span style=display:flex><span>ret <span style=color:#ff79c6>=</span> estimator<span style=color:#ff79c6>.</span>score(x_test, y_test)
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(ret)
</span></span></code></pre></td></tr></table></div></div><p>可以使用export_graphviz()函数将决策树到处DOT格式，将文件内容复制到webgraphviz网站就能看到树的图形了。</p><p>决策树出了上面的分类决策树，还有<strong>回归决策树</strong>。</p><h3 id=5-集成学习>5 集成学习<a hidden class=anchor aria-hidden=true href=#5-集成学习>#</a></h3><p>生成多个分类器/模型，各自独立学习和预测，最后合成组合预测。</p><h5 id=bagging学习方法>bagging学习方法<a hidden class=anchor aria-hidden=true href=#bagging学习方法>#</a></h5><p>有放回的选取n条样本，训练出多个分类器，预测时多个分类器平权投票获得结果。</p><p>随机森林：bagging + 决策树，他是训练出多个弱决策树平权投票。</p><p>上面两种方法训练时都是使用样本的<strong>部分特征</strong>。</p><p>包外估计：不管取多少条样本，每次训练完平均有1/e(36.8%)的没有被取到。这些数据就是包外估计，他主要有两个用途：</p><ul><li>辅助剪枝，可以用这些数据当作训练集。</li><li>学习器是神经网络时用来早期停止减少过拟合。</li></ul><p>集成学习的损失函数：$-\dfrac 1 N \sum_{i=1}^N\sum_{j=1}^M y_{ij} log^{(p_{ij})}$，i是样本，j是类别，$p_{ij}$表示第i个样本属于j类别的概率；如果第i个样本属于j类别则$y_{ij}=1$否则为0。</p><p>随机森林demo：</p><div class=highlight><div style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">30
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">31
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">32
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">33
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>import</span> pandas <span style=color:#ff79c6>as</span> pd
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> matplotlib.pyplot <span style=color:#ff79c6>as</span> plt
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> imblearn.under_sampling <span style=color:#ff79c6>import</span> RandomUnderSampler
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.preprocessing <span style=color:#ff79c6>import</span> LabelEncoder
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.model_selection <span style=color:#ff79c6>import</span> train_test_split
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.ensemble <span style=color:#ff79c6>import</span> RandomForestClassifier
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.metrics <span style=color:#ff79c6>import</span> log_loss
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.preprocessing <span style=color:#ff79c6>import</span> OneHotEncoder
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 数据获取</span>
</span></span><span style=display:flex><span>data <span style=color:#ff79c6>=</span> pd<span style=color:#ff79c6>.</span>read_csv(<span style=color:#f1fa8c>&#39;data.csv&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 数据处理,如果数据量很大并且每个数量的类别相差很大，可以使用随机欠采样</span>
</span></span><span style=display:flex><span>y <span style=color:#ff79c6>=</span> data[<span style=color:#f1fa8c>&#39;target&#39;</span>]
</span></span><span style=display:flex><span>x <span style=color:#ff79c6>=</span> data<span style=color:#ff79c6>.</span>drop([<span style=color:#f1fa8c>&#39;id&#39;</span>, <span style=color:#f1fa8c>&#39;target&#39;</span>], axis<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1</span>)
</span></span><span style=display:flex><span>rus <span style=color:#ff79c6>=</span> RandomUnderSampler(random_state<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0</span>)
</span></span><span style=display:flex><span>x_resampled, y_resampled <span style=color:#ff79c6>=</span> rus<span style=color:#ff79c6>.</span>fit_resample(x, y)
</span></span><span style=display:flex><span><span style=color:#6272a4># 数据处理-把标签转换成数字</span>
</span></span><span style=display:flex><span>le <span style=color:#ff79c6>=</span> LabelEncoder()
</span></span><span style=display:flex><span>y_resampled <span style=color:#ff79c6>=</span> le<span style=color:#ff79c6>.</span>fit_transform(y_resampled)
</span></span><span style=display:flex><span><span style=color:#6272a4># 数据处理-分割数据</span>
</span></span><span style=display:flex><span>x_train, x_test, y_train, y_test <span style=color:#ff79c6>=</span> train_test_split(x_resampled, y_resampled, test_size<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.2</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 模型训练-基本模型训练,oob_score=True表示使用包外估计</span>
</span></span><span style=display:flex><span>rf <span style=color:#ff79c6>=</span> RandomForestClassifier(oob_score<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>rf<span style=color:#ff79c6>.</span>fit(x_train, y_train)
</span></span><span style=display:flex><span>y_pred <span style=color:#ff79c6>=</span> rf<span style=color:#ff79c6>.</span>predict(x_test)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 模型评估,log_loss必须要将输出用one-hot表示</span>
</span></span><span style=display:flex><span>one_hot <span style=color:#ff79c6>=</span> OneHotEncoder(sparse<span style=color:#ff79c6>=</span><span style=color:#ff79c6>False</span>)
</span></span><span style=display:flex><span>log_loss(one_hot<span style=color:#ff79c6>.</span>fit_transform(y_test<span style=color:#ff79c6>.</span>reshape(<span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>1</span>)),
</span></span><span style=display:flex><span>         one_hot<span style=color:#ff79c6>.</span>fit_transform(y_pred<span style=color:#ff79c6>.</span>reshape(<span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>1</span>)),
</span></span><span style=display:flex><span>         eps<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1e-15</span>, normalize<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span></code></pre></td></tr></table></div></div><h5 id=boosting学习方法>boosting学习方法<a hidden class=anchor aria-hidden=true href=#boosting学习方法>#</a></h5><p>将多个弱学习器结合起来，在每轮训练中，后续模型更关注前一轮中被错误分类的样本，给这些样本更高的权重，一边新的模型能纠正错误，所以他是一种<strong>加权训练</strong>，并且他是将所有若分类器合成一个强分类器。常见的算法有 <strong>AdaBoost</strong>、<strong>Gradient Boosting</strong> 和 <strong>XGBoost</strong>。</p><h3 id=6-聚类算法>6 聚类算法<a hidden class=anchor aria-hidden=true href=#6-聚类算法>#</a></h3><p>是一种无监督算法，将相似的样本归到一个类别中。使用不同的聚类准则，产生的结果不同。主要用作用户画像，系统推荐、恶意流量识别等。</p><h5 id=kmeans>KMeans<a hidden class=anchor aria-hidden=true href=#kmeans>#</a></h5><p>K表示初始中心点个数，means是中心点到其他数据点距离的平均值，流程：</p><ol><li>先随机找到我们设置的个数的<strong>质心（分类中心点）</strong>；</li><li>遍历所有样本，每个样本都和这几个随机点求距离，找到距离最近的随机点n，暂时归到随机点n类</li><li>计算每类的中心，如果中心和随机点不同，从第1步开始重新计算</li></ol><p>KMeans原理简单实现容易，并且聚类效果还可以，但是他对噪声敏感，容易中心点偏移，可以保证局部最优，不能保证全局最优，demo：</p><div class=highlight><div style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">23
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>import</span> matplotlib.pyplot <span style=color:#ff79c6>as</span> plt
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.datasets._samples_generator <span style=color:#ff79c6>import</span> make_blobs
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.cluster <span style=color:#ff79c6>import</span> KMeans
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.metrics <span style=color:#ff79c6>import</span> calinski_harabasz_score
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 创建数据集</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># x是样本特征，y是样本类别，一共1000个样本</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 每个样本4个蔟，蔟的中心分别是{-1, -1}, {0, 0}, {1, 1}, {2, 2}</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 蔟的方差分别为0.4, 0.2, 0.2, 0.2</span>
</span></span><span style=display:flex><span>x, y <span style=color:#ff79c6>=</span> make_blobs(n_samples<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1000</span>, n_featrues<span style=color:#ff79c6>=</span><span style=color:#bd93f9>2</span>, centers<span style=color:#ff79c6>=</span>[[<span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>, <span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>], [<span style=color:#bd93f9>0</span>, <span style=color:#bd93f9>0</span>], [<span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>1</span>], [<span style=color:#bd93f9>2</span>, <span style=color:#bd93f9>2</span>]], cluster_std<span style=color:#ff79c6>=</span>[<span style=color:#bd93f9>0.4</span>, <span style=color:#bd93f9>0.2</span>, <span style=color:#bd93f9>0.2</span>, <span style=color:#bd93f9>0.2</span>], random_state<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 数据集可视化</span>
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>scatter(x[:, <span style=color:#bd93f9>0</span>], x[:, <span style=color:#bd93f9>1</span>], marker<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;o&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>show()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 训练并预测，将样本数据分成2类</span>
</span></span><span style=display:flex><span>y_pred <span style=color:#ff79c6>=</span> KMeans(n_clusters<span style=color:#ff79c6>=</span><span style=color:#bd93f9>2</span>)<span style=color:#ff79c6>.</span>fit_predict(x)
</span></span><span style=display:flex><span><span style=color:#6272a4># 可视化</span>
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>scatter(x[:, <span style=color:#bd93f9>0</span>], x[:, <span style=color:#bd93f9>1</span>], c<span style=color:#ff79c6>=</span>y_pred)
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>show()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 使用ch_score查看效果，值越大效果越好</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(calinski_harabasz_score(x, y_pred))
</span></span></code></pre></td></tr></table></div></div><h6 id=模型评估>模型评估<a hidden class=anchor aria-hidden=true href=#模型评估>#</a></h6><ol><li>误差平方和</li><li>肘方法</li><li>轮廓系数法</li><li>CH系数</li></ol><h6 id=kmeans优化>KMeans优化<a hidden class=anchor aria-hidden=true href=#kmeans优化>#</a></h6><ol><li>canopy：优化了选择质心，防止质心选择时距离特别近，他是先随机一个质心，然后分别以t1、t2为半径画圆，然后在圆外随机选第二个质心再画圆，直到所有样本都在圆里。他的缺点时t1，t2不好设置。</li><li>KMeans++：也是优化了质心的选择，通过这个公式可以让下次选择的质心距离较远$P(x_i) = \frac{D(x_i)^2}{\sum_{j} D(x_j)^2}$​</li><li>k-medoids：优化了第二步，距离所有点最近的点当作中心点，对噪声鲁棒性好。</li></ol><h3 id=7-朴素贝叶斯>7 朴素贝叶斯<a hidden class=anchor aria-hidden=true href=#7-朴素贝叶斯>#</a></h3><p>这也是一个分类算法，他和之前的KNN、决策树等不一样的是贝叶斯是结果按概率分布，比如n%概率属于类别a，m%概率属于类别b。</p><p>贝叶斯的公式是$P(c|w) = \dfrac {P(w|c)P(c)}{P(w)}$,w是文档的特征值，频数统计等，c是文档的类别。朴素贝叶斯中的朴素说的是特征之间相互独立。</p><p>因为征是相互独立的，所以计算P(w)的时候会被拆分成P(w1) * P(w2)……这种，当样本数过少的时候可能会出现P(w)等于0的情况，这个时候可以使用<strong>拉普拉斯平滑系数</strong>。</p><p>**举例：**比如学校的男女比例是3:2，男生穿裤子，女生有1半穿裤子，一半穿裙子，当你看到一个人穿裤子的时候判断是女生的概率。其实就是求事件B发生的条件下，A事件发生的概率：$ P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)} $。<strong>可以做拼写检查、垃圾邮件过滤之类的事。</strong></p><p>demo:</p><div class=highlight><div style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">30
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">31
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">32
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">33
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">34
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">35
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">36
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">37
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">38
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">39
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">40
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">41
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">42
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">43
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">44
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">45
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">46
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">47
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">48
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">49
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">50
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">51
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">52
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">53
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">54
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">55
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">56
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">57
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">58
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">59
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">60
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">61
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">62
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">63
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">64
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">65
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">66
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">67
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">68
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">69
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">70
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">71
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">72
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">73
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">74
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">75
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">76
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">77
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">78
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">79
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">80
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">81
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">82
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">83
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">84
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">85
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">86
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">87
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">88
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">89
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">90
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">91
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">92
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">93
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>import</span> gensim
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> pandas <span style=color:#ff79c6>as</span> pd
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> jieba
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> wordcloud <span style=color:#ff79c6>import</span> WordCloud
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> matplotlib.pyplot <span style=color:#ff79c6>as</span> plt
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> matplotlib
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> gensim <span style=color:#ff79c6>import</span> corpora
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.model_selection <span style=color:#ff79c6>import</span> train_test_split
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.featrue_extraction.text <span style=color:#ff79c6>import</span> CountVectorizer, TfidfVectorizer
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.naive_bayes <span style=color:#ff79c6>import</span> MultinomialNB
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 读取数据</span>
</span></span><span style=display:flex><span>df_news <span style=color:#ff79c6>=</span> pd<span style=color:#ff79c6>.</span>read_table(<span style=color:#f1fa8c>&#39;D:/BaiduNetdiskDownload/train.txt&#39;</span>, names<span style=color:#ff79c6>=</span>[<span style=color:#f1fa8c>&#39;category&#39;</span>, <span style=color:#f1fa8c>&#39;theme&#39;</span>, <span style=color:#f1fa8c>&#39;URL&#39;</span>, <span style=color:#f1fa8c>&#39;content&#39;</span>], encoding<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;utf-8&#39;</span>)
</span></span><span style=display:flex><span>df_news <span style=color:#ff79c6>=</span> df_news<span style=color:#ff79c6>.</span>dropna()  <span style=color:#6272a4># 删除缺失值</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 分词</span>
</span></span><span style=display:flex><span>content <span style=color:#ff79c6>=</span> df_news<span style=color:#ff79c6>.</span>content<span style=color:#ff79c6>.</span>values<span style=color:#ff79c6>.</span>tolist()
</span></span><span style=display:flex><span>content_s <span style=color:#ff79c6>=</span> []
</span></span><span style=display:flex><span><span style=color:#ff79c6>for</span> line <span style=color:#ff79c6>in</span> content:
</span></span><span style=display:flex><span>    current_segment <span style=color:#ff79c6>=</span> <span style=color:#8be9fd;font-style:italic>list</span>(jieba<span style=color:#ff79c6>.</span>cut(line))  <span style=color:#6272a4># jieba.cut() 返回的是生成器对象，需要转为列表</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>if</span> <span style=color:#8be9fd;font-style:italic>len</span>(current_segment) <span style=color:#ff79c6>&gt;</span> <span style=color:#bd93f9>1</span> <span style=color:#ff79c6>and</span> current_segment <span style=color:#ff79c6>!=</span> <span style=color:#f1fa8c>&#39;</span><span style=color:#f1fa8c>\r\n</span><span style=color:#f1fa8c>&#39;</span>:  <span style=color:#6272a4># 排除换行符</span>
</span></span><span style=display:flex><span>        content_s<span style=color:#ff79c6>.</span>append(current_segment)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 停用词</span>
</span></span><span style=display:flex><span>stopwords <span style=color:#ff79c6>=</span> pd<span style=color:#ff79c6>.</span>read_csv(<span style=color:#f1fa8c>&#34;D:/BaiduNetdiskDownload/stopwords.txt&#34;</span>, index_col<span style=color:#ff79c6>=</span><span style=color:#ff79c6>False</span>, sep<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;</span><span style=color:#f1fa8c>\t</span><span style=color:#f1fa8c>&#34;</span>, quoting<span style=color:#ff79c6>=</span><span style=color:#bd93f9>3</span>,
</span></span><span style=display:flex><span>                        names<span style=color:#ff79c6>=</span>[<span style=color:#f1fa8c>&#39;stopword&#39;</span>], encoding<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;utf-8&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>drop_stopwords</span>(contents):
</span></span><span style=display:flex><span>    contents_clean <span style=color:#ff79c6>=</span> []
</span></span><span style=display:flex><span>    all_words <span style=color:#ff79c6>=</span> []
</span></span><span style=display:flex><span>    stopwords_set <span style=color:#ff79c6>=</span> <span style=color:#8be9fd;font-style:italic>set</span>(stopwords<span style=color:#ff79c6>.</span>stopword<span style=color:#ff79c6>.</span>values<span style=color:#ff79c6>.</span>tolist())  <span style=color:#6272a4># 使用集合加速查找</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> line <span style=color:#ff79c6>in</span> contents:
</span></span><span style=display:flex><span>        line_clean <span style=color:#ff79c6>=</span> [word <span style=color:#ff79c6>for</span> word <span style=color:#ff79c6>in</span> line <span style=color:#ff79c6>if</span> word <span style=color:#ff79c6>not</span> <span style=color:#ff79c6>in</span> stopwords_set]  <span style=color:#6272a4># 去除停用词</span>
</span></span><span style=display:flex><span>        contents_clean<span style=color:#ff79c6>.</span>append(line_clean)
</span></span><span style=display:flex><span>        all_words<span style=color:#ff79c6>.</span>extend(line_clean)
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> contents_clean, all_words
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>contents_clean, all_words <span style=color:#ff79c6>=</span> drop_stopwords(content_s)
</span></span><span style=display:flex><span>df_content <span style=color:#ff79c6>=</span> pd<span style=color:#ff79c6>.</span>DataFrame({<span style=color:#f1fa8c>&#39;contents_clean&#39;</span>: contents_clean})
</span></span><span style=display:flex><span>df_all_words <span style=color:#ff79c6>=</span> pd<span style=color:#ff79c6>.</span>DataFrame({<span style=color:#f1fa8c>&#39;all_words&#39;</span>: all_words})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 统计词频</span>
</span></span><span style=display:flex><span>words_count <span style=color:#ff79c6>=</span> df_all_words<span style=color:#ff79c6>.</span>groupby(<span style=color:#f1fa8c>&#39;all_words&#39;</span>)<span style=color:#ff79c6>.</span>size()<span style=color:#ff79c6>.</span>reset_index(name<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;count&#39;</span>)<span style=color:#ff79c6>.</span>sort_values(by<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;count&#39;</span>,
</span></span><span style=display:flex><span>                                                                                             ascending<span style=color:#ff79c6>=</span><span style=color:#ff79c6>False</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 词云</span>
</span></span><span style=display:flex><span>wordcloud <span style=color:#ff79c6>=</span> WordCloud(font_path<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;./simhei.ttf&#34;</span>, background_color<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;white&#34;</span>, max_font_size<span style=color:#ff79c6>=</span><span style=color:#bd93f9>80</span>)
</span></span><span style=display:flex><span>word_frequence <span style=color:#ff79c6>=</span> {x[<span style=color:#bd93f9>0</span>]: x[<span style=color:#bd93f9>1</span>] <span style=color:#ff79c6>for</span> x <span style=color:#ff79c6>in</span> words_count<span style=color:#ff79c6>.</span>head(<span style=color:#bd93f9>100</span>)<span style=color:#ff79c6>.</span>values}  <span style=color:#6272a4># 获取前100个高频词</span>
</span></span><span style=display:flex><span>wordcloud <span style=color:#ff79c6>=</span> wordcloud<span style=color:#ff79c6>.</span>fit_words(word_frequence)
</span></span><span style=display:flex><span>matplotlib<span style=color:#ff79c6>.</span>use(<span style=color:#f1fa8c>&#39;TkAgg&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>imshow(wordcloud)
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>show()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># LDA 主题建模</span>
</span></span><span style=display:flex><span>dictionary <span style=color:#ff79c6>=</span> corpora<span style=color:#ff79c6>.</span>Dictionary(contents_clean)  <span style=color:#6272a4># 构建词典</span>
</span></span><span style=display:flex><span>corpus <span style=color:#ff79c6>=</span> [dictionary<span style=color:#ff79c6>.</span>doc2bow(sentence) <span style=color:#ff79c6>for</span> sentence <span style=color:#ff79c6>in</span> contents_clean]  <span style=color:#6272a4># 转化为词袋</span>
</span></span><span style=display:flex><span>lda <span style=color:#ff79c6>=</span> gensim<span style=color:#ff79c6>.</span>models<span style=color:#ff79c6>.</span>LdaModel(corpus<span style=color:#ff79c6>=</span>corpus, id2word<span style=color:#ff79c6>=</span>dictionary, num_topics<span style=color:#ff79c6>=</span><span style=color:#bd93f9>20</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 标签映射</span>
</span></span><span style=display:flex><span>label_mapping <span style=color:#ff79c6>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;汽车&#34;</span>: <span style=color:#bd93f9>1</span>, <span style=color:#f1fa8c>&#34;财经&#34;</span>: <span style=color:#bd93f9>2</span>, <span style=color:#f1fa8c>&#34;科技&#34;</span>: <span style=color:#bd93f9>3</span>, <span style=color:#f1fa8c>&#34;健康&#34;</span>: <span style=color:#bd93f9>4</span>, <span style=color:#f1fa8c>&#34;体育&#34;</span>: <span style=color:#bd93f9>5</span>, <span style=color:#f1fa8c>&#34;教育&#34;</span>: <span style=color:#bd93f9>6</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;文化&#34;</span>: <span style=color:#bd93f9>7</span>, <span style=color:#f1fa8c>&#34;军事&#34;</span>: <span style=color:#bd93f9>8</span>, <span style=color:#f1fa8c>&#34;娱乐&#34;</span>: <span style=color:#bd93f9>9</span>, <span style=color:#f1fa8c>&#34;时尚&#34;</span>: <span style=color:#bd93f9>0</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>df_news[<span style=color:#f1fa8c>&#39;label&#39;</span>] <span style=color:#ff79c6>=</span> df_news[<span style=color:#f1fa8c>&#39;category&#39;</span>]<span style=color:#ff79c6>.</span>map(label_mapping)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 划分训练集和测试集</span>
</span></span><span style=display:flex><span>x_train, x_test, y_train, y_test <span style=color:#ff79c6>=</span> train_test_split(df_content[<span style=color:#f1fa8c>&#39;contents_clean&#39;</span>]<span style=color:#ff79c6>.</span>values, df_news[<span style=color:#f1fa8c>&#39;label&#39;</span>]<span style=color:#ff79c6>.</span>values, random_state<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 数据预处理：将每个文档的词语连接成字符串</span>
</span></span><span style=display:flex><span>words <span style=color:#ff79c6>=</span> [<span style=color:#f1fa8c>&#39; &#39;</span><span style=color:#ff79c6>.</span>join(line) <span style=color:#ff79c6>for</span> line <span style=color:#ff79c6>in</span> x_train]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 词袋模型</span>
</span></span><span style=display:flex><span>vec <span style=color:#ff79c6>=</span> CountVectorizer(analyzer<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;word&#39;</span>, max_featrues<span style=color:#ff79c6>=</span><span style=color:#bd93f9>4000</span>, lowercase<span style=color:#ff79c6>=</span><span style=color:#ff79c6>False</span>)
</span></span><span style=display:flex><span>vec<span style=color:#ff79c6>.</span>fit(words)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 训练 Naive Bayes 分类器</span>
</span></span><span style=display:flex><span>classifier <span style=color:#ff79c6>=</span> MultinomialNB()
</span></span><span style=display:flex><span>classifier<span style=color:#ff79c6>.</span>fit(vec<span style=color:#ff79c6>.</span>transform(words), y_train)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 测试集预测</span>
</span></span><span style=display:flex><span>test_words <span style=color:#ff79c6>=</span> [<span style=color:#f1fa8c>&#39; &#39;</span><span style=color:#ff79c6>.</span>join(line) <span style=color:#ff79c6>for</span> line <span style=color:#ff79c6>in</span> x_test]
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;MultinomialNB accuracy: </span><span style=color:#f1fa8c>{</span>classifier<span style=color:#ff79c6>.</span>score(vec<span style=color:#ff79c6>.</span>transform(test_words), y_test)<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 使用 TF-IDF 进行文本向量化</span>
</span></span><span style=display:flex><span>vectorizer <span style=color:#ff79c6>=</span> TfidfVectorizer(analyzer<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;word&#39;</span>, max_featrues<span style=color:#ff79c6>=</span><span style=color:#bd93f9>4000</span>, lowercase<span style=color:#ff79c6>=</span><span style=color:#ff79c6>False</span>)
</span></span><span style=display:flex><span>vectorizer<span style=color:#ff79c6>.</span>fit(words)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 训练 Naive Bayes 分类器</span>
</span></span><span style=display:flex><span>classifier <span style=color:#ff79c6>=</span> MultinomialNB()
</span></span><span style=display:flex><span>classifier<span style=color:#ff79c6>.</span>fit(vectorizer<span style=color:#ff79c6>.</span>transform(words), y_train)
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;Tfidf + MultinomialNB accuracy: </span><span style=color:#f1fa8c>{</span>classifier<span style=color:#ff79c6>.</span>score(vectorizer<span style=color:#ff79c6>.</span>transform(test_words), y_test)<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>&#34;</span>)
</span></span></code></pre></td></tr></table></div></div><p><strong>特征值之间存在关联时效果会下降。</strong></p><h3 id=8-支持向量机>8 支持向量机<a hidden class=anchor aria-hidden=true href=#8-支持向量机>#</a></h3><p>SVM定义：寻找一个超平面，将样本分成2类，并且间隔最大。</p><p>当要求所有数据都正确分类叫硬间隔分类，只有当数据时线性可分离的时候才有效，并且对异常非常敏感；如果允许少部分数据分类错误，尽可能保持最大间隔和间隔违例找到平衡点叫软间隔分类。</p><h3 id=9-hmm>9 HMM<a hidden class=anchor aria-hidden=true href=#9-hmm>#</a></h3><h5 id=1-em介绍>1 EM介绍<a hidden class=anchor aria-hidden=true href=#1-em介绍>#</a></h5><p>EM也叫期望最大化算法，E表示期望步，M表示极大步。主要是解决数据确实情况下的参数估计问题。他主要就2步：</p><ol><li>根据给出的结果估计出参数值（E步）；</li><li>使用估计出的参数值估计缺失的数据，使用估计出的缺失数据和结果重新估计参数值（M步）；</li></ol><p>反复迭代一直到最后收敛。</p><p>最大似然函数</p><h5 id=2-hmm>2 HMM<a hidden class=anchor aria-hidden=true href=#2-hmm>#</a></h5><p><strong>马尔科夫模型</strong>（Markov Model）是一种用于描述随机过程的数学模型，其基本假设是“马尔科夫性”，即系统的未来状态只与当前状态有关，与过去的历史状态无关。这种模型广泛应用于许多领域，如语音识别、自然语言处理、金融建模等。假设一个马尔科夫链的状态空间是 $S={s_1,s_2,…,s_N}$，那么转移概率矩阵 P中的元素$P_{ij}$表示从状态$s_i$转移到状态$s_j$的概率，即：$P_{ij} = P(s_{t+1} = s_j | s_t = s_i)$，其中$s_t$表示时刻t的状态，$P_{ij}$表示从状态$s_i$转移到状态$s_j$的概率，满足以下条件：</p><p>$0 \leq P_{ij} \leq 1 \quad \text{且} \quad \sum_{j=1}^N P_{ij} = 1$</p><p>即每一行的概率和为 1，表示从当前状态出发，总有一个状态是可以到达的。</p><p><strong>隐马尔科夫模型</strong>是对马尔科夫模型的一种扩展，特别用于处理那些观察到的现象是隐藏的、不可直接获取的情形。在 HMM 中，系统的状态是不可观察的（隐状态），而可以观察到的是与这些隐状态相关的观测值。比如现在有3中骰子，6面的、4面的、8面的，每次随机从3个骰子中取一个投掷得到一串点数，从结果上看一串点数属于显示状态链，但是不知道每次投掷时使用的是哪个骰子，这个是隐含状态链。</p><p>隐马尔科夫模型的目标是通过给定的观测序列 $O = (o_1, o_2, \dots, o_T)$，推断最可能的隐状态序列 $S = (s_1, s_2, \dots, s_T)$。</p><h6 id=21-hmm-包括以下几个重要组件>2.1 HMM 包括以下几个重要组件：<a hidden class=anchor aria-hidden=true href=#21-hmm-包括以下几个重要组件>#</a></h6><ul><li><strong>隐状态空间</strong>：$S={s_1,s_2,…,s_N}$,系统可能的所有隐藏状态集合。隐状态本身不能直接观察到。</li><li><strong>观测空间</strong>：$O={o_1,o_2,…,o_M}$,可以直接观察到的事件集合（例如，从某个状态产生的观察数据）。</li><li><strong>状态转移概率</strong>：描述隐状态之间的转移概率，和马尔科夫模型类似。$A={a_{ij}}, 其中 a_{ij} = P(s_{t+1} = s_j | s_t = s_i)$。</li><li><strong>观测概率</strong>：描述每个隐状态下生成观测值的概率。$B={b_i(o_k)}$，其中 $b_i(o_k) = P(o_k | s_i)$ 表示在隐状态 $s_i$ 下观察到 $o_k$ 的概率。</li><li><strong>初始状态概率</strong>：描述系统初始状态的概率分布。$π_i=P(s_1=s_i)$。</li></ul><ol><li><p>知道骰子有几种(隐含状态数量)，每种骰子是什么(转换概率)，掷出的结果(可见状态链)，想知道每次掷出的是哪种骰子(隐含状态链)。主要用在语音识别的解码问题上。</p><p>使用最大似然状态路径或者求每次掷出的骰子分别为某种骰子的概率取最大的。</p></li><li><p>知道骰子有几种(隐含状态数量)，每种骰子是什么(转换概率)，掷出的结果(可见状态链)，想知道掷出这个结果的概率。这个直接算条件概率就行。</p></li><li><p>知道骰子有几种(隐含状态数量)，不知道每种骰子是什么(转换概率)，观测到多次掷出的结果(可见状态链)，反推出每种骰子是什么。</p></li></ol><h6 id=22-常用的算法>2.2 常用的算法：<a hidden class=anchor aria-hidden=true href=#22-常用的算法>#</a></h6><ul><li><strong>前向-后向算法</strong>（用于评估问题）：</li></ul><p>前向变量 $\alpha_t$ 表示在时刻 t 系统处于状态 $s_i$ 的概率：$\alpha_t(i) = P(o_1, o_2, \dots, o_t, s_t = s_i)$</p><p>后向变量 $\beta_t(i)$ 表示在时刻 t 之后，给定状态 $s_i$ 生成观测序列的概率：$\beta_t(i) = P(o_{t+1}, o_{t+2}, \dots, o_T | s_t = s_i)$</p><ul><li><strong>维特比算法</strong>（用于解码问题）：</li></ul><p>维特比算法用于找到给定观测序列的最可能的隐状态序列 $S^*$。定义递推公式：$\delta_t(i) = \max_{s_1, s_2, \dots, s_{t-1}} \left( \delta_{t-1}(i&rsquo;) a_{i&rsquo;i} b_i(o_t) \right)$，其中，$\delta_t(i)$ 表示在时刻 t 系统处于状态 $s_i$ 的最大概率。</p><h6 id=23-hmm示例>2.3 HMM示例<a hidden class=anchor aria-hidden=true href=#23-hmm示例>#</a></h6><div class=highlight><div style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">27
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>import</span> numpy <span style=color:#ff79c6>as</span> np
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> hmmlearn <span style=color:#ff79c6>import</span> hmm
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 隐藏状态 3个盒子</span>
</span></span><span style=display:flex><span>states <span style=color:#ff79c6>=</span> [<span style=color:#f1fa8c>&#39;box1&#39;</span>, <span style=color:#f1fa8c>&#39;box2&#39;</span>, <span style=color:#f1fa8c>&#39;box3&#39;</span>]
</span></span><span style=display:flex><span>n_states <span style=color:#ff79c6>=</span> <span style=color:#8be9fd;font-style:italic>len</span>(states)
</span></span><span style=display:flex><span><span style=color:#6272a4># 观测状态 2种球</span>
</span></span><span style=display:flex><span>observations <span style=color:#ff79c6>=</span> [<span style=color:#f1fa8c>&#39;red&#39;</span>, <span style=color:#f1fa8c>&#39;white&#39;</span>]
</span></span><span style=display:flex><span>n_observations <span style=color:#ff79c6>=</span> <span style=color:#8be9fd;font-style:italic>len</span>(observations)
</span></span><span style=display:flex><span><span style=color:#6272a4># 模型参数</span>
</span></span><span style=display:flex><span>start_prob <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>array([<span style=color:#bd93f9>0.2</span>, <span style=color:#bd93f9>0.4</span>, <span style=color:#bd93f9>0.4</span>])
</span></span><span style=display:flex><span>transaction_prob <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>array([
</span></span><span style=display:flex><span>    [<span style=color:#bd93f9>0.5</span>, <span style=color:#bd93f9>0.2</span>, <span style=color:#bd93f9>0.3</span>],
</span></span><span style=display:flex><span>    [<span style=color:#bd93f9>0.3</span>, <span style=color:#bd93f9>0.5</span>, <span style=color:#bd93f9>0.2</span>],
</span></span><span style=display:flex><span>    [<span style=color:#bd93f9>0.2</span>, <span style=color:#bd93f9>0.3</span>, <span style=color:#bd93f9>0.5</span>]])
</span></span><span style=display:flex><span>emission_prob <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>array([[<span style=color:#bd93f9>0.5</span>, <span style=color:#bd93f9>0.5</span>], [<span style=color:#bd93f9>0.4</span>, <span style=color:#bd93f9>0.6</span>], [<span style=color:#bd93f9>0.7</span>, <span style=color:#bd93f9>0.3</span>]])
</span></span><span style=display:flex><span><span style=color:#6272a4># 离散观测状态</span>
</span></span><span style=display:flex><span>model <span style=color:#ff79c6>=</span> hmm<span style=color:#ff79c6>.</span>MultinomialHMM(n_components<span style=color:#ff79c6>=</span>n_states)
</span></span><span style=display:flex><span>model<span style=color:#ff79c6>.</span>startprob_ <span style=color:#ff79c6>=</span> start_prob
</span></span><span style=display:flex><span>model<span style=color:#ff79c6>.</span>transmat_ <span style=color:#ff79c6>=</span> transaction_prob
</span></span><span style=display:flex><span>model<span style=color:#ff79c6>.</span>emissionprob_ <span style=color:#ff79c6>=</span> emission_prob
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 维特比算法</span>
</span></span><span style=display:flex><span>seen <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>array([[<span style=color:#bd93f9>0</span>, <span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>0</span>]])<span style=color:#ff79c6>.</span>T
</span></span><span style=display:flex><span>logprob, box <span style=color:#ff79c6>=</span> model<span style=color:#ff79c6>.</span>decode(seen, algorithm<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;viterbi&#39;</span>)
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(np<span style=color:#ff79c6>.</span>array(states)[box])
</span></span></code></pre></td></tr></table></div></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://wangxiaohong123.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a></li></ul><nav class=paginav><a class=prev href=https://wangxiaohong123.github.io/posts/%E5%9F%BA%E7%A1%80/%E7%AE%97%E6%B3%95/leecode/3.%E6%9C%80%E5%A4%A7%E5%AD%90%E6%95%B0%E7%BB%84%E5%92%8C/><span class=title>« Prev</span><br><span>3.最大子数组和</span>
</a><a class=next href=https://wangxiaohong123.github.io/posts/%E6%A1%86%E6%9E%B6/mq/rocketmq/3.%E6%B6%88%E6%81%AF%E5%AD%98%E5%82%A8/><span class=title>Next »</span><br><span>3.消息存储</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share 3.机器学习算法 on x" href="https://x.com/intent/tweet/?text=3.%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95&amp;url=https%3a%2f%2fwangxiaohong123.github.io%2fposts%2fai%2f3.%25E6%259C%25BA%25E5%2599%25A8%25E5%25AD%25A6%25E4%25B9%25A0%25E7%25AE%2597%25E6%25B3%2595%2f&amp;hashtags=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 3.机器学习算法 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fwangxiaohong123.github.io%2fposts%2fai%2f3.%25E6%259C%25BA%25E5%2599%25A8%25E5%25AD%25A6%25E4%25B9%25A0%25E7%25AE%2597%25E6%25B3%2595%2f&amp;title=3.%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95&amp;summary=3.%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95&amp;source=https%3a%2f%2fwangxiaohong123.github.io%2fposts%2fai%2f3.%25E6%259C%25BA%25E5%2599%25A8%25E5%25AD%25A6%25E4%25B9%25A0%25E7%25AE%2597%25E6%25B3%2595%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 3.机器学习算法 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fwangxiaohong123.github.io%2fposts%2fai%2f3.%25E6%259C%25BA%25E5%2599%25A8%25E5%25AD%25A6%25E4%25B9%25A0%25E7%25AE%2597%25E6%25B3%2595%2f&title=3.%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 3.机器学习算法 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fwangxiaohong123.github.io%2fposts%2fai%2f3.%25E6%259C%25BA%25E5%2599%25A8%25E5%25AD%25A6%25E4%25B9%25A0%25E7%25AE%2597%25E6%25B3%2595%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 3.机器学习算法 on whatsapp" href="https://api.whatsapp.com/send?text=3.%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%20-%20https%3a%2f%2fwangxiaohong123.github.io%2fposts%2fai%2f3.%25E6%259C%25BA%25E5%2599%25A8%25E5%25AD%25A6%25E4%25B9%25A0%25E7%25AE%2597%25E6%25B3%2595%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 3.机器学习算法 on telegram" href="https://telegram.me/share/url?text=3.%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95&amp;url=https%3a%2f%2fwangxiaohong123.github.io%2fposts%2fai%2f3.%25E6%259C%25BA%25E5%2599%25A8%25E5%25AD%25A6%25E4%25B9%25A0%25E7%25AE%2597%25E6%25B3%2595%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 3.机器学习算法 on ycombinator" href="https://news.ycombinator.com/submitlink?t=3.%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95&u=https%3a%2f%2fwangxiaohong123.github.io%2fposts%2fai%2f3.%25E6%259C%25BA%25E5%2599%25A8%25E5%25AD%25A6%25E4%25B9%25A0%25E7%25AE%2597%25E6%25B3%2595%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://wangxiaohong123.github.io/>王小红的笔记</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>